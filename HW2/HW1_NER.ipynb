{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "HW1. NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlegBEZb/NLP_advanced_course/blob/master/HW2/HW1_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVRaU_zG-PaI",
        "colab_type": "text"
      },
      "source": [
        "# About notebook\n",
        "Dataset\n",
        "*\tGroningen Meaning Bank (version 2.2.0)\n",
        "*\tTask: named entity recognition\n",
        "*\tTarget – named entity tags (BIO + entity type)\n",
        "*\tInput data: \n",
        "  * Use “en.met” files to extract the subcorpus\n",
        "  corpus = 'Voice of America' (for honogeneity of the input data set)\n",
        "  * Use \"en.tags\" files for the main input data:\n",
        "      *\traw tokens + may use the lemmas and the POS-tags \n",
        "  (i.e. take the “golden” POS-tagging);\n",
        "      *\twhich means:\n",
        "        *\tfirst three columns for input: ['word', 'pos', 'lemma']\n",
        "        *\tthe fourth column for target variable (‘ne_tags’)\n",
        "        (BIO annotation + the named-entity type in one tag)  \n",
        "\n",
        "# Tasks\n",
        "1.\tThe most trivial model = supervised HMM:\n",
        "  *\tTake hmmlearn (former sklearn), modify MultinomialHMM (I.e. inherit a new class from _BaseHMM making it a modified copy of the latter) to allow for supervised HMM training. The states of the HMM model = the NE tags.\n",
        "  *\tNOTE: may use NaiveBayes to learn emission probabilities in a supervized manner.\n",
        "  *\tOr implement from scratch (with Viterbi for prediction).\n",
        "  *\tNOTE: use tuples of features for X (not just the word, but additional info).\n",
        "  *\tNOTE: use smoothing for state transitions.\n",
        "2.\tCRF\n",
        "  *\tModify the input features;\n",
        "  *\tUse CRFSuite.\n",
        "3.\tBi-LSTM:\n",
        "  *\tUse keras or tensorflow;\n",
        "  *\thttps://github.com/hse-aml/natural-language-processing/blob/master/week2/week2-NER.ipynb\n",
        "  *\tA plus for incorporating CNN-layers;\n",
        "\n",
        "# Metrics\n",
        "* normalized confusion matrices, precision, recall, F-score \n",
        "(macro- and micro-) \n",
        "* (token level, entity level, partial matching (i.e. boundary-detection problem), binary).  \n",
        "NOTE: taking into account vocabulary transfer is a plus.\n",
        "\n",
        "# Evaluation Criteria\n",
        "Scoring (14.5 max):  \n",
        "*\tDataset overview – 0.5\n",
        "  *\ttext lengths, vocabulary size, frequencies of patterns (<UNK-type-i>) \n",
        "  *\tstats over the target tags\n",
        "*\tFeature engineering – 2 (1+1)\n",
        "  *\tgrammatical words = closed set (~ stop words)\n",
        "  *\tStemming + POS\n",
        "  *\tWord shape\n",
        "  *\tAd hoc features ( +1)  \n",
        "*\tWord patterns -> encode types of unknown words +0.5\n",
        "*\tSmoothing in HMM – 0.5 \n",
        "  *\tIn HMM: for state transitions.\n",
        "*\tIncorporating tupled features in HMM (on top of tokens) – 1\n",
        "*\tThe correct HMM implementation – 1\n",
        "*\tMore fine-grained feature engineering for the Neural Network + 0.5\n",
        "  *\tDifferentiate between POS-relevancy for the word and the context, etc.\n",
        "  *\tSentence-level features (may use “golden” sentence-splitting)\n",
        "*\tEvaluation (on all levels) – 1\n",
        "*\tConclusion on HMM deficiency (as a model) – 1\n",
        "*\tCRF: 1 point for use and evaluation, + 0.5 points for comparison and conclusions;\n",
        "*\tNN:\n",
        "  *\tMain network: 4\n",
        "  *\tCNN layers: +2  \n",
        "\n",
        "Libraries: hmmlearn, crfsuite, tensorflow, keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30D0Scxk4kn3",
        "colab_type": "text"
      },
      "source": [
        "# Libs import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ZnXUdrxQ3TAN",
        "colab_type": "code",
        "outputId": "910fb799-41ed-4c9b-98ca-55a44b62566f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import os, math, operator, csv, random, pickle, re, sys\n",
        "# import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import gc\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.util import pad_sequence\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gc\n",
        "# from keras import backend as K\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords');\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "! pip install Unidecode;\n",
        "from unidecode import unidecode\n",
        "#! pip install pyspellchecker;\n",
        "#from spellchecker import SpellChecker\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i673BlTJEazT",
        "colab_type": "text"
      },
      "source": [
        "# Global vars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okymGz_lEjLC",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "COLAB = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD2z8D2V7Dod",
        "colab_type": "text"
      },
      "source": [
        "# Authorization on Google drive and configurings paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auzLnrdK51Rg",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "if COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    data_folder = '/content/drive/My Drive/Advanced NLP/Homework 2: Named entity recognition on Groningen Meaning Bank dataset/gmb-2.2.0'\n",
        "else:\n",
        "    pass\n",
        "    \n",
        "print('data found:', os.listdir(data_folder))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UYdAXsdT3TAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pretrained_folder = \"../input/\"\n",
        "train_filepath = os.path.join(data_folder,\"train.csv\")\n",
        "test_filepath = os.path.join(data_folder,\"test.csv\")\n",
        "test_labels_filepath = os.path.join(data_folder,\"test_labels.csv\")\n",
        "\n",
        "#path to a submission\n",
        "submission_path = os.path.join(data_folder,\"sample_submission.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15GVrJYa717o",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8APgvUyskaf",
        "colab_type": "text"
      },
      "source": [
        "# Training model"
      ]
    }
  ]
}