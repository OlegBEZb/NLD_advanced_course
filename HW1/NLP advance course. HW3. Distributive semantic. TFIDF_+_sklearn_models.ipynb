{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"NLP advance course. HW3. Distributive semantic. TFIDF_+_sklearn_models.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"KVRaU_zG-PaI","colab_type":"text"},"source":["## Embeddings to check\n","\n","* Word Embeddings (word2vec, GloVe, etc.)\n","* Starspace and other similarity learning embeddings\n","* char-gram embeddings (bpemb, fasttext etc.)\n","* doc and sentence embeddings (doc2vec, sent2vec etc.)\n","* specific context models (BERT, ELMo)\n","* Advanced: Poincare Embeddings concept"]},{"cell_type":"markdown","metadata":{"id":"30D0Scxk4kn3","colab_type":"text"},"source":["# Libs import"]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"ZnXUdrxQ3TAN","colab_type":"code","outputId":"47c3be9e-93ff-4638-9b05-2fd65d1f34e8","executionInfo":{"status":"ok","timestamp":1576579459030,"user_tz":-180,"elapsed":6927,"user":{"displayName":"Олег Литвинов","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC8rY4OprXFCt_FxC9zp02Jpl3SMZAtMJxOocMO=s64","userId":"10665736335049067601"}},"colab":{"base_uri":"https://localhost:8080/","height":154}},"source":["import os, math, operator, csv, random, pickle, re, sys\n","# import tensorflow as tf\n","from tqdm import tqdm\n","import pandas as pd\n","import gc\n","from collections import Counter\n","\n","from nltk.util import pad_sequence\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import gc\n","# from keras import backend as K\n","\n","import nltk\n","nltk.download('stopwords');\n","from nltk.tokenize import TweetTokenizer\n","\n","from nltk.corpus import stopwords\n","\n","! pip install Unidecode;\n","from unidecode import unidecode\n","#! pip install pyspellchecker;\n","#from spellchecker import SpellChecker\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import KFold, train_test_split, cross_val_score\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import roc_auc_score, accuracy_score\n","\n","import numpy as np\n","import pandas as pd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","Collecting Unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n","\u001b[?25hInstalling collected packages: Unidecode\n","Successfully installed Unidecode-1.1.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i673BlTJEazT","colab_type":"text"},"source":["# Global vars"]},{"cell_type":"code","metadata":{"id":"okymGz_lEjLC","colab_type":"code","trusted":true,"colab":{}},"source":["COLAB = True"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VD2z8D2V7Dod","colab_type":"text"},"source":["# Authorization on Google drive and configurings paths"]},{"cell_type":"code","metadata":{"id":"auzLnrdK51Rg","colab_type":"code","outputId":"2bbf78fe-255c-4e69-88af-a912ec0c1ff8","trusted":true,"executionInfo":{"status":"ok","timestamp":1576579489228,"user_tz":-180,"elapsed":37084,"user":{"displayName":"Олег Литвинов","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC8rY4OprXFCt_FxC9zp02Jpl3SMZAtMJxOocMO=s64","userId":"10665736335049067601"}},"colab":{"base_uri":"https://localhost:8080/","height":138}},"source":["if COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    data_folder = '/content/drive/My Drive/Advanced NLP/Homework 1: Classical classification task like Kaggle Toxic or Quora/Toxic data'\n","    embeddings_folder = '/content/drive/My Drive/Advanced NLP/Homework 1: Classical classification task like Kaggle Toxic or Quora'\n","    print('data found:', os.listdir(data_folder))\n","else:\n","    data_folder = '../input/jigsaw-toxic-comment-classification-challenge/'\n","    embeddings_folder = '../input/glove-global-vectors-for-word-representation'\n","    print('data found:', os.listdir(data_folder))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","data found: ['test.csv', 'test_labels.csv', 'train.csv', 'sample_submission.csv', 'submission.csv']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"trusted":true,"id":"UYdAXsdT3TAd","colab_type":"code","colab":{}},"source":["#pretrained_folder = \"../input/\"\n","train_filepath = os.path.join(data_folder,\"train.csv\")\n","test_filepath = os.path.join(data_folder,\"test.csv\")\n","test_labels_filepath = os.path.join(data_folder,\"test_labels.csv\")\n","\n","#path to a submission\n","submission_path = os.path.join(data_folder,\"sample_submission.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"15GVrJYa717o","colab_type":"text"},"source":["# Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"jYljyuKTCm8r","colab_type":"text"},"source":["## Dicts and lists of useful words and transformations"]},{"cell_type":"code","metadata":{"id":"YjL5O731CtDD","colab_type":"code","trusted":true,"colab":{}},"source":["#List of some words that often appear in toxic comments\n","#Sorry about the level of toxicity in it!\n","toxic_words = [\"poop\", \"crap\", \"prick\", \"twat\", \"wikipedia\", \"wiki\", \"hahahahaha\", \"lol\", \"bastard\", \"sluts\", \"slut\", \"douchebag\", \"douche\", \"blowjob\", \"nigga\", \"dumb\", \"jerk\", \"wanker\", \"wank\", \"penis\", \"motherfucker\", \"fucker\", \"fuk\", \"fucking\", \"fucked\", \"fuck\", \"bullshit\", \"shit\", \"stupid\", \"bitches\", \"bitch\", \"suck\", \"cunt\", \"dick\", \"cocks\", \"cock\", \"die\", \"kill\", \"gay\", \"jewish\", \"jews\", \"jew\", \"niggers\", \"nigger\", \"faggot\", \"fag\", \"asshole\"]\n","astericks_words = [('mother****ers', 'motherfuckers'), ('motherf*cking', 'motherfucking'), ('mother****er', 'motherfucker'), ('motherf*cker', 'motherfucker'), ('bullsh*t', 'bullshit'), ('f**cking', 'fucking'), ('f*ucking', 'fucking'), ('fu*cking', 'fucking'), ('****ing', 'fucking'), ('a**hole', 'asshole'), ('assh*le', 'asshole'), ('f******', 'fucking'), ('f*****g', 'fucking'), ('f***ing', 'fucking'), ('f**king', 'fucking'), ('f*cking', 'fucking'), ('fu**ing', 'fucking'), ('fu*king', 'fucking'), ('fuc*ers', 'fuckers'), ('f*****', 'fucking'), ('f***ed', 'fucked'), ('f**ker', 'fucker'), ('f*cked', 'fucked'), ('f*cker', 'fucker'), ('f*ckin', 'fucking'), ('fu*ker', 'fucker'), ('fuc**n', 'fucking'), ('ni**as', 'niggas'), ('b**ch', 'bitch'), ('b*tch', 'bitch'), ('c*unt', 'cunt'), ('f**ks', 'fucks'), ('f*ing', 'fucking'), ('ni**a', 'nigga'), ('c*ck', 'cock'), ('c*nt', 'cunt'), ('cr*p', 'crap'), ('d*ck', 'dick'), ('f***', 'fuck'), ('f**k', 'fuck'), ('f*ck', 'fuck'), ('fc*k', 'fuck'), ('fu**', 'fuck'), ('fu*k', 'fuck'), ('s***', 'shit'), ('s**t', 'shit'), ('sh**', 'shit'), ('sh*t', 'shit'), ('tw*t', 'twat')]\n","fasttext_misspelings = {\"'n'balls\": 'balls', \"-nazi's\": 'nazis', 'adminabuse': 'admin abuse', \"admins's\": 'admins', 'arsewipe': 'arse wipe', 'assfack': 'asshole', 'assholifity': 'asshole', 'assholivity': 'asshole', 'asshoul': 'asshole', 'asssholeee': 'asshole', 'belizeans': 'mexicans', \"blowing's\": 'blowing', 'bolivians': 'mexicans', 'celtofascists': 'fascists', 'censorshipmeisters': 'censor', 'chileans': 'mexicans', 'clerofascist': 'fascist', 'cowcrap': 'crap', 'crapity': 'crap', \"d'idiots\": 'idiots', 'deminazi': 'nazi', 'dftt': \"don't feed the troll\", 'dildohs': 'dildo', 'dramawhores': 'drama whores', 'edophiles': 'pedophiles', 'eurocommunist': 'communist', 'faggotkike': 'faggot', 'fantard': 'retard', 'fascismnazism': 'fascism', 'fascistisized': 'fascist', 'favremother': 'mother', 'fuxxxin': 'fucking', \"g'damn\": 'goddamn', 'harassmentat': 'harassment', 'harrasingme': 'harassing me', 'herfuc': 'motherfucker', 'hilterism': 'fascism', 'hitlerians': 'nazis', 'hitlerites': 'nazis', 'hubrises': 'pricks', 'idiotizing': 'idiotic', 'inadvandals': 'vandals', \"jackass's\": 'jackass', 'jiggabo': 'nigga', 'jizzballs': 'jizz balls', 'jmbass': 'dumbass', 'lejittament': 'legitimate', \"m'igger\": 'nigger', \"m'iggers\": 'niggers', 'motherfacking': 'motherfucker', 'motherfuckenkiwi': 'motherfucker', 'muthafuggas': 'niggas', 'nazisms': 'nazis', 'netsnipenigger': 'nigger', 'niggercock': 'nigger', 'niggerspic': 'nigger', 'nignog': 'nigga', 'niqqass': 'niggas', \"non-nazi's\": 'not a nazi', 'panamanians': 'mexicans', 'pedidiots': 'idiots', 'picohitlers': 'hitler', 'pidiots': 'idiots', 'poopia': 'poop', 'poopsies': 'poop', 'presumingly': 'obviously', 'propagandaanddisinformation': 'propaganda and disinformation', 'propagandaministerium': 'propaganda', 'puertoricans': 'mexicans', 'puertorricans': 'mexicans', 'pussiest': 'pussies', 'pussyitis': 'pussy', 'rayaridiculous': 'ridiculous', 'redfascists': 'fascists', 'retardzzzuuufff': 'retard', \"revertin'im\": 'reverting', 'scumstreona': 'scums', 'southamericans': 'mexicans', 'strasserism': 'fascism', 'stuptarded': 'retarded', \"t'nonsense\": 'nonsense', \"threatt's\": 'threat', 'titoists': 'communists', 'twatbags': 'douchebags', 'youbollocks': 'you bollocks'}\n","acronym_words = {\"btw\":\"by the way\", \"yo\": \"you\", \"u\": \"you\", \"r\": \"are\", \"ur\": \"your\", \"ima\": \"i am going to\", \"imma\": \"i am going to\", \"i'ma\":\"i am going to\", \"cos\":\"because\", \"coz\":\"because\", \"stfu\": \"shut the fuck up\", \"wat\": \"what\"}\n","CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n","\n","english_stopwords = set(stopwords.words('english'))\n","\n","#spell_checker = SpellChecker()\n","\n","cont_patterns = [\n","    (r'(W|w)on\\'t', r'will not'),\n","    (r'(C|c)an\\'t', r'can not'),\n","    (r'(I|i)\\'m', r'i am'),\n","    (r'(A|a)in\\'t', r'is not'),\n","    (r'(\\w+)\\'ll', r'\\g<1> will'),\n","    (r'(\\w+)n\\'t', r'\\g<1> not'),\n","    (r'(\\w+)\\'ve', r'\\g<1> have'),\n","    (r'(\\w+)\\'s', r'\\g<1> is'),\n","    (r'(\\w+)\\'re', r'\\g<1> are'),\n","    (r'(\\w+)\\'d', r'\\g<1> would'),\n","]\n","patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n","\n","#We will filter all characters except alphabet characters and some punctuation\n","valid_characters = \" \" + \"@$\" + \"'!?-\" + \"abcdefghijklmnopqrstuvwxyz\" + \"abcdefghijklmnopqrstuvwxyz\".upper()\n","valid_set = set(x for x in valid_characters)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cs_nlu1iILY1","colab_type":"text"},"source":["## Funcs for processing"]},{"cell_type":"code","metadata":{"id":"GfUaIP-wIKTZ","colab_type":"code","trusted":true,"colab":{}},"source":["def replace_url(word):\n","    if \"http://\" in word or \"www.\" in word or \"https://\" in word or \"wikipedia.org\" in word:\n","        return \"\"\n","    return word\n","\n","\n","def word_tokenize(sentence, tokenizer):\n","    sentence = sentence.replace(\"$\", \"s\")\n","    sentence = sentence.replace(\"@\", \"a\")    \n","    sentence = sentence.replace(\"!\", \" ! \")\n","    sentence = sentence.replace(\"?\", \" ? \")\n","\n","    return tokenizer.tokenize(sentence)\n","\n","\n","def split_toxic_and_normal_parts(word, toxic_words):\n","    if word == \"\":\n","        return \"\"\n","    \n","    lower = word.lower()\n","    for toxic_word in toxic_words:\n","        start = lower.find(toxic_word)\n","        if start >= 0:\n","            end = start + len(toxic_word)\n","            result = \" \".join([word[0:start], word[start:end], split_toxic_and_normal_parts(word[end:], toxic_words)])\n","            return result.replace(\"  \", \" \").strip()\n","    return word\n","\n","\n","def normalize_by_dictionary(normalized_word, dictionary):\n","    result = []\n","    for word in normalized_word.split():\n","        if word == word.upper():\n","            if word.lower() in dictionary:\n","                result.append(dictionary[word.lower()].upper())\n","            else:\n","                result.append(word)\n","        else:\n","            if word.lower() in dictionary:\n","                result.append(dictionary[word.lower()])\n","            else:\n","                result.append(word)\n","    \n","    return \" \".join(result)\n","\n","\n","def normalize_comment(comment, tokenizer, max_comment_length):\n","    comment = unidecode(comment)\n","    comment = comment[:max_comment_length]\n","\n","    normalized_words = []\n","    \n","    # ('mother****ers', 'motherfuckers')\n","    # for w in astericks_words:\n","    #     if w[0] in comment:\n","    #         comment = comment.replace(w[0], w[1])\n","    #     if w[0].upper() in comment:\n","    #         comment = comment.replace(w[0].upper(), w[1].upper())\n","    \n","    for word in word_tokenize(comment, tokenizer):\n","        if word in english_stopwords: continue\n","\n","        # # '(W|w)on\\'t', r'will not'\n","        # for (pattern, repl) in patterns:\n","        #    word = re.sub(pattern, repl, word)\n","\n","        if word == \".\" or word == \",\":\n","            normalized_words.append(word)\n","            continue\n","        \n","        # drop url parts from links\n","        # word = replace_url(word)\n","\n","        # replace single dots to whitespaces\n","        if word.count(\".\") == 1:\n","            word = word.replace(\".\", \" \")\n","\n","        # leave only legalized symbols\n","        filtered_word = \"\".join([x for x in word if x in valid_set])\n","                    \n","        #Kind of hack: for every word check if it has a toxic word as a part of it\n","        #If so, split this word by swear and non-swear part.\n","        #normalized_word = split_toxic_and_normal_parts(filtered_word, toxic_words)\n","        normalized_word = filtered_word\n","\n","#         normalized_word = normalize_by_dictionary(normalized_word, hyphens_dict)\n","#         normalized_word = normalize_by_dictionary(normalized_word, merged_dict)\n","        \n","        # check misspellings\n","        # temp = []\n","        # for word in normalized_word.split():\n","        #   temp.append(spell_checker.correction(word))\n","        # normalized_word = \" \".join(temp)\n","          \n","        # normalized_word = normalize_by_dictionary(normalized_word, fasttext_misspelings)\n","        normalized_word = normalize_by_dictionary(normalized_word, acronym_words)\n","\n","        normalized_words.append(normalized_word)\n","        \n","    normalized_comment = \" \".join(normalized_words)\n","    \n","    result = []\n","    for word in normalized_comment.split():\n","        # if word is upper\n","        if word.upper() == word:\n","            result.append(word)\n","        else:\n","            result.append(word.lower())\n","    \n","    #apparently, people on wikipedia love to talk about sockpuppets :-)\n","    result = \" \".join(result)\n","    if \"sock puppet\" in result:\n","        result = result.replace(\"sock puppet\", \"sockpuppet\")\n","    \n","    if \"SOCK PUPPET\" in result:\n","        result = result.replace(\"SOCK PUPPET\", \"SOCKPUPPET\")\n","    \n","    return result\n","\n","\n","def preprocess_text(df, preprocessing_func, tokenizer, max_comment_length):\n","  text_ndarray = df.fillna('_na').values\n","  np_preprocessing_func = np.vectorize(preprocessing_func)\n","\n","  preprocessed_text = np_preprocessing_func(text_ndarray, tokenizer, max_comment_length)\n","\n","  print('Gained shape:', preprocessed_text.shape)\n","  return preprocessed_text\n","\n","\n","def tokenize_and_pad(text, tokenizer):\n","\n","  tokenized_text = tokenizer.texts_to_sequences(text)\n","  padded_text = pad_sequences(tokenized_text, maxlen=MAX_LEN, dtype='int', value=0)\n","\n","  print('Gained shape:', padded_text.shape)\n","  return padded_text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tn1-XTZEIZ3y","colab_type":"text"},"source":["## Loading frames and processing them"]},{"cell_type":"code","metadata":{"id":"1AWRdxZsHG3B","colab_type":"code","trusted":true,"colab":{}},"source":["TEXT_COLUMN = 'comment_text'\n","TARGET_COLS = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n","\n","MAX_LEN = 220\n","max_tokens = 20000\n","max_comment_length = 20000 #We are going to truncate a comment if its length > threshold"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAe8xpV48CO1","colab_type":"code","outputId":"405ec415-4e07-4d2b-e46e-d598b4c422d9","trusted":true,"executionInfo":{"status":"ok","timestamp":1576579490605,"user_tz":-180,"elapsed":38360,"user":{"displayName":"Олег Литвинов","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC8rY4OprXFCt_FxC9zp02Jpl3SMZAtMJxOocMO=s64","userId":"10665736335049067601"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["%%time\n","\n","train_df = pd.read_csv(train_filepath)\n","train_labels = train_df[TARGET_COLS].values"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 733 ms, sys: 114 ms, total: 847 ms\n","Wall time: 1.12 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Dxwo5A8VfRRL","colab_type":"code","outputId":"50b362cd-cd92-4892-f13a-d54a3f7a138b","trusted":true,"executionInfo":{"status":"ok","timestamp":1576579553460,"user_tz":-180,"elapsed":101173,"user":{"displayName":"Олег Литвинов","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC8rY4OprXFCt_FxC9zp02Jpl3SMZAtMJxOocMO=s64","userId":"10665736335049067601"}},"colab":{"base_uri":"https://localhost:8080/","height":138}},"source":["%%time\n","\n","tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n","\n","preprocessed_train = preprocess_text(train_df[TEXT_COLUMN], \n","                                     normalize_comment, \n","                                     tknzr,\n","                                     max_comment_length)\n","\n","print(preprocessed_train[:3])\n","\n","del train_df\n","gc.collect()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Gained shape: (159571,)\n","[\"explanation why edits made username hardcore metallica fan reverted ? they vandalisms , closure gas I voted new york dolls FAC . and please remove template talk page since i'm retired . .\"\n"," \"d'aww ! he matches background colour i'm seemingly stuck . thanks . talk , january , UTC\"\n"," \"hey man , i'm really trying edit war . it's guy constantly removing relevant information talking edits instead talk page . he seems care formatting actual info .\"]\n","CPU times: user 59.8 s, sys: 3.02 s, total: 1min 2s\n","Wall time: 1min 2s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x5MLLz630n8i","colab_type":"code","outputId":"f9106038-802c-4ee1-953b-6a20b71bc4e0","trusted":true,"executionInfo":{"status":"ok","timestamp":1576579611182,"user_tz":-180,"elapsed":158851,"user":{"displayName":"Олег Литвинов","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC8rY4OprXFCt_FxC9zp02Jpl3SMZAtMJxOocMO=s64","userId":"10665736335049067601"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["%%time\n","\n","test_df = pd.read_csv(test_filepath)\n","\n","preprocessed_test = preprocess_text(test_df[TEXT_COLUMN], \n","                                    normalize_comment, \n","                                    tknzr,\n","                                    max_comment_length)\n","\n","del test_df, tknzr\n","gc.collect()\n","# y_train = np.apply_along_axis(document_vector, 1, tokenized_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Gained shape: (153164,)\n","CPU times: user 54.5 s, sys: 2.87 s, total: 57.4 s\n","Wall time: 57.7 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"n8APgvUyskaf","colab_type":"text"},"source":["# Training model"]},{"cell_type":"markdown","metadata":{"id":"wCRY8qnNJTh_","colab_type":"text"},"source":["## Preparing features"]},{"cell_type":"code","metadata":{"id":"jSAJK0AYsdW2","colab_type":"code","trusted":true,"colab":{}},"source":["# word_index = tokenizer.word_index\n","# print('Found %s unique tokens.' %len(word_index))\n","\n","\n","# # %%time\n","# # embedding_matrix = np.load('../input/embedding-2/embedding_matrix_big.npy')\n","\n","\n","# embedding_len = 25\n","# #g = GloveEmbedding('twitter', d_emb=embedding_len, show_progress=True)\n","# words = pd.read_csv(os.path.join(HW_folder, \"glove.twitter.27B.{}d.txt\".format(embedding_len)), \n","#                     sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n","\n","\n","# vocab_words = words[words.index.isin(list(word_index.keys()))]\n","# print('vocab_words.shape', vocab_words.shape)\n","\n","\n","# def get_vec(word, words_df):\n","#   return words_df.loc[word].values\n","\n","\n","# # embedding_matrix = np.zeros((len(vocab), embedding_len))\n","# # for word, emb in tqdm(embeddings_dict.items(), total=len(embeddings_dict), mininterval=10):\n","# #     index = list(embeddings_dict.keys()).index(word)\n","# #     if emb is not None:\n","# #       embedding_matrix[index] = emb\n","\n","\n","# num_words = min(max_tokens, len(word_index) + 1)\n","# embedding_matrix = np.zeros((num_words, embedding_len)) # zeroth row for UNK\n","# for word, i in word_index.items():\n","#   if i >= max_tokens:\n","#     continue\n","#   try:\n","#     embedding_vector = get_vec(word, vocab_words)\n","#   except:\n","#     continue\n","#   if embedding_vector is not None:\n","#     # words not found in embedding index will be all-zeros.\n","#     embedding_matrix[i] = embedding_vector\n","\n","\n","# print('embedding_matrix.shape', embedding_matrix.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xys_jQL_tOnx","colab_type":"code","outputId":"5c365c4a-c02f-4fe4-9c9c-ae7fc72dca59","trusted":true,"executionInfo":{"status":"ok","timestamp":1576579632056,"user_tz":-180,"elapsed":179695,"user":{"displayName":"Олег Литвинов","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC8rY4OprXFCt_FxC9zp02Jpl3SMZAtMJxOocMO=s64","userId":"10665736335049067601"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["%%time\n","\n","tfidf_vec = TfidfVectorizer(tokenizer=None,\n","                            min_df=2,\n","                            max_df=.625,\n","                            strip_accents='unicode',\n","                            use_idf=1,\n","                            smooth_idf=True,\n","                            sublinear_tf=True,\n","                            max_features=max_tokens)#, stop_words=stop_words_en)\n","\n","train_features = tfidf_vec.fit_transform(preprocessed_train)\n","test_features = tfidf_vec.transform(preprocessed_test)\n","\n","del preprocessed_train, preprocessed_test, tfidf_vec\n","gc.collect()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 20.3 s, sys: 508 ms, total: 20.8 s\n","Wall time: 20.9 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AslnHsTy0gCQ","colab_type":"text"},"source":["## Build and fit"]},{"cell_type":"markdown","metadata":{"id":"PhX9sRHYK7LB","colab_type":"text"},"source":["### Log Reg"]},{"cell_type":"code","metadata":{"id":"JBSKOOA63aR-","colab_type":"code","trusted":true,"colab":{}},"source":["submission = pd.read_csv(submission_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NhCdejbILCqa","colab_type":"code","trusted":true,"colab":{}},"source":["# %%time\n","\n","# def document_vector(doc, embedding_matrix=embedding_matrix):\n","#     \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n","#     doc = [embedding_matrix[index] for index in doc]\n","#     return np.mean(doc, axis=0)\n","\n","\n","# x_train = np.apply_along_axis(document_vector, 1, tokenized_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W-GuOc0A0evP","colab_type":"code","trusted":true,"colab":{}},"source":["models = {\n","    'RandomForestClassifier': RandomForestClassifier(),\n","    'LogisticRegression': LogisticRegression()\n","}\n","\n","# the optimisation parameters for each of the above models\n","params = {\n","    'RandomForestClassifier':{ \n","        \"n_estimators\": [100],\n","        \"max_features\": [\"auto\"],\n","        \"bootstrap\": [True],\n","        \"criterion\": ['gini', 'entropy'],\n","        \"oob_score\": [True, False]\n","            },\n","    'LogisticRegression': {\n","        'solver': ['newton-cg', 'sag', 'lbfgs'],\n","        'C': [0.01, 0.1, 0.5, 1, 10, 100, 1000]\n","        }  \n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kCAI_v8uOabV","colab_type":"code","outputId":"11a1888b-093d-4e2a-f291-3369f5b4eba9","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["%%time\n","\n","for i, class_name in enumerate(TARGET_COLS):\n","    print('**Processing {} comments...**'.format(class_name))\n","\n","    best_score = 0\n","    best_model_with_params = tuple()\n","\n","    for name in models.keys():\n","      print(' **Processing {} model...**'.format(name))\n","      est = models[name]\n","      est_params = params[name]\n","      gscv = GridSearchCV(estimator=est, param_grid=est_params, iid=False, cv=4, \n","                          verbose=1, n_jobs=-1, scoring='roc_auc')\n","      gscv.fit(train_features, train_labels[:, i])\n","      print(\"Best score for the model\\n{}\\nis:{}\".format(gscv.best_estimator_, gscv.best_score_))\n","\n","      if gscv.best_score_ > best_score:\n","        best_score = gscv.best_score_\n","        best_model = gscv.best_estimator_ \n","    \n","    best_model.fit(train_features, train_labels[:, i])\n","    submission[class_name] = best_model.predict_proba(test_features)[:, 1]\n","\n","    print(\"\\nBest score for the class {} is {}\\n\".format(class_name, best_score))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["**Processing toxic comments...**\n"," **Processing RandomForestClassifier model...**\n","Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"SQeEcE3N8oL8","colab_type":"code","trusted":true,"colab":{}},"source":["submission.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ujanlBVs61IJ","colab_type":"text"},"source":["## Cheaty evaluating on test labels"]},{"cell_type":"code","metadata":{"id":"YUF4Uxrl8pXr","colab_type":"code","trusted":true,"colab":{}},"source":["# labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)\n","test_labels_df = pd.read_csv(test_labels_filepath)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6crgLcLx7X-p","colab_type":"code","trusted":true,"colab":{}},"source":["test_labels_df = test_labels_df[(test_labels_df[\"toxic\"] != -1) &\n","                                (test_labels_df[\"severe_toxic\"] != -1) &\n","                                (test_labels_df[\"obscene\"] != -1) &\n","                                (test_labels_df[\"threat\"] != -1) &\n","                                (test_labels_df[\"insult\"] != -1) &\n","                                (test_labels_df[\"identity_hate\"] != -1)]\n","test_labels_df.shape                               "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ymumFOQ9sA1","colab_type":"code","trusted":true,"colab":{}},"source":["submission_to_evaluate = submission[submission['id'].isin(test_labels_df['id'].values)]\n","submission_to_evaluate.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C3r8VTCK_TaM","colab_type":"code","trusted":true,"colab":{}},"source":["scores = []\n","for class_name in TARGET_COLS:\n","    train_target = test_labels_df[class_name]\n","    train_predicted = submission_to_evaluate[class_name]\n","\n","    cv_score = np.mean(roc_auc_score(train_target.values, train_predicted.values))\n","    scores.append(cv_score)\n","    print('CV score for class {} is {}'.format(class_name, cv_score))\n","\n","print('Total CV score is {}'.format(np.mean(scores)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"YKsSnwkd3TBJ","colab_type":"code","colab":{}},"source":["submission.to_csv(\"submission_tfidf_sklean.csv\", index = False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0o-WGik8qtjo","colab_type":"text"},"source":["TFIDF_+_logreg  0.97620"]}]}