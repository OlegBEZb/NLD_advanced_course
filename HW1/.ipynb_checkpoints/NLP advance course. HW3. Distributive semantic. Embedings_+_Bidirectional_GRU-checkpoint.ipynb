{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/OlegBEZb/NLP_advanced_course/blob/master/HW1/NLP%20advance%20course.%20HW3.%20Distributive%20semantic.%20Embedings_%2B_Bidirectional_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVRaU_zG-PaI"
   },
   "source": [
    "## Embeddings to check\n",
    "\n",
    "* Word Embeddings (word2vec, GloVe, etc.)\n",
    "* Starspace and other similarity learning embeddings\n",
    "* char-gram embeddings (bpemb, fasttext etc.)\n",
    "* doc and sentence embeddings (doc2vec, sent2vec etc.)\n",
    "* specific context models (BERT, ELMo)\n",
    "* Advanced: Poincare Embeddings concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30D0Scxk4kn3"
   },
   "source": [
    "# Libs import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "ZnXUdrxQ3TAN",
    "outputId": "6f3ff212-8218-4994-91a8-6a3f5d95eee0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "Collecting Unidecode\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n",
      "\u001b[?25hInstalling collected packages: Unidecode\n",
      "Successfully installed Unidecode-1.1.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import os, csv, random, pickle, re, sys\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "! pip install Unidecode;\n",
    "from unidecode import unidecode\n",
    "#! pip install pyspellchecker;\n",
    "#from spellchecker import SpellChecker\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
    "from tensorflow.keras.layers import Activation, Dropout, CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, CuDNNGRU\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, Callback\n",
    "\n",
    "import tensorflow as tf\n",
    "%tensorflow_version 1.x\n",
    "print(tf.__version__)\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijseJsWUosmg"
   },
   "source": [
    "TODO: check if we can no to use this sess and use only default session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wi85ywI6MouM"
   },
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ypZqfsNI6-Ne"
   },
   "source": [
    "# Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3JtxOKtA35gA",
    "outputId": "d2664afc-e233-4389-c6a7-3c15742f3763"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    "):\n",
    "    print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VD2z8D2V7Dod"
   },
   "source": [
    "# Authorization on Google drive and configurings paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okymGz_lEjLC"
   },
   "outputs": [],
   "source": [
    "WORKSPACE = 'COLAB' # or 'KAGGLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "auzLnrdK51Rg",
    "outputId": "52318ebd-6f43-4212-d77e-7735426d2e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n",
      "data found: ['test.csv', 'test_labels.csv', 'train.csv', 'sample_submission.csv', 'submission.csv', 'submission_emb_sklearn.csv', 'submission_bidirectional_GRU.csv', 'train_labels.csv.npy', 'train_labels.npy', 'submission_fine_tuned_bert.csv']\n"
     ]
    }
   ],
   "source": [
    "if WORKSPACE == 'COLAB':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    homework_folder = os.path.join('/content/drive/My Drive', 'Advanced NLP/Homework 1: Classical classification task like Kaggle Toxic or Quora')\n",
    "    data_folder = os.path.join(homework_folder, 'Toxic data')\n",
    "    embeddings_folder = os.path.join(homework_folder, 'embeddings')\n",
    "    output_folder = os.path.join(homework_folder, 'output')\n",
    "elif WORKSPACE == 'KAGGLE':\n",
    "    data_folder = '../input/jigsaw-toxic-comment-classification-challenge/'\n",
    "    embeddings_folder = '../input/glove-global-vectors-for-word-representation'\n",
    "else: # TODO: add computing on premise\n",
    "    pass\n",
    "\n",
    "print('data found:', os.listdir(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYdAXsdT3TAd"
   },
   "outputs": [],
   "source": [
    "train_filepath = os.path.join(data_folder,\"train.csv\")\n",
    "test_filepath = os.path.join(data_folder,\"test.csv\")\n",
    "test_labels_filepath = os.path.join(data_folder,\"test_labels.csv\")\n",
    "\n",
    "#path to a sample submission\n",
    "submission_path = os.path.join(data_folder,\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15GVrJYa717o"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "jYljyuKTCm8r"
   },
   "source": [
    "## Dicts and lists of useful words and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "YjL5O731CtDD"
   },
   "outputs": [],
   "source": [
    "#List of some words that often appear in toxic comments\n",
    "#Sorry about the level of toxicity in it!\n",
    "toxic_words = [\n",
    "    \"poop\", \"crap\", \"prick\", \"twat\", \"wikipedia\", \"wiki\", \"hahahahaha\", \"lol\",\n",
    "    \"bastard\", \"sluts\", \"slut\", \"douchebag\", \"douche\", \"blowjob\", \"nigga\",\n",
    "    \"dumb\", \"jerk\", \"wanker\", \"wank\", \"penis\", \"motherfucker\", \"fucker\", \"fuk\",\n",
    "    \"fucking\", \"fucked\", \"fuck\", \"bullshit\", \"shit\", \"stupid\", \"bitches\",\n",
    "    \"bitch\", \"suck\", \"cunt\", \"dick\", \"cocks\", \"cock\", \"die\", \"kill\", \"gay\",\n",
    "    \"jewish\", \"jews\", \"jew\", \"niggers\", \"nigger\", \"faggot\", \"fag\", \"asshole\"\n",
    "]\n",
    "# todo: convert to dict and use in normalising by dict\n",
    "astericks_words = [('mother****ers', 'motherfuckers'),\n",
    "                   ('motherf*cking', 'motherfucking'),\n",
    "                   ('mother****er', 'motherfucker'),\n",
    "                   ('motherf*cker', 'motherfucker'), ('bullsh*t', 'bullshit'),\n",
    "                   ('f**cking', 'fucking'), ('f*ucking', 'fucking'),\n",
    "                   ('fu*cking', 'fucking'), ('****ing', 'fucking'),\n",
    "                   ('a**hole', 'asshole'), ('assh*le', 'asshole'),\n",
    "                   ('f******', 'fucking'), ('f*****g', 'fucking'),\n",
    "                   ('f***ing', 'fucking'), ('f**king', 'fucking'),\n",
    "                   ('f*cking', 'fucking'), ('fu**ing', 'fucking'),\n",
    "                   ('fu*king', 'fucking'), ('fuc*ers', 'fuckers'),\n",
    "                   ('f*****', 'fucking'), ('f***ed', 'fucked'),\n",
    "                   ('f**ker', 'fucker'), ('f*cked', 'fucked'),\n",
    "                   ('f*cker', 'fucker'), ('f*ckin', 'fucking'),\n",
    "                   ('fu*ker', 'fucker'), ('fuc**n', 'fucking'),\n",
    "                   ('ni**as', 'niggas'), ('b**ch', 'bitch'),\n",
    "                   ('b*tch', 'bitch'), ('c*unt', 'cunt'), ('f**ks', 'fucks'),\n",
    "                   ('f*ing', 'fucking'), ('ni**a', 'nigga'), ('c*ck', 'cock'),\n",
    "                   ('c*nt', 'cunt'), ('cr*p', 'crap'), ('d*ck', 'dick'),\n",
    "                   ('f***', 'fuck'), ('f**k', 'fuck'), ('f*ck', 'fuck'),\n",
    "                   ('fc*k', 'fuck'), ('fu**', 'fuck'), ('fu*k', 'fuck'),\n",
    "                   ('s***', 'shit'), ('s**t', 'shit'), ('sh**', 'shit'),\n",
    "                   ('sh*t', 'shit'), ('tw*t', 'twat')]\n",
    "fasttext_misspelings = {\n",
    "    \"'n'balls\": 'balls',\n",
    "    \"-nazi's\": 'nazis',\n",
    "    'adminabuse': 'admin abuse',\n",
    "    \"admins's\": 'admins',\n",
    "    'arsewipe': 'arse wipe',\n",
    "    'assfack': 'asshole',\n",
    "    'assholifity': 'asshole',\n",
    "    'assholivity': 'asshole',\n",
    "    'asshoul': 'asshole',\n",
    "    'asssholeee': 'asshole',\n",
    "    'belizeans': 'mexicans',\n",
    "    \"blowing's\": 'blowing',\n",
    "    'bolivians': 'mexicans',\n",
    "    'celtofascists': 'fascists',\n",
    "    'censorshipmeisters': 'censor',\n",
    "    'chileans': 'mexicans',\n",
    "    'clerofascist': 'fascist',\n",
    "    'cowcrap': 'crap',\n",
    "    'crapity': 'crap',\n",
    "    \"d'idiots\": 'idiots',\n",
    "    'deminazi': 'nazi',\n",
    "    'dftt': \"don't feed the troll\",\n",
    "    'dildohs': 'dildo',\n",
    "    'dramawhores': 'drama whores',\n",
    "    'edophiles': 'pedophiles',\n",
    "    'eurocommunist': 'communist',\n",
    "    'faggotkike': 'faggot',\n",
    "    'fantard': 'retard',\n",
    "    'fascismnazism': 'fascism',\n",
    "    'fascistisized': 'fascist',\n",
    "    'favremother': 'mother',\n",
    "    'fuxxxin': 'fucking',\n",
    "    \"g'damn\": 'goddamn',\n",
    "    'harassmentat': 'harassment',\n",
    "    'harrasingme': 'harassing me',\n",
    "    'herfuc': 'motherfucker',\n",
    "    'hilterism': 'fascism',\n",
    "    'hitlerians': 'nazis',\n",
    "    'hitlerites': 'nazis',\n",
    "    'hubrises': 'pricks',\n",
    "    'idiotizing': 'idiotic',\n",
    "    'inadvandals': 'vandals',\n",
    "    \"jackass's\": 'jackass',\n",
    "    'jiggabo': 'nigga',\n",
    "    'jizzballs': 'jizz balls',\n",
    "    'jmbass': 'dumbass',\n",
    "    'lejittament': 'legitimate',\n",
    "    \"m'igger\": 'nigger',\n",
    "    \"m'iggers\": 'niggers',\n",
    "    'motherfacking': 'motherfucker',\n",
    "    'motherfuckenkiwi': 'motherfucker',\n",
    "    'muthafuggas': 'niggas',\n",
    "    'nazisms': 'nazis',\n",
    "    'netsnipenigger': 'nigger',\n",
    "    'niggercock': 'nigger',\n",
    "    'niggerspic': 'nigger',\n",
    "    'nignog': 'nigga',\n",
    "    'niqqass': 'niggas',\n",
    "    \"non-nazi's\": 'not a nazi',\n",
    "    'panamanians': 'mexicans',\n",
    "    'pedidiots': 'idiots',\n",
    "    'picohitlers': 'hitler',\n",
    "    'pidiots': 'idiots',\n",
    "    'poopia': 'poop',\n",
    "    'poopsies': 'poop',\n",
    "    'presumingly': 'obviously',\n",
    "    'propagandaanddisinformation': 'propaganda and disinformation',\n",
    "    'propagandaministerium': 'propaganda',\n",
    "    'puertoricans': 'mexicans',\n",
    "    'puertorricans': 'mexicans',\n",
    "    'pussiest': 'pussies',\n",
    "    'pussyitis': 'pussy',\n",
    "    'rayaridiculous': 'ridiculous',\n",
    "    'redfascists': 'fascists',\n",
    "    'retardzzzuuufff': 'retard',\n",
    "    \"revertin'im\": 'reverting',\n",
    "    'scumstreona': 'scums',\n",
    "    'southamericans': 'mexicans',\n",
    "    'strasserism': 'fascism',\n",
    "    'stuptarded': 'retarded',\n",
    "    \"t'nonsense\": 'nonsense',\n",
    "    \"threatt's\": 'threat',\n",
    "    'titoists': 'communists',\n",
    "    'twatbags': 'douchebags',\n",
    "    'youbollocks': 'you bollocks'\n",
    "}\n",
    "acronym_words = {\n",
    "    \"btw\": \"by the way\",\n",
    "    \"yo\": \"you\",\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"ur\": \"your\",\n",
    "    \"ima\": \"i am going to\",\n",
    "    \"imma\": \"i am going to\",\n",
    "    \"i'ma\": \"i am going to\",\n",
    "    \"cos\": \"because\",\n",
    "    \"coz\": \"because\",\n",
    "    \"stfu\": \"shut the fuck up\",\n",
    "    \"wat\": \"what\"\n",
    "}\n",
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
    "letter_replacement = {\"$\": \"s\", \"@\": \"a\"}\n",
    "unspaced_signs = {\"!\": \" ! \", \"?\": \" ? \"}\n",
    "url_replace_dict = {\n",
    "    \"http://\": '',\n",
    "    \"www.\": '',\n",
    "    \"https://\": '',\n",
    "    \"wikipedia.org\": ''\n",
    "}\n",
    "#apparently, people on wikipedia love to talk about sockpuppets :-)\n",
    "sockpuppets_dict = {\"sock puppet\": \"sockpuppet\", \"SOCK PUPPET\": \"SOCKPUPPET\"}\n",
    "\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#spell_checker = SpellChecker()\n",
    "\n",
    "cont_patterns = [\n",
    "    (r'(W|w)on\\'t', r'will not'),\n",
    "    (r'(C|c)an\\'t', r'can not'),\n",
    "    (r'(I|i)\\'m', r'i am'),\n",
    "    (r'(A|a)in\\'t', r'is not'),\n",
    "    (r'(\\w+)\\'ll', r'\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', r'\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', r'\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', r'\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', r'\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', r'\\g<1> would'),\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "#We will filter all characters except alphabet characters and some punctuation\n",
    "valid_characters = \" \" + \"@$\" + \"'!?-\" + \"abcdefghijklmnopqrstuvwxyz\" + \"abcdefghijklmnopqrstuvwxyz\".upper(\n",
    ")\n",
    "valid_set = set(x for x in valid_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cs_nlu1iILY1"
   },
   "source": [
    "## Funcs for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GfUaIP-wIKTZ"
   },
   "outputs": [],
   "source": [
    "def replace_in_sentence(sentence, dictionary):\n",
    "    for key in dictionary.keys():\n",
    "        if key in sentence:\n",
    "            sentence = sentence.replace(key, dictionary[key])\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def split_toxic_and_normal_parts(word, toxic_words):\n",
    "    if word == \"\":\n",
    "        return \"\"\n",
    "\n",
    "    lower = word.lower()\n",
    "    for toxic_word in toxic_words:\n",
    "        start = lower.find(toxic_word)\n",
    "        if start >= 0:\n",
    "            end = start + len(toxic_word)\n",
    "            result = \" \".join([\n",
    "                word[0:start], word[start:end],\n",
    "                split_toxic_and_normal_parts(word[end:], toxic_words)\n",
    "            ])\n",
    "            return result.replace(\"  \", \" \").strip()\n",
    "    return word\n",
    "\n",
    "\n",
    "def normalize_by_dictionary(normalized_word, dictionary):\n",
    "    \"\"\"\n",
    "    Word to be normalized (could be a phrase) with dictionary and the dictionary passed.\n",
    "    Replaces each word in passed word-phrase into their representations in dictionary.\n",
    "    Uppercased words are matched in lowercase but returned in upper. Other cases\n",
    "    are checked in lower and returned in lower.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for word in normalized_word.split():\n",
    "        if word == word.upper():\n",
    "            if word.lower() in dictionary:\n",
    "                result.append(dictionary[word.lower()].upper())\n",
    "            else:\n",
    "                result.append(word)\n",
    "        else:\n",
    "            #print('word', '\\'', word, '\\'', 'is not upper')\n",
    "            #print('word.lower()', word.lower())\n",
    "            if word.lower() in dictionary:\n",
    "                result.append(dictionary[word.lower()])\n",
    "            else:\n",
    "                result.append(word)\n",
    "\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "def normalize_comment(comment, tokenizer, max_chars_in_comment):\n",
    "    comment = unidecode(comment)\n",
    "    comment = comment[:max_chars_in_comment]\n",
    "\n",
    "    comment = replace_in_sentence(comment, letter_replacement)\n",
    "    comment = replace_in_sentence(comment, unspaced_signs)\n",
    "    comment = replace_in_sentence(comment, url_replace_dict)\n",
    "    comment = replace_in_sentence(comment, sockpuppets_dict)\n",
    "\n",
    "    # ('mother****ers', 'motherfuckers')\n",
    "    # for w in astericks_words:\n",
    "    #     if w[0] in comment:\n",
    "    #         comment = comment.replace(w[0], w[1])\n",
    "    #     if w[0].upper() in comment:\n",
    "    #         comment = comment.replace(w[0].upper(), w[1].upper())\n",
    "\n",
    "    normalized_words = []\n",
    "    for word in tokenizer.tokenize(comment):\n",
    "        if word in english_stopwords: continue\n",
    "\n",
    "        # # '(W|w)on\\'t', r'will not'\n",
    "        # for (pattern, repl) in patterns:\n",
    "        #    word = re.sub(pattern, repl, word)\n",
    "\n",
    "        if word == \".\" or word == \",\":\n",
    "            normalized_words.append(word)\n",
    "            continue\n",
    "\n",
    "        # drop url parts from links\n",
    "        # word = replace_url(word)\n",
    "\n",
    "        # replace single dots to whitespaces\n",
    "        if word.count(\".\") == 1:\n",
    "            word = word.replace(\".\", \" \")\n",
    "\n",
    "        # leave only legalized symbols\n",
    "        filtered_word = \"\".join([x for x in word if x in valid_set])\n",
    "\n",
    "        #Kind of hack: for every word check if it has a toxic word as a part of it\n",
    "        #If so, split this word by swear and non-swear part.\n",
    "        #normalized_word = split_toxic_and_normal_parts(filtered_word, toxic_words)\n",
    "        normalized_word = filtered_word\n",
    "\n",
    "        #         normalized_word = normalize_by_dictionary(normalized_word, hyphens_dict)\n",
    "        #         normalized_word = normalize_by_dictionary(normalized_word, merged_dict)\n",
    "\n",
    "        # check misspellings\n",
    "        # temp = []\n",
    "        # for word in normalized_word.split():\n",
    "        #   temp.append(spell_checker.correction(word))\n",
    "        # normalized_word = \" \".join(temp)\n",
    "\n",
    "        # normalized_word = normalize_by_dictionary(normalized_word, fasttext_misspelings)\n",
    "        normalized_word = normalize_by_dictionary(normalized_word,\n",
    "                                                  acronym_words)\n",
    "\n",
    "        normalized_words.append(normalized_word)\n",
    "\n",
    "    normalized_comment = \" \".join(normalized_words)\n",
    "\n",
    "    result = []\n",
    "    for word in normalized_comment.split():\n",
    "        # if word is upper\n",
    "        if word.upper() == word:\n",
    "            result.append(word)\n",
    "        else:\n",
    "            result.append(word.lower())\n",
    "\n",
    "    result = \" \".join(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def preprocess_text(df, preprocessing_func, tokenizer, max_chars_in_comment):\n",
    "    # cheap and bad preprocessing\n",
    "    # preprocessed_df = df[TEXT_COLUMN].fillna(\"CVxTz\").values\n",
    "    # preprocessed_df = df[TEXT_COLUMN]\n",
    "\n",
    "    text_ndarray = df.fillna('_na').values\n",
    "    np_preprocessing_func = np.vectorize(preprocessing_func)\n",
    "\n",
    "    preprocessed_df = np_preprocessing_func(text_ndarray, tokenizer,\n",
    "                                            max_chars_in_comment)\n",
    "\n",
    "    print('Gained shape:', preprocessed_df.shape)\n",
    "    return preprocessed_df\n",
    "\n",
    "\n",
    "def tokenize_and_pad(text, tokenizer, max_words_in_comment):\n",
    "\n",
    "    tokenized_text = tokenizer.texts_to_sequences(text)\n",
    "    padded_text = pad_sequences(tokenized_text,\n",
    "                                maxlen=max_words_in_comment,\n",
    "                                dtype='int',\n",
    "                                value=0)\n",
    "\n",
    "    print('Gained shape:', padded_text.shape)\n",
    "    return padded_text\n",
    "\n",
    "\n",
    "def load_and_preprocess(filepath,\n",
    "                        preprocessing_func,\n",
    "                        tokenizer,\n",
    "                        max_chars_in_comment,\n",
    "                        get_labels=False):\n",
    "    df = pd.read_csv(filepath)\n",
    "    preprocessed_df = preprocess_text(\n",
    "        df=df[TEXT_COLUMN],\n",
    "        preprocessing_func=preprocessing_func,\n",
    "        tokenizer=tokenizer,\n",
    "        max_chars_in_comment=max_chars_in_comment)\n",
    "    if get_labels:\n",
    "        labels = df[TARGET_COLS].values\n",
    "        return preprocessed_df, labels\n",
    "    else:\n",
    "        return preprocessed_df\n",
    "\n",
    "\n",
    "def preproc2tokenized(preprocessed_df,\n",
    "                      max_tokens,\n",
    "                      max_words_in_comment,\n",
    "                      tokenizer=None):\n",
    "    # keras text tokenizer. simple, only for splitting and padding\n",
    "    if not tokenizer:\n",
    "        tokenizer = Tokenizer(num_words=MAX_TOKENS)\n",
    "        tokenizer.fit_on_texts(list(preprocessed_df))\n",
    "    tokenized_df = tokenize_and_pad(preprocessed_df, tokenizer,\n",
    "                                    max_words_in_comment)\n",
    "    return tokenized_df, tokenizer\n",
    "\n",
    "\n",
    "def get_preprocessed_dfs(train_filepath,\n",
    "                         test_filepath,\n",
    "                         preprocessing_func,\n",
    "                         max_chars_in_comment,\n",
    "                         max_tokens,\n",
    "                         max_words_in_comment,\n",
    "                         return_tokenized=False):\n",
    "\n",
    "    preprocessing_tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "    preprocessed_train, train_labels = load_and_preprocess(\n",
    "        train_filepath,\n",
    "        preprocessing_func,\n",
    "        preprocessing_tknzr,\n",
    "        max_chars_in_comment,\n",
    "        get_labels=True)\n",
    "    preprocessed_test = load_and_preprocess(test_filepath, preprocessing_func,\n",
    "                                            preprocessing_tknzr,\n",
    "                                            max_chars_in_comment)\n",
    "\n",
    "    if return_tokenized:\n",
    "        tokenized_train, tokenizer = preproc2tokenized(\n",
    "            preprocessed_train,\n",
    "            max_tokens=max_tokens,\n",
    "            max_words_in_comment=max_words_in_comment,\n",
    "            tokenizer=None)\n",
    "        tokenized_test, _ = preproc2tokenized(\n",
    "            preprocessed_test,\n",
    "            max_tokens=max_tokens,\n",
    "            max_words_in_comment=max_words_in_comment,\n",
    "            tokenizer=tokenizer)\n",
    "        return tokenized_train, tokenized_test, train_labels\n",
    "    else:\n",
    "        return preprocessed_train, preprocessed_test, train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tn1-XTZEIZ3y"
   },
   "source": [
    "## Loading frames and processing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AWRdxZsHG3B"
   },
   "outputs": [],
   "source": [
    "TEXT_COLUMN = 'comment_text'\n",
    "TARGET_COLS = [\n",
    "    \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"\n",
    "]\n",
    "\n",
    "MAX_WORDS_IN_COMMENT = 220\n",
    "MAX_TOKENS = 20000\n",
    "MAX_CHARS_IN_COMMENT = 20000  #We are going to truncate a comment if its length > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "z54MJ5i2i-UA",
    "outputId": "44151f8e-3a3d-4e4d-f8fe-92308ed63f9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gained shape: (159571,)\n",
      "Gained shape: (153164,)\n",
      "[\"explanation why edits made username hardcore metallica fan reverted ? they vandalisms , closure gas I voted new york dolls FAC . and please remove template talk page since i'm retired . .\"\n",
      " \"d'aww ! he matches background colour i'm seemingly stuck . thanks . talk , january , UTC\"\n",
      " \"hey man , i'm really trying edit war . it's guy constantly removing relevant information talking edits instead talk page . he seems care formatting actual info .\"]\n",
      "CPU times: user 1min 54s, sys: 5.97 s, total: 2min\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preprocessed_train, preprocessed_test, train_labels = get_preprocessed_dfs(\n",
    "    train_filepath,\n",
    "    test_filepath,\n",
    "    normalize_comment,\n",
    "    max_chars_in_comment=MAX_CHARS_IN_COMMENT,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    max_words_in_comment=MAX_WORDS_IN_COMMENT,\n",
    "    return_tokenized=False)\n",
    "gc.collect()\n",
    "\n",
    "print(preprocessed_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks for keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTf9q8JGsc5G"
   },
   "outputs": [],
   "source": [
    "# class RocAucEvaluation(Callback):\n",
    "#     def __init__(self, validation_data=(), interval=1):\n",
    "#         super(Callback, self).__init__()\n",
    "\n",
    "#         self.interval = interval\n",
    "#         self.X_val, self.y_val = validation_data\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         if epoch % self.interval == 0:\n",
    "#             y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "#             score = 0\n",
    "#             for i in range(6):\n",
    "#              score += roc_auc_score(self.y_val[:,i], y_pred[:,i])/6.\n",
    "#             print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
    "\n",
    "# RocAuc_val = RocAucEvaluation(validation_data=(x_valid, y_valid), interval = 1)\n",
    "\n",
    "\n",
    "class RocAucEarlyStopping(Callback):\n",
    "    \"\"\"Callback for early stopping based on roc auc on the validation set\"\"\"\n",
    "    def __init__(self, validation_data, patience=0, digits=3):\n",
    "        super().__init__()\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.best = 0\n",
    "        self.patience = patience\n",
    "        self.digits = digits\n",
    "        self.current_patience = patience\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "        score = 0\n",
    "        for i in range(6):\n",
    "            score += roc_auc_score(self.y_val[:, i], y_pred[:, i]) / 6.\n",
    "\n",
    "        score = round(score, self.digits)\n",
    "        print(\"\\nROC-AUC - epoch: {:d} - score: {}\\n\".format(epoch + 1, score))\n",
    "\n",
    "        # check digits\n",
    "        if np.greater(score, self.best):\n",
    "            self.best = score\n",
    "            self.current_patience = self.patience\n",
    "        else:\n",
    "            print('\\nbest:{}\\ncurrent:{}'.format(self.best, score))\n",
    "            self.current_patience -= 1\n",
    "            if self.current_patience < 0:\n",
    "                self.model.stop_training = True\n",
    "                print('Early stopping due to lower roc auc')\n",
    "                self.current_patience = self.patience\n",
    "            else:\n",
    "                print('{} patience remained'.format(self.current_patience))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8APgvUyskaf"
   },
   "source": [
    "# Preparing features and training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Embedding matrix and Bidirectional_GRU model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "wCRY8qnNJTh_"
   },
   "source": [
    "### Preparing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "hidden": true,
    "id": "3jqSUK8Dpd3Q",
    "outputId": "3d43d16b-2699-43e9-ebd9-ca298ab894c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 184924 unique tokens.\n",
      "embedding matrix shape (20000, 1536)\n"
     ]
    }
   ],
   "source": [
    "embedding_len = 1536\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' %len(word_index))\n",
    "\n",
    "#del tokenizer\n",
    "gc.collect()\n",
    "\n",
    "num_words = min(MAX_TOKENS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_len)) # zeroth row for UNK\n",
    "print('embedding matrix shape', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "GRh7ig80jk3s"
   },
   "source": [
    "#### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "BNjZiXPZEYvX"
   },
   "outputs": [],
   "source": [
    "# embedding_len = 200\n",
    "# words = pd.read_csv(os.path.join(embeddings_folder, \"glove.twitter.27B.{}d.txt\".format(embedding_len)), \n",
    "#                     sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "# embedding_len = 300\n",
    "# words = pd.read_csv(os.path.join(embeddings_folder, \"glove.840B.300d.txt\"), \n",
    "#                     sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "\n",
    "# vocab_words = words[words.index.isin(list(word_index.keys()))]\n",
    "# print('vocab_words.shape', vocab_words.shape)\n",
    "\n",
    "# del words\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "m0-ZU5VYq-Kh"
   },
   "outputs": [],
   "source": [
    "# def get_vec(word, words_df):\n",
    "#     return words_df.loc[word].values\n",
    "\n",
    "\n",
    "\n",
    "# for word, i in word_index.items():\n",
    "#   if i >= max_tokens:\n",
    "#     continue\n",
    "#   try:\n",
    "#     embedding_vector = get_vec(word, vocab_words)\n",
    "#   except:\n",
    "#     continue\n",
    "#   if embedding_vector is not None:\n",
    "#     # words not found in embedding index will be all-zeros.\n",
    "#     embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# # %%time\n",
    "# # embedding_matrix = np.load('../input/embedding-2/embedding_matrix_big.npy')\n",
    "    \n",
    "# print('embedding_matrix.shape', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "1_nWe7O-jqak"
   },
   "source": [
    "#### StarSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "OD8SO8xXfsVz"
   },
   "outputs": [],
   "source": [
    "# Starspace attempt\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_vectorizer = CountVectorizer(min_df=0, max_df=0.99, max_features=10000)\n",
    "# X_train = count_vectorizer.fit_transform(article_contents.main_content.iloc[0:train_row])\n",
    "# X_train = count_vectorizer.inverse_transform(X_train)\n",
    "# label_prefix = '__label__'\n",
    "# with open(\"train_starspace.txt\", 'w+') as file:\n",
    "#     for i in range(10):\n",
    "#         file.write(' '.join(preprocessed_train[i].split(' ')) + ' ' + label_prefix + train_labels[i])\n",
    "#         file.write('\\n')\n",
    "# file.close()\n",
    "\n",
    "# The result file will look like this (all separeted by space, and label will have prefix __label__)\n",
    "# how are you ... __label__b\n",
    "# this is just an example ... __label__c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "TGzxJch3juGy"
   },
   "source": [
    "#### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "hidden": true,
    "id": "voep54c6Q-I-",
    "outputId": "64afe0b8-56c3-40e5-87f5-7372096ffc2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (42.0.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.17.4)\n",
      "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "SBeehRN-oS9V"
   },
   "outputs": [],
   "source": [
    "# import codecs\n",
    "\n",
    "# #load embeddings\n",
    "# print('loading word embeddings...')\n",
    "# embeddings_index = {}\n",
    "# f = codecs.open(os.path.join(embeddings_folder, 'crawl-300d-2M-subword.vec'), encoding='utf-8')\n",
    "# for line in tqdm(f, mininterval=5):\n",
    "#     values = line.rstrip().rsplit(' ')\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "# print('found %s word vectors' % len(embeddings_index))\n",
    "# print('preparing embedding matrix...')\n",
    "# words_not_found = []\n",
    "\n",
    "\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= max_tokens:\n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "#     else:\n",
    "#         words_not_found.append(word)\n",
    "# print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "\n",
    "# words_not_found[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "sWmhQEFAj1VE"
   },
   "source": [
    "#### BPEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "MdMBN-HPjjek"
   },
   "outputs": [],
   "source": [
    "# !pip install bpemb;\n",
    "# from bpemb import BPEmb\n",
    "# # load English BPEmb model with default vocabulary size of max_tokens\n",
    "# bpemb_en = BPEmb(lang=\"en\", dim=embedding_len, vs=max_tokens)\n",
    "# bpemb_en.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "6JQn_7WjrNP1"
   },
   "outputs": [],
   "source": [
    "# # mean vector of word parts\n",
    "# def get_vec(word):\n",
    "#     vec = bpemb_en.embed(word)\n",
    "#     return vec.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "hidden": true,
    "id": "jSAJK0AYsdW2",
    "outputId": "bd9383ce-edd7-4c2f-b258-1dea1d46675c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix.shape (20000, 300)\n",
      "CPU times: user 504 ms, sys: 18.1 ms, total: 522 ms\n",
      "Wall time: 520 ms\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# for word, i in word_index.items():\n",
    "#   if i >= max_tokens:\n",
    "#     continue\n",
    "#   try:\n",
    "#     embedding_vector = get_vec(word)\n",
    "#   except:\n",
    "#     continue\n",
    "#   if embedding_vector is not None:\n",
    "#     # words not found in embedding index will be all-zeros.\n",
    "#     embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# print('embedding_matrix.shape', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "W2ezEW492Nnj"
   },
   "source": [
    "#### Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "hidden": true,
    "id": "ScKTCXs-trr_",
    "outputId": "01af3e73-bbbf-4cbf-9253-3a2dd3befe54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiny-tokenizer\n",
      "  Downloading https://files.pythonhosted.org/packages/8d/0f/aa52c227c5af69914be05723b3deaf221805a4ccbce87643194ef2cdde43/tiny_tokenizer-3.1.0.tar.gz\n",
      "Building wheels for collected packages: tiny-tokenizer\n",
      "  Building wheel for tiny-tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tiny-tokenizer: filename=tiny_tokenizer-3.1.0-cp36-none-any.whl size=10550 sha256=aa15269699b787314d1c790861bba985df9096ce9ad75c95d32baf09b8b5ec6d\n",
      "  Stored in directory: /root/.cache/pip/wheels/d1/c8/36/334497a689fab90128232e86b5829b800dd271a3d5d5959c53\n",
      "Successfully built tiny-tokenizer\n",
      "Installing collected packages: tiny-tokenizer\n",
      "Successfully installed tiny-tokenizer-3.1.0\n",
      "Collecting flair\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/22/8fc8e5978ec05b710216735ca47415700e83f304dec7e4281d61cefb6831/flair-0.4.4-py3-none-any.whl (193kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 2.8MB/s \n",
      "\u001b[?25hCollecting bpemb>=0.2.9\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
      "Collecting transformers>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 48.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
      "Collecting langdetect\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 51.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from flair) (3.10.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.6)\n",
      "Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
      "Collecting segtok>=1.5.7\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.2)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from flair) (0.2.0)\n",
      "Requirement already satisfied: tiny-tokenizer[all] in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.0)\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Downloading https://files.pythonhosted.org/packages/f6/89/62912e01f3cede11edcc0abf81298e3439d9c06c8dce644369380ed13f6d/Deprecated-1.2.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
      "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.3.1)\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from flair) (0.4.2)\n",
      "Collecting ipython==7.6.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/2c/c7d44277b599df35af734d8f4142d501192fdb7aef5d04daf882d7eccfbc/ipython-7.6.1-py3-none-any.whl (774kB)\n",
      "\u001b[K     |████████████████████████████████| 778kB 35.3MB/s \n",
      "\u001b[?25hCollecting mpld3==0.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
      "\u001b[K     |████████████████████████████████| 798kB 43.1MB/s \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 35.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (2.21.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (1.17.5)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (1.10.47)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 47.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.22.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->flair) (1.12.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (42.0.2)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (8.0.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.3.0)\n",
      "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.6.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
      "Collecting SudachiDict-core@ https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_core-20190927.tar.gz ; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_core-20190927.tar.gz (70.7MB)\n",
      "\u001b[K     |████████████████████████████████| 70.7MB 44kB/s \n",
      "\u001b[?25hCollecting SudachiPy; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/c9/40bfb291a7995ad218451ef97083432f998b822e3ecbd9f586f593d2cfb6/SudachiPy-0.4.2-py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 9.2MB/s \n",
      "\u001b[?25hCollecting janome; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/f0/bd7f90806132d7d9d642d418bdc3e870cfdff5947254ea3cab27480983a7/Janome-0.3.10-py2.py3-none-any.whl (21.5MB)\n",
      "\u001b[K     |████████████████████████████████| 21.5MB 158kB/s \n",
      "\u001b[?25hCollecting natto-py; extra == \"all\"\n",
      "  Downloading https://files.pythonhosted.org/packages/f1/14/1d4258247a00b7b8a115563effb1d0bd30501d69580629d36593ce0af92d/natto-py-0.9.2.tar.gz\n",
      "Collecting kytea; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/bc/702d01a96d5d094bd9f3c2eb1d12153daf8edf7bf5d78b9a2dae1202df07/kytea-0.1.4-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 30.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.4.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.2)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.9.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->flair) (6.2.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.4.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.1.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (2.1.3)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.7.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.7.5)\n",
      "Collecting prompt-toolkit<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/61/2dfea88583d5454e3a64f9308a686071d58d59a55db638268a6413e1eb6d/prompt_toolkit-2.0.10-py3-none-any.whl (340kB)\n",
      "\u001b[K     |████████████████████████████████| 348kB 44.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.15.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.3.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (3.0.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (1.13.47)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.9.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (0.14.1)\n",
      "Collecting dartsclone~=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/4d/45acbe9d0795d8ceef0fee1f9ac2dcbf27dca3a0578a023fcdc3fef6fd89/dartsclone-0.6.tar.gz\n",
      "Requirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (2.1.0)\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (1.13.2)\n",
      "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython==7.6.1->flair) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.6.1->flair) (0.1.8)\n",
      "Requirement already satisfied: parso>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython==7.6.1->flair) (0.5.2)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers>=2.0.0->flair) (0.15.2)\n",
      "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.6.0->SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (0.29.14)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (2.19)\n",
      "Building wheels for collected packages: langdetect, segtok, sqlitedict, mpld3, sacremoses, SudachiDict-core, natto-py, dartsclone\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.7-cp36-none-any.whl size=993460 sha256=65bda05c34cc4f7d2803f6893027a63e2645e041b68e14522d5c50dedfda6109\n",
      "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
      "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for segtok: filename=segtok-1.5.7-cp36-none-any.whl size=23258 sha256=a6068dca9b07d0d23c5b2578cd91ef376e73852632a166e378c7b110177d5e19\n",
      "  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=db59b3364116b169a671b96c7c5deb471b6d402d4b0645175eef90fb39422521\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
      "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=787a055e5a4ff87af3e8b3181f09441fbba207b4b392baa00e0a0b7d410c92ed\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=a9bc6ff7947fee6ea9cb980e90406dbce3648c3047162a4fd6e55e8e9c6fca29\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "  Building wheel for SudachiDict-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for SudachiDict-core: filename=SudachiDict_core-20190927-cp36-none-any.whl size=70878518 sha256=928042bfce672ea42c8ee0a4c92767c51249df8109d969ad92eeb9979adfd8fd\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/d8/6e/b107d7fef6e80915aa1e46db741b98a3da011f567526347ccc\n",
      "  Building wheel for natto-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for natto-py: filename=natto_py-0.9.2-cp36-none-any.whl size=45164 sha256=b575b15b9aef2c24bc8e25450742d03fcbf00a323ecc3ac97ccd477615c70910\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/51/dd/67f87608b124a23eecf5c1fc3557cc0b7ffdeae33fe6ee89df\n",
      "  Building wheel for dartsclone (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for dartsclone: filename=dartsclone-0.6-cp36-cp36m-linux_x86_64.whl size=413251 sha256=4b52a1db155cf8b637be04577552e255282764ba767ae7c523150b48026fbe49\n",
      "  Stored in directory: /root/.cache/pip/wheels/be/cd/70/fe43307bf7398243155108f4f5a258ef336923d65ec4af93cd\n",
      "Successfully built langdetect segtok sqlitedict mpld3 sacremoses SudachiDict-core natto-py dartsclone\n",
      "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.6.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: sentencepiece, bpemb, sacremoses, transformers, langdetect, segtok, deprecated, sqlitedict, prompt-toolkit, ipython, mpld3, flair, dartsclone, SudachiPy, SudachiDict-core, janome, natto-py, kytea\n",
      "  Found existing installation: prompt-toolkit 1.0.18\n",
      "    Uninstalling prompt-toolkit-1.0.18:\n",
      "      Successfully uninstalled prompt-toolkit-1.0.18\n",
      "  Found existing installation: ipython 5.5.0\n",
      "    Uninstalling ipython-5.5.0:\n",
      "      Successfully uninstalled ipython-5.5.0\n",
      "Successfully installed SudachiDict-core-20190927 SudachiPy-0.4.2 bpemb-0.3.0 dartsclone-0.6 deprecated-1.2.7 flair-0.4.4 ipython-7.6.1 janome-0.3.10 kytea-0.1.4 langdetect-1.0.7 mpld3-0.3 natto-py-0.9.2 prompt-toolkit-2.0.10 sacremoses-0.0.38 segtok-1.5.7 sentencepiece-0.1.85 sqlitedict-1.6.0 transformers-2.3.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "IPython",
         "prompt_toolkit"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "! pip install tiny-tokenizer\n",
    "! pip install flair\n",
    "\n",
    "import flair\n",
    "from flair.data import Sentence\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "_yDXCBmqtkPl"
   },
   "source": [
    "##### ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "iVZ4xx34vSmd"
   },
   "outputs": [],
   "source": [
    "! pip install allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "uNDAZuOvt4wR",
    "outputId": "306e7fb8-db28-4628-ea6e-c06cc0ae1183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding len 1536\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import ELMoEmbeddings\n",
    "\n",
    "# init embedding\n",
    "embedding = ELMoEmbeddings('medium')\n",
    "\n",
    "# create a sentence #\n",
    "sentence = Sentence('test sent')\n",
    "# embed words in sentence #\n",
    "embedding.embed(sentence)\n",
    "embedding_len = sentence[0].embedding.size()[0]\n",
    "print('embedding len', embedding_len)\n",
    "\n",
    "\n",
    "def get_comment_emb(comment):\n",
    "  sentence = Sentence(comment)\n",
    "  embedding.embed(sentence)\n",
    "  emb_list = []\n",
    "  for word in sentence:\n",
    "    emb_list.append(word.embedding)\n",
    "  return emb_list\n",
    "\n",
    "get_comment_emb_vec = np.vectorize(get_comment_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "FFD8Xd565HgQ",
    "outputId": "aaea15ab-6a6f-4f01-c47f-7d111ebd707c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184924/184924 [03:02<00:00, 1014.44it/s]\n"
     ]
    }
   ],
   "source": [
    "for word, i in tqdm(word_index.items(), total=len(word_index)):\n",
    "    if i >= max_tokens:\n",
    "        continue\n",
    "    try:\n",
    "        word_sent = Sentence(word)\n",
    "        embedding.embed(word_sent)\n",
    "        embedding_vector = word_sent[0].embedding.cpu().detach().numpy()\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "           embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        pass\n",
    "        #embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "F6tfsIpFJ6YG",
    "outputId": "c515518f-a7ce-4831-cf0e-713070b8baf6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "648"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "5I7VZVVF69Fh"
   },
   "outputs": [],
   "source": [
    "# # creating a tensor for storing sentence embeddings #\n",
    "# s = torch.zeros(0, embedding_len)\n",
    "\n",
    "# # iterating Sentence (tqdm tracks progress) #\n",
    "# for tweet in tqdm(preprocessed_train[10]):   \n",
    "#   # empty tensor for words #\n",
    "#   w = torch.zeros(0, embedding_len)   \n",
    "#   sentence = Sentence(tweet)\n",
    "#   embedding.embed(sentence)\n",
    "#   # for every word #\n",
    "#   for token in sentence:\n",
    "#     # storing word Embeddings of each word in a sentence #\n",
    "#     w = torch.cat((w,token.embedding.view(-1, embedding_len)),0)\n",
    "#   # storing sentence Embeddings (mean of embeddings of all words)   #\n",
    "#   s = torch.cat((s, w.mean(dim = 0).view(-1, embedding_len)),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "0hTlPvNY8gi1"
   },
   "source": [
    "##### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "1gEnMtmwooBE"
   },
   "outputs": [],
   "source": [
    "from flair.embeddings import RoBERTaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "CvXWeyYWo0J9",
    "outputId": "0c27d2a0-5e69-4428-fed4-20262dc73bc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding len 768\n"
     ]
    }
   ],
   "source": [
    "# init embedding\n",
    "embedding = RoBERTaEmbeddings('roberta-base')\n",
    "\n",
    "# create a sentence #\n",
    "sentence = Sentence('test sent')\n",
    "# embed words in sentence #\n",
    "embedding.embed(sentence)\n",
    "embedding_len = sentence[0].embedding.size()[0]\n",
    "print('embedding len', embedding_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "hidden": true,
    "id": "sDdLhAjJqY-G",
    "outputId": "5cf088ff-3fd9-4e44-f690-e37eb45c37db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184924/184924 [04:29<00:00, 685.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "648"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word, i in tqdm(word_index.items(), total=len(word_index)):\n",
    "    if i >= max_tokens:\n",
    "        continue\n",
    "    try:\n",
    "        word_sent = Sentence(word)\n",
    "        embedding.embed(word_sent)\n",
    "        embedding_vector = word_sent[0].embedding.cpu().detach().numpy()\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "           embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        pass\n",
    "        #embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "McJb1Sn5tw4a"
   },
   "source": [
    "##### RoBERTa + ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "ztnGoOlrt3PR"
   },
   "outputs": [],
   "source": [
    "! pip install allennlp\n",
    "from flair.embeddings import StackedEmbeddings, RoBERTaEmbeddings, ELMoEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "hidden": true,
    "id": "6_4YEiP1t5-n",
    "outputId": "7009dff1-41c1-45be-b19b-bae803e40e0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 336/336 [00:00<00:00, 742511.14B/s]\n",
      "100%|██████████| 54402456/54402456 [00:02<00:00, 20212502.71B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding len 1536\n"
     ]
    }
   ],
   "source": [
    "elmo_embeddings = ELMoEmbeddings('small')\n",
    "roberta_embeddings = RoBERTaEmbeddings('roberta-base')\n",
    "stacked_embeddings = StackedEmbeddings([\n",
    "                                        elmo_embeddings,\n",
    "                                        roberta_embeddings,\n",
    "                                       ])\n",
    "\n",
    "# create a sentence #\n",
    "sentence = Sentence('test sent')\n",
    "# embed words in sentence #\n",
    "stacked_embeddings.embed(sentence)\n",
    "embedding_len = sentence[0].embedding.size()[0]\n",
    "print('embedding len', embedding_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "MJNcWzMduAtb",
    "outputId": "c878817b-90ae-40ce-a383-cd2934362b5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184924/184924 [11:48<00:00, 260.89it/s]\n"
     ]
    }
   ],
   "source": [
    "for word, i in tqdm(word_index.items(), total=len(word_index)):\n",
    "    if i >= MAX_TOKENS:\n",
    "        continue\n",
    "    try:\n",
    "        word_sent = Sentence(word)\n",
    "        stacked_embeddings.embed(word_sent)\n",
    "        embedding_vector = word_sent[0].embedding.cpu().detach().numpy()\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "           embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        pass\n",
    "        #embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "aMl76miAyRyI",
    "outputId": "3f1eae01-4bf3-4e00-ec66-7ec4bf4c2c87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del stacked_embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "AslnHsTy0gCQ"
   },
   "source": [
    "### Build and fit the Bidirectional_GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "hidden": true,
    "id": "0m6GDvsd3WRX",
    "outputId": "fdac63b2-b009-4c48-a5dd-127d8436cdfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (143613, 220)\n",
      "x_valid shape: (15958, 220)\n",
      "y_train shape: (143613, 6)\n",
      "y_valid shape: (15958, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(tokenized_train,\n",
    "                                                      train_labels,\n",
    "                                                      test_size=0.1,\n",
    "                                                      shuffle=True)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_valid shape:', x_valid.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_valid shape:', y_valid.shape)\n",
    "\n",
    "del tokenized_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "GLOBAL_EPOCHS = 2\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "6306i8ha3TBF"
   },
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix):\n",
    "    words = Input(shape=(MAX_WORDS_IN_COMMENT,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    #x = Input(batch_shape=(batch_size, MAX_CHARS_IN_COMMENT, embedding_len))\n",
    "    \n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(len(TARGET_COLS), activation='sigmoid')(hidden)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=words, outputs=result)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "hidden": true,
    "id": "TRvdcnky3TBH",
    "outputId": "24f73e36-a368-4a42-fe8b-2bb0901d3766"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\n",
      "global_epoch 0\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/5\n",
      "143613/143613 [==============================] - 87s 605us/step - loss: 0.0700 - val_loss: 0.0497\n",
      "ROC-AUC - epoch: 1 - score: 0.9787\n",
      "\n",
      "Epoch 2/5\n",
      "143613/143613 [==============================] - 82s 569us/step - loss: 0.0473 - val_loss: 0.0446\n",
      "ROC-AUC - epoch: 2 - score: 0.9856\n",
      "\n",
      "Epoch 3/5\n",
      "143613/143613 [==============================] - 82s 569us/step - loss: 0.0429 - val_loss: 0.0445\n",
      "ROC-AUC - epoch: 3 - score: 0.9868\n",
      "\n",
      "Epoch 4/5\n",
      "143613/143613 [==============================] - 82s 568us/step - loss: 0.0402 - val_loss: 0.0428\n",
      "ROC-AUC - epoch: 4 - score: 0.9879\n",
      "\n",
      "Epoch 5/5\n",
      "143613/143613 [==============================] - 82s 569us/step - loss: 0.0382 - val_loss: 0.0447\n",
      "ROC-AUC - epoch: 5 - score: 0.9881\n",
      "\n",
      "fitted\n",
      "\n",
      "global_epoch 1\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/5\n",
      "143613/143613 [==============================] - 82s 569us/step - loss: 0.0340 - val_loss: 0.0440\n",
      "ROC-AUC - epoch: 1 - score: 0.9878\n",
      "\n",
      "\n",
      "best:0.9881\n",
      "current:0.9878\n",
      "0 patience remained\n",
      "Epoch 2/5\n",
      "143613/143613 [==============================] - 82s 570us/step - loss: 0.0321 - val_loss: 0.0443\n",
      "ROC-AUC - epoch: 2 - score: 0.9877\n",
      "\n",
      "\n",
      "best:0.9881\n",
      "current:0.9877\n",
      "Early stopping due to lower roc auc\n",
      "fitted\n",
      "153164/153164 [==============================] - 33s 213us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 1/2 [11:51<11:51, 711.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "global_epoch 0\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/5\n",
      "143613/143613 [==============================] - 82s 570us/step - loss: 0.0693 - val_loss: 0.0518\n",
      "ROC-AUC - epoch: 1 - score: 0.98\n",
      "\n",
      "\n",
      "best:0.9881\n",
      "current:0.98\n",
      "0 patience remained\n",
      "Epoch 2/5\n",
      "143613/143613 [==============================] - 81s 567us/step - loss: 0.0469 - val_loss: 0.0469\n",
      "ROC-AUC - epoch: 2 - score: 0.9854\n",
      "\n",
      "\n",
      "best:0.9881\n",
      "current:0.9854\n",
      "Early stopping due to lower roc auc\n",
      "fitted\n",
      "\n",
      "global_epoch 1\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/5\n",
      "143613/143613 [==============================] - 81s 567us/step - loss: 0.0417 - val_loss: 0.0426\n",
      "ROC-AUC - epoch: 1 - score: 0.9873\n",
      "\n",
      "\n",
      "best:0.9881\n",
      "current:0.9873\n",
      "0 patience remained\n",
      "Epoch 2/5\n",
      "143613/143613 [==============================] - 81s 566us/step - loss: 0.0395 - val_loss: 0.0427\n",
      "ROC-AUC - epoch: 2 - score: 0.9878\n",
      "\n",
      "\n",
      "best:0.9881\n",
      "current:0.9878\n",
      "Early stopping due to lower roc auc\n",
      "fitted\n",
      "153164/153164 [==============================] - 33s 213us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 2/2 [18:48<00:00, 623.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 36s, sys: 4min, total: 14min 36s\n",
      "Wall time: 18min 48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SEEDS = 2\n",
    "pred = 0\n",
    "\n",
    "for ii in tqdm(range(SEEDS), total=SEEDS):\n",
    "\n",
    "    model = build_model(embedding_matrix)\n",
    "    for global_epoch in range(GLOBAL_EPOCHS):\n",
    "        print('\\nglobal_epoch', global_epoch)\n",
    "\n",
    "        model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            validation_data=(x_valid, y_valid),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            verbose=1,\n",
    "            callbacks=[\n",
    "                LearningRateScheduler(lambda _: 1e-3 * (0.5**global_epoch)),\n",
    "                #RocAuc_val\n",
    "                RocAucEarlyStopping(validation_data=(x_valid, y_valid),\n",
    "                                    patience=1,\n",
    "                                    digits=4)\n",
    "            ])\n",
    "        print('fitted')\n",
    "\n",
    "    pred += model.predict(tokenized_test, batch_size=1024, verbose=1) / SEEDS\n",
    "    np.save('pred', pred)\n",
    "    model.save_weights('model_weights_' + str(ii) + '.h5')\n",
    "    os.system('gzip ' + 'model_weights_' + str(ii) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eyxmTYb4Q2dp"
   },
   "source": [
    "## BERT fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "CE-uRZUmQ4d9",
    "outputId": "160753d4-8b0c-4723-80f3-4ae06d71744d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7.0\n",
      "Collecting bert-tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 2.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
      "Installing collected packages: bert-tensorflow\n",
      "Successfully installed bert-tensorflow-1.0.1\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "print(hub.__version__)\n",
    "\n",
    "#Installing BERT module\n",
    "!pip install bert-tensorflow\n",
    "\n",
    "#Importing BERT modules\n",
    "import bert\n",
    "from bert import run_classifier, optimization, tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "SMoU4oQJW8yi",
    "outputId": "160c4877-9149-4935-a966-33f6ec376521"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why edits made username hardcore m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww ! he matches background colour i'm seemi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   preprocessed_text  ...  identity_hate\n",
       "0  explanation why edits made username hardcore m...  ...              0\n",
       "1  d'aww ! he matches background colour i'm seemi...  ...              0\n",
       "\n",
       "[2 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train_with_labels = pd.concat([\n",
    "    pd.DataFrame(preprocessed_train, columns=['preprocessed_text']),\n",
    "    pd.DataFrame(train_labels, columns=TARGET_COLS)\n",
    "],\n",
    "                                           axis=1)\n",
    "preprocessed_train_with_labels.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTP1fY-25QaB"
   },
   "outputs": [],
   "source": [
    "def create_examples(df, labels_available=True):\n",
    "    \"\"\"\n",
    "    Creates input examples for the sets of texts and labels.\n",
    "    https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/run_classifier.py#L127    \n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for (i, row) in enumerate(df.values):\n",
    "        guid = None  #row[0]\n",
    "        text_a = row[0]\n",
    "        if labels_available:\n",
    "            labels = row[1:].tolist()\n",
    "        else:  # what should be here in test phase\n",
    "            labels = [0, 0, 0, 0, 0, 0]\n",
    "        examples.append(\n",
    "            bert.run_classifier.InputExample(guid=guid,\n",
    "                                             text_a=text_a,\n",
    "                                             label=labels))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "iMiU0RrN77JS",
    "outputId": "0f0304c3-3f8f-4f75-c729-01010f129dcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer without lowercasing\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
    "\n",
    "\n",
    "def create_tokenizer_from_hub_module(bert_path):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module = hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\",\n",
    "                                    as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run([\n",
    "        tokenization_info[\"vocab_file\"],\n",
    "        tokenization_info[\"do_lower_case\"],\n",
    "    ])\n",
    "    print('loading tokenizer with{} lowercasing'.format('out' *\n",
    "                                                        (not do_lower_case)))\n",
    "\n",
    "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file,\n",
    "                                           do_lower_case=do_lower_case)\n",
    "\n",
    "\n",
    "bert_tokenizer = create_tokenizer_from_hub_module(BERT_MODEL_HUB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhlik1IGTLfu"
   },
   "outputs": [],
   "source": [
    "#todo: split to funct like here https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22\n",
    "class InputFeatures(object):\n",
    "    \"\"\"\n",
    "    A single set of features of data.\n",
    "    https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/run_classifier.py#L161\n",
    "    The only difference from the original implementation (line above) is label_ids instead of label_id because\n",
    "    here we solve multi-label not multi-class task.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids,\n",
    "                 label_ids,\n",
    "                 is_real_example=True):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids,\n",
    "        self.is_real_example = is_real_example\n",
    "\n",
    "\n",
    "# split like here\n",
    "# https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22\n",
    "def convert_examples_to_features(examples, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in tqdm(enumerate(examples), total=len(examples)):\n",
    "\n",
    "        #print(example.text_a)\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        \"\"\"\n",
    "        # in original repo\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "        \"\"\"\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        \"\"\"\n",
    "        # in original repo\n",
    "        if tokens_b:\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "        \"\"\"\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        '''\n",
    "        # in original repo\n",
    "        while len(input_ids) < max_seq_length:\n",
    "          input_ids.append(0)\n",
    "          input_mask.append(0)\n",
    "          segment_ids.append(0)\n",
    "        '''\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        labels_ids = []\n",
    "        for label in example.label:\n",
    "            labels_ids.append(int(label))\n",
    "\n",
    "        if ex_index < 2:  # make as param with default value\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x)\n",
    "                                                    for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" %\n",
    "                        \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\"segment_ids: %s\" %\n",
    "                        \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %s)\" % (example.label, labels_ids))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_ids=labels_ids))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXl9kZi-wuXx"
   },
   "outputs": [],
   "source": [
    "def features_to_arrays(features):\n",
    "    \"\"\"Convert a list of InputFeatures to np.arrays\"\"\"\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_segment_ids = []\n",
    "\n",
    "    for feature in features:\n",
    "        all_input_ids.append(feature.input_ids)\n",
    "        all_input_mask.append(feature.input_mask)\n",
    "        all_segment_ids.append(feature.segment_ids)\n",
    "\n",
    "    return (np.array(all_input_ids, dtype='int32'), \n",
    "            np.array(all_input_mask, dtype='int32'), \n",
    "            np.array(all_segment_ids, dtype='int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xcUsR46o-ou"
   },
   "outputs": [],
   "source": [
    "def text2input_arrays(text, tokenizer, max_seq_length, labels_available):\n",
    "    examples = create_examples(text, labels_available=labels_available)\n",
    "    features = convert_examples_to_features(examples,  max_seq_length, tokenizer)\n",
    "    input_ids, input_masks, segment_ids = features_to_arrays(features)\n",
    "\n",
    "    return input_ids, input_masks, segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354,
     "referenced_widgets": [
      "693ed392d7ea43b8b5ce1cd2fe629d60",
      "520c94443d89403bb0ed6cf99c029a4c",
      "3fccbae61ad24aa09a95580e762d0eda",
      "65b5fde62f4b4e86a4b55d842747b770",
      "f03be4b2fc834037b909849a7a116557",
      "0053a0672e6244278fb4f697b1526526",
      "8240a2ab6f7c406aa06bb7651599e7c2",
      "f012a5ae7f4645999f81b7b7771099f8"
     ]
    },
    "colab_type": "code",
    "id": "q-2wIb1Ns-uv",
    "outputId": "f3facf83-6b2e-4012-f6e8-7692c32ac274"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693ed392d7ea43b8b5ce1cd2fe629d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=159571), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:*** Example ***\n",
      "INFO:__main__:guid: None\n",
      "INFO:__main__:tokens: [CLS] explanation why edit ##s made user ##name hardcore metallic ##a fan reverted ? they van ##dal ##isms , closure gas I voted new yo ##rk dolls FA ##C . and please remove template talk page since i ' m retired . . [SEP]\n",
      "INFO:__main__:input_ids: 101 7108 1725 14609 1116 1189 4795 16124 16883 13256 1161 5442 17464 136 1152 3498 6919 16762 117 8354 3245 146 4751 1207 26063 4661 22084 6820 1658 119 1105 4268 5782 27821 2037 3674 1290 178 112 182 2623 119 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:__main__:label: [0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0])\n",
      "INFO:__main__:*** Example ***\n",
      "INFO:__main__:guid: None\n",
      "INFO:__main__:tokens: [CLS] d ' a ##w ##w ! he matches background colour i ' m seemingly stuck . thanks . talk , j ##anu ##ary , UTC [SEP]\n",
      "INFO:__main__:input_ids: 101 173 112 170 2246 2246 106 1119 2697 3582 5922 178 112 182 9321 5342 119 5438 119 2037 117 179 19762 3113 117 11390 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:__main__:label: [0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2min 20s, sys: 1.49 s, total: 2min 22s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_input_ids, train_input_masks, train_segment_ids = text2input_arrays(\n",
    "    text=preprocessed_train_with_labels,\n",
    "    tokenizer=bert_tokenizer,\n",
    "    max_seq_length=MAX_WORDS_IN_COMMENT,\n",
    "    labels_available=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EDqqF7PZ7yrP"
   },
   "outputs": [],
   "source": [
    "train_input_ids, valid_input_ids, \\\n",
    "train_input_masks, valid_input_masks, \\\n",
    "train_segment_ids, valid_segment_ids, \\\n",
    "train_labels, valid_labels = \\\n",
    "train_test_split(train_input_ids, train_input_masks, train_segment_ids, train_labels, test_size = 0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLJDThTwf_Vb"
   },
   "source": [
    "TODO: create pipeline which uses features as is or without data->features->arrays but data->arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YF8fnd_OMGPX"
   },
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 bert_path,\n",
    "                 verbose=True,\n",
    "                 n_fine_tune_layers=10,\n",
    "                 **kwargs):\n",
    "        self.bert_path = bert_path\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        # parse it with regular expression\n",
    "        self.output_size = 768\n",
    "        self.verbose = verbose\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(self.bert_path,\n",
    "                               trainable=self.trainable,\n",
    "                               name=\"{}_module\".format(self.name))\n",
    "        trainable_vars = self.bert.variables\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = [\n",
    "            var for var in trainable_vars if not \"/cls/\" in var.name\n",
    "        ]\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        trainable_vars = trainable_vars[-self.n_fine_tune_layers:]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        # Add non-trainable weights\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"*** TRAINABLE VARS *** \")\n",
    "            for var in self._trainable_weights:\n",
    "                print(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        #inputs = [inputs.input_ids, inputs.input_mask, inputs.segment_ids]\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(input_ids=input_ids,\n",
    "                           input_mask=input_mask,\n",
    "                           segment_ids=segment_ids)\n",
    "        result = self.bert(inputs=bert_inputs,\n",
    "                           signature=\"tokens\",\n",
    "                           as_dict=True)[\"pooled_output\"]\n",
    "        \"\"\"\n",
    "        output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
    "        \n",
    "        if self.pooling == \"cls\":\n",
    "            pooled = output[\"pooled_output\"]\n",
    "        else:\n",
    "            result = output[\"sequence_output\"]\n",
    "            \n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            \n",
    "            if self.pooling == \"mean\":\n",
    "              pooled = masked_reduce_mean(result, input_mask)\n",
    "            else:\n",
    "              pooled = mul_mask(result, input_mask)\n",
    "\n",
    "        return pooled\n",
    "        \"\"\"\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'bert_path': self.bert_path,\n",
    "            'verbose': self.verbose,\n",
    "            'n_fine_tune_layers': self.n_fine_tune_layers,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# can take more fully model from https://github.com/strongio/keras-bert/blob/master/keras-bert.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9D8BuKRaMmiD"
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(max_seq_length): \n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    # Instantiate the custom Bert Layer defined above\n",
    "    bert_output = BertLayer(BERT_MODEL_HUB, verbose=True, n_fine_tune_layers=10)(bert_inputs)\n",
    "\n",
    "    # Build the rest of the classifier \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    pred = tf.keras.layers.Dense(len(TARGET_COLS), activation='sigmoid')(dense)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QNCf8ZpEsQcL"
   },
   "source": [
    "TODO: clear memory and increase batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9fg-0EEBgK1"
   },
   "source": [
    "TODO: add save best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tiH9J6v7IsZV",
    "outputId": "c9b898b9-a837-4622-c08c-6fcf83852ac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TRAINABLE VARS *** \n",
      "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
      "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
      "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
      "<tf.Variable 'bert_layer_module/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'bert_layer_module/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 220)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 220)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 220)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, 768)          108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            1542        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 109,129,802\n",
      "Trainable params: 6,103,558\n",
      "Non-trainable params: 103,026,244\n",
      "__________________________________________________________________________________________________\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/5\n",
      "143488/143613 [============================>.] - ETA: 1s - loss: 0.0564\n",
      "ROC-AUC - epoch: 1 - score: 0.9794\n",
      "\n",
      "143613/143613 [==============================] - 1775s 12ms/sample - loss: 0.0564 - val_loss: 0.0476\n",
      "Epoch 2/5\n",
      "143488/143613 [============================>.] - ETA: 1s - loss: 0.0451\n",
      "ROC-AUC - epoch: 2 - score: 0.9834\n",
      "\n",
      "143613/143613 [==============================] - 1769s 12ms/sample - loss: 0.0451 - val_loss: 0.0450\n",
      "Epoch 3/5\n",
      "143488/143613 [============================>.] - ETA: 1s - loss: 0.0405\n",
      "ROC-AUC - epoch: 3 - score: 0.9851\n",
      "\n",
      "143613/143613 [==============================] - 1769s 12ms/sample - loss: 0.0405 - val_loss: 0.0439\n",
      "Epoch 4/5\n",
      "143488/143613 [============================>.] - ETA: 1s - loss: 0.0366\n",
      "ROC-AUC - epoch: 4 - score: 0.9836\n",
      "\n",
      "\n",
      "best:0.9851\n",
      "current:0.9836\n",
      "0 patience remained\n",
      "143613/143613 [==============================] - 1770s 12ms/sample - loss: 0.0366 - val_loss: 0.0451\n",
      "Epoch 5/5\n",
      "143488/143613 [============================>.] - ETA: 1s - loss: 0.0330\n",
      "ROC-AUC - epoch: 5 - score: 0.9836\n",
      "\n",
      "\n",
      "best:0.9851\n",
      "current:0.9836\n",
      "Early stopping due to lower roc auc\n",
      "143613/143613 [==============================] - 1769s 12ms/sample - loss: 0.0330 - val_loss: 0.0471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f52dba99e10>"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "model = build_model(max_seq_length=MAX_WORDS_IN_COMMENT)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([valid_input_ids, valid_input_masks, valid_segment_ids], valid_labels),\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    callbacks=[RocAucEarlyStopping(validation_data=([valid_input_ids, valid_input_masks, valid_segment_ids], valid_labels), \n",
    "                                   patience=1, digits=4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "imo2XJzmTxY7",
    "outputId": "18379b1c-2034-44a4-bcb8-55e28fbbe096"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d4eb5aef74eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/BertModel_multilabel.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \"\"\"\n\u001b[1;32m   1170\u001b[0m     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1171\u001b[0;31m                       signatures)\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures)\u001b[0m\n\u001b[1;32m    107\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    108\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 109\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    110\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mmodel_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[0;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# From the earliest layers on.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m       \u001b[0mlayer_class_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m       \u001b[0mlayer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0mfiltered_inbound_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-7d76af3fa7bb>\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         config.update({\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;34m'bert_path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbert_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;34m'verbose'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;34m'n_fine_tune_layers'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn_fine_tune_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bert_path' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(output_folder+'/BertModel_multilabel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354,
     "referenced_widgets": [
      "ea66eaaba5d44805bc1107aa0b1d4fda",
      "0d6560e05ccc4828a4017eb289081f1c",
      "3e87389a294e46d68c0295ba10b83b80",
      "50aa8584c02c4911ba02ddec622b80d2",
      "52ca61812992475995aeab049db2e439",
      "f01dd6ab656747c2b38bc922c653cec2",
      "5529768392ff4bcbb8ad552729197171",
      "6d82dbcda00649d099ab9b4f8a84bcf3"
     ]
    },
    "colab_type": "code",
    "id": "V21lhkWODcfn",
    "outputId": "eb4e7097-5d7f-4604-aa52-574be7b09345"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea66eaaba5d44805bc1107aa0b1d4fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=153164), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:*** Example ***\n",
      "INFO:__main__:guid: None\n",
      "INFO:__main__:tokens: [CLS] you bitch j ##a rule su ##cc ##es ##ful ever what ##s hating sad m ##of ##uck ##as bitch slap your pet ##hed ##ic white faces get kiss ass guys sick ##en . j ##a rule pride da music man . don ##t di ##ss shit . not ##hin wrong be ##in like t ##up ##ac brother fuck ##in white boys get things right next time . , [SEP]\n",
      "INFO:__main__:input_ids: 101 1128 7979 179 1161 3013 28117 19515 1279 2365 1518 1184 1116 26766 6782 182 10008 8474 2225 7979 15933 1240 11109 8961 1596 1653 4876 1243 3041 3919 3713 4809 1424 119 179 1161 3013 8188 5358 1390 1299 119 1274 1204 4267 3954 4170 119 1136 8265 2488 1129 1394 1176 189 4455 7409 1711 9367 1394 1653 3287 1243 1614 1268 1397 1159 119 117 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:__main__:label: [0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0])\n",
      "INFO:__main__:*** Example ***\n",
      "INFO:__main__:guid: None\n",
      "INFO:__main__:tokens: [CLS] from r ##f ##c the title fine , I ##MO . [SEP]\n",
      "INFO:__main__:input_ids: 101 1121 187 2087 1665 1103 1641 2503 117 146 20647 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:__main__:label: [0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2min 11s, sys: 0 ns, total: 2min 11s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preprocessed_test_df = pd.DataFrame(preprocessed_test,\n",
    "                                    columns=['preprocessed_text'])\n",
    "test_input_ids, test_input_masks, test_segment_ids = text2input_arrays(\n",
    "    text=preprocessed_test_df,\n",
    "    tokenizer=bert_tokenizer,\n",
    "    max_seq_length=MAX_WORDS_IN_COMMENT,\n",
    "    labels_available=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "Odnkbt-ZnIj6",
    "outputId": "a09877fa-f5eb-456c-e7e2-0a2f92e61c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 36s, sys: 9min 27s, total: 23min 4s\n",
      "Wall time: 20min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pred  = model.predict([test_input_ids, test_input_masks, test_segment_ids])\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ujanlBVs61IJ"
   },
   "source": [
    "# Cheaty evaluating on test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIOGCjO11DrT"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(submission_path)\n",
    "submission[TARGET_COLS] = (pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "CCukoB8QB2Rz",
    "outputId": "2a7ff76c-123c-435c-a59e-700c87d1c07b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.997917</td>\n",
       "      <td>0.292032</td>\n",
       "      <td>0.957219</td>\n",
       "      <td>2.721301e-02</td>\n",
       "      <td>0.842092</td>\n",
       "      <td>0.174070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.788139e-07</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>8.940697e-08</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>8.940697e-08</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.384186e-07</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>8.940697e-08</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>4.172325e-07</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>0.615153</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.063064</td>\n",
       "      <td>5.566061e-03</td>\n",
       "      <td>0.060806</td>\n",
       "      <td>0.002648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>2.171397e-04</td>\n",
       "      <td>0.003716</td>\n",
       "      <td>0.000363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.788139e-07</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  ...    insult  identity_hate\n",
       "0  00001cee341fdb12  0.997917  ...  0.842092       0.174070\n",
       "1  0000247867823ef7  0.000053  ...  0.000028       0.000004\n",
       "2  00013b17ad220c46  0.000039  ...  0.000028       0.000003\n",
       "3  00017563c3f7919a  0.000032  ...  0.000018       0.000002\n",
       "4  00017695ad8997eb  0.000080  ...  0.000035       0.000005\n",
       "5  0001ea8717f6de06  0.000023  ...  0.000013       0.000002\n",
       "6  00024115d4cbde0f  0.000046  ...  0.000018       0.000006\n",
       "7  000247e83dcc1211  0.615153  ...  0.060806       0.002648\n",
       "8  00025358d4737918  0.040541  ...  0.003716       0.000363\n",
       "9  00026d1092fe71cc  0.000085  ...  0.000035       0.000006\n",
       "\n",
       "[10 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YUF4Uxrl8pXr"
   },
   "outputs": [],
   "source": [
    "# labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)\n",
    "test_labels_df = pd.read_csv(test_labels_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6crgLcLx7X-p",
    "outputId": "2ec8bec1-d688-4f45-e887-ba9feb352452"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63978, 7)"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_df = test_labels_df[(test_labels_df[\"toxic\"] != -1) &\n",
    "                                (test_labels_df[\"severe_toxic\"] != -1) &\n",
    "                                (test_labels_df[\"obscene\"] != -1) &\n",
    "                                (test_labels_df[\"threat\"] != -1) &\n",
    "                                (test_labels_df[\"insult\"] != -1) &\n",
    "                                (test_labels_df[\"identity_hate\"] != -1)]\n",
    "test_labels_df.shape                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-ymumFOQ9sA1",
    "outputId": "953c68fb-bed0-409b-a1f7-78e421e2e76f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63978, 7)"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_to_evaluate = submission[submission['id'].isin(test_labels_df['id'].values)]\n",
    "submission_to_evaluate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "C3r8VTCK_TaM",
    "outputId": "2eb88de4-059e-41f5-f303-50d343e1621e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9623272200051558\n",
      "CV score for class severe_toxic is 0.9856258687799999\n",
      "CV score for class obscene is 0.9753285733840358\n",
      "CV score for class threat is 0.9720786658359368\n",
      "CV score for class insult is 0.9708459918444602\n",
      "CV score for class identity_hate is 0.9773383035494506\n",
      "Total CV score is 0.97392410389984\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for class_name in TARGET_COLS:\n",
    "    train_target = test_labels_df[class_name]\n",
    "    train_predicted = submission_to_evaluate[class_name]\n",
    "\n",
    "    cv_score = np.mean(roc_auc_score(train_target.values, train_predicted.values))\n",
    "    scores.append(cv_score)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKsSnwkd3TBJ"
   },
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(output_folder,\"submission_fine_tuned_bert.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tNf0KsAHCSJB"
   },
   "source": [
    "Жирный шрифт означает изменения в процессе валидации и оценивания и при изменении действует начиная со строчки указания и ниже\n",
    "\n",
    "* glove twitter 200:  0.98052\n",
    "\n",
    "**С поднятием колва epochs и global_epochs + RocAucEarlyStopping** скор упал с 0.98052 до 0.97573\n",
    "\n",
    "* GLOBAL_EPOCHS = 2, EPOCHS = 5, glove.840B.300d.txt   0.97994  \n",
    "* BPEmb(lang=\"en\", dim=25, vs=20000)   0.97375  \n",
    "* BPEmb(lang=\"en\", dim=300, vs=20000) without preprocessing   0.97699  \n",
    "* BPEmb(lang=\"en\", dim=300, vs=20000)   0.97865   \n",
    "* fasttext crawl-300d-2M-subword 0.95732  \n",
    "* BPEmb(lang=\"en\", dim=300, vs=20000) **Patience 2->1 SEEDS 1->1**  0.98041   \n",
    "* flair ELMoEmbeddings('small') embedding len 768  0.98005  \n",
    "* flair ELMoEmbeddings('medium') embedding len 1536  0.98038  \n",
    "* flair RoBERTaEmbeddings('roberta-base') embedding len 768  0.97413  \n",
    "* flair stack of ELMoEmbeddings('small') + RoBERTaEmbeddings('roberta-base') embedding len 1536  0.98069      \n",
    "* 3 tuned layers in BERT modeldense = tf.keras.layers.Dense(256, activation='relu')(bert_output)pred = tf.keras.layers.Dense(len(TARGET_COLS), activation='sigmoid')(dense)model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])epochs=1,batch_size=32   0.97658\n",
    "* 10 tuned layers in BERT modeldense = tf.keras.layers.Dense(256, activation='relu')(bert_output)pred = tf.keras.layers.Dense(len(TARGET_COLS), activation='sigmoid')(dense)model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])epochs=1,batch_size=128, 5 epochs+ES(patience=1)  0.97377"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "GRh7ig80jk3s",
    "1_nWe7O-jqak",
    "TGzxJch3juGy",
    "sWmhQEFAj1VE",
    "_yDXCBmqtkPl",
    "0hTlPvNY8gi1"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "NLP advance course. HW3. Distributive semantic. Embedings_+_Bidirectional_GRU.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0053a0672e6244278fb4f697b1526526": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d6560e05ccc4828a4017eb289081f1c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e87389a294e46d68c0295ba10b83b80": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f01dd6ab656747c2b38bc922c653cec2",
      "max": 153164,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_52ca61812992475995aeab049db2e439",
      "value": 153164
     }
    },
    "3fccbae61ad24aa09a95580e762d0eda": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0053a0672e6244278fb4f697b1526526",
      "max": 159571,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f03be4b2fc834037b909849a7a116557",
      "value": 159571
     }
    },
    "50aa8584c02c4911ba02ddec622b80d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d82dbcda00649d099ab9b4f8a84bcf3",
      "placeholder": "​",
      "style": "IPY_MODEL_5529768392ff4bcbb8ad552729197171",
      "value": "100% 153164/153164 [02:00&lt;00:00, 1275.08it/s]"
     }
    },
    "520c94443d89403bb0ed6cf99c029a4c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52ca61812992475995aeab049db2e439": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5529768392ff4bcbb8ad552729197171": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65b5fde62f4b4e86a4b55d842747b770": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f012a5ae7f4645999f81b7b7771099f8",
      "placeholder": "​",
      "style": "IPY_MODEL_8240a2ab6f7c406aa06bb7651599e7c2",
      "value": "100% 159571/159571 [02:11&lt;00:00, 1211.32it/s]"
     }
    },
    "693ed392d7ea43b8b5ce1cd2fe629d60": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3fccbae61ad24aa09a95580e762d0eda",
       "IPY_MODEL_65b5fde62f4b4e86a4b55d842747b770"
      ],
      "layout": "IPY_MODEL_520c94443d89403bb0ed6cf99c029a4c"
     }
    },
    "6d82dbcda00649d099ab9b4f8a84bcf3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8240a2ab6f7c406aa06bb7651599e7c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea66eaaba5d44805bc1107aa0b1d4fda": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3e87389a294e46d68c0295ba10b83b80",
       "IPY_MODEL_50aa8584c02c4911ba02ddec622b80d2"
      ],
      "layout": "IPY_MODEL_0d6560e05ccc4828a4017eb289081f1c"
     }
    },
    "f012a5ae7f4645999f81b7b7771099f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f01dd6ab656747c2b38bc922c653cec2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f03be4b2fc834037b909849a7a116557": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
