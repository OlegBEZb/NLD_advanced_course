{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVRaU_zG-PaI"
   },
   "source": [
    "## Embeddings to check\n",
    "\n",
    "* Word Embeddings (word2vec, GloVe, etc.)\n",
    "* Starspace and other similarity learning embeddings\n",
    "* char-gram embeddings (bpemb, fasttext etc.)\n",
    "* doc and sentence embeddings (doc2vec, sent2vec etc.)\n",
    "* specific context models (BERT, ELMo)\n",
    "* Advanced: Poincare Embeddings concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30D0Scxk4kn3"
   },
   "source": [
    "# Libs import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6927,
     "status": "ok",
     "timestamp": 1576579459030,
     "user": {
      "displayName": "Олег Литвинов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC8rY4OprXFCt_FxC9zp02Jpl3SMZAtMJxOocMO=s64",
      "userId": "10665736335049067601"
     },
     "user_tz": -180
    },
    "id": "ZnXUdrxQ3TAN",
    "outputId": "47c3be9e-93ff-4638-9b05-2fd65d1f34e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "Collecting Unidecode\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n",
      "\u001b[?25hInstalling collected packages: Unidecode\n",
      "Successfully installed Unidecode-1.1.1\n"
     ]
    }
   ],
   "source": [
    "import os, math, operator, csv, random, pickle, re, sys\n",
    "# import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import gc\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.util import pad_sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "# from keras import backend as K\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords');\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "! pip install Unidecode;\n",
    "from unidecode import unidecode\n",
    "#! pip install pyspellchecker;\n",
    "#from spellchecker import SpellChecker\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i673BlTJEazT"
   },
   "source": [
    "# Global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okymGz_lEjLC"
   },
   "outputs": [],
   "source": [
    "COLAB = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VD2z8D2V7Dod"
   },
   "source": [
    "# Authorization on Google drive and configurings paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37084,
     "status": "ok",
     "timestamp": 1576579489228,
     "user": {
      "displayName": "Олег Литвинов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC8rY4OprXFCt_FxC9zp02Jpl3SMZAtMJxOocMO=s64",
      "userId": "10665736335049067601"
     },
     "user_tz": -180
    },
    "id": "auzLnrdK51Rg",
    "outputId": "2bbf78fe-255c-4e69-88af-a912ec0c1ff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n",
      "data found: ['test.csv', 'test_labels.csv', 'train.csv', 'sample_submission.csv', 'submission.csv']\n"
     ]
    }
   ],
   "source": [
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    data_folder = '/content/drive/My Drive/Advanced NLP/Homework 1: Classical classification task like Kaggle Toxic or Quora/Toxic data'\n",
    "    embeddings_folder = '/content/drive/My Drive/Advanced NLP/Homework 1: Classical classification task like Kaggle Toxic or Quora'\n",
    "    print('data found:', os.listdir(data_folder))\n",
    "else:\n",
    "    data_folder = '../input/jigsaw-toxic-comment-classification-challenge/'\n",
    "    embeddings_folder = '../input/glove-global-vectors-for-word-representation'\n",
    "    print('data found:', os.listdir(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYdAXsdT3TAd"
   },
   "outputs": [],
   "source": [
    "#pretrained_folder = \"../input/\"\n",
    "train_filepath = os.path.join(data_folder,\"train.csv\")\n",
    "test_filepath = os.path.join(data_folder,\"test.csv\")\n",
    "test_labels_filepath = os.path.join(data_folder,\"test_labels.csv\")\n",
    "\n",
    "#path to a submission\n",
    "submission_path = os.path.join(data_folder,\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15GVrJYa717o"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYljyuKTCm8r"
   },
   "source": [
    "## Dicts and lists of useful words and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YjL5O731CtDD"
   },
   "outputs": [],
   "source": [
    "#List of some words that often appear in toxic comments\n",
    "#Sorry about the level of toxicity in it!\n",
    "toxic_words = [\"poop\", \"crap\", \"prick\", \"twat\", \"wikipedia\", \"wiki\", \"hahahahaha\", \"lol\", \"bastard\", \"sluts\", \"slut\", \"douchebag\", \"douche\", \"blowjob\", \"nigga\", \"dumb\", \"jerk\", \"wanker\", \"wank\", \"penis\", \"motherfucker\", \"fucker\", \"fuk\", \"fucking\", \"fucked\", \"fuck\", \"bullshit\", \"shit\", \"stupid\", \"bitches\", \"bitch\", \"suck\", \"cunt\", \"dick\", \"cocks\", \"cock\", \"die\", \"kill\", \"gay\", \"jewish\", \"jews\", \"jew\", \"niggers\", \"nigger\", \"faggot\", \"fag\", \"asshole\"]\n",
    "astericks_words = [('mother****ers', 'motherfuckers'), ('motherf*cking', 'motherfucking'), ('mother****er', 'motherfucker'), ('motherf*cker', 'motherfucker'), ('bullsh*t', 'bullshit'), ('f**cking', 'fucking'), ('f*ucking', 'fucking'), ('fu*cking', 'fucking'), ('****ing', 'fucking'), ('a**hole', 'asshole'), ('assh*le', 'asshole'), ('f******', 'fucking'), ('f*****g', 'fucking'), ('f***ing', 'fucking'), ('f**king', 'fucking'), ('f*cking', 'fucking'), ('fu**ing', 'fucking'), ('fu*king', 'fucking'), ('fuc*ers', 'fuckers'), ('f*****', 'fucking'), ('f***ed', 'fucked'), ('f**ker', 'fucker'), ('f*cked', 'fucked'), ('f*cker', 'fucker'), ('f*ckin', 'fucking'), ('fu*ker', 'fucker'), ('fuc**n', 'fucking'), ('ni**as', 'niggas'), ('b**ch', 'bitch'), ('b*tch', 'bitch'), ('c*unt', 'cunt'), ('f**ks', 'fucks'), ('f*ing', 'fucking'), ('ni**a', 'nigga'), ('c*ck', 'cock'), ('c*nt', 'cunt'), ('cr*p', 'crap'), ('d*ck', 'dick'), ('f***', 'fuck'), ('f**k', 'fuck'), ('f*ck', 'fuck'), ('fc*k', 'fuck'), ('fu**', 'fuck'), ('fu*k', 'fuck'), ('s***', 'shit'), ('s**t', 'shit'), ('sh**', 'shit'), ('sh*t', 'shit'), ('tw*t', 'twat')]\n",
    "fasttext_misspelings = {\"'n'balls\": 'balls', \"-nazi's\": 'nazis', 'adminabuse': 'admin abuse', \"admins's\": 'admins', 'arsewipe': 'arse wipe', 'assfack': 'asshole', 'assholifity': 'asshole', 'assholivity': 'asshole', 'asshoul': 'asshole', 'asssholeee': 'asshole', 'belizeans': 'mexicans', \"blowing's\": 'blowing', 'bolivians': 'mexicans', 'celtofascists': 'fascists', 'censorshipmeisters': 'censor', 'chileans': 'mexicans', 'clerofascist': 'fascist', 'cowcrap': 'crap', 'crapity': 'crap', \"d'idiots\": 'idiots', 'deminazi': 'nazi', 'dftt': \"don't feed the troll\", 'dildohs': 'dildo', 'dramawhores': 'drama whores', 'edophiles': 'pedophiles', 'eurocommunist': 'communist', 'faggotkike': 'faggot', 'fantard': 'retard', 'fascismnazism': 'fascism', 'fascistisized': 'fascist', 'favremother': 'mother', 'fuxxxin': 'fucking', \"g'damn\": 'goddamn', 'harassmentat': 'harassment', 'harrasingme': 'harassing me', 'herfuc': 'motherfucker', 'hilterism': 'fascism', 'hitlerians': 'nazis', 'hitlerites': 'nazis', 'hubrises': 'pricks', 'idiotizing': 'idiotic', 'inadvandals': 'vandals', \"jackass's\": 'jackass', 'jiggabo': 'nigga', 'jizzballs': 'jizz balls', 'jmbass': 'dumbass', 'lejittament': 'legitimate', \"m'igger\": 'nigger', \"m'iggers\": 'niggers', 'motherfacking': 'motherfucker', 'motherfuckenkiwi': 'motherfucker', 'muthafuggas': 'niggas', 'nazisms': 'nazis', 'netsnipenigger': 'nigger', 'niggercock': 'nigger', 'niggerspic': 'nigger', 'nignog': 'nigga', 'niqqass': 'niggas', \"non-nazi's\": 'not a nazi', 'panamanians': 'mexicans', 'pedidiots': 'idiots', 'picohitlers': 'hitler', 'pidiots': 'idiots', 'poopia': 'poop', 'poopsies': 'poop', 'presumingly': 'obviously', 'propagandaanddisinformation': 'propaganda and disinformation', 'propagandaministerium': 'propaganda', 'puertoricans': 'mexicans', 'puertorricans': 'mexicans', 'pussiest': 'pussies', 'pussyitis': 'pussy', 'rayaridiculous': 'ridiculous', 'redfascists': 'fascists', 'retardzzzuuufff': 'retard', \"revertin'im\": 'reverting', 'scumstreona': 'scums', 'southamericans': 'mexicans', 'strasserism': 'fascism', 'stuptarded': 'retarded', \"t'nonsense\": 'nonsense', \"threatt's\": 'threat', 'titoists': 'communists', 'twatbags': 'douchebags', 'youbollocks': 'you bollocks'}\n",
    "acronym_words = {\"btw\":\"by the way\", \"yo\": \"you\", \"u\": \"you\", \"r\": \"are\", \"ur\": \"your\", \"ima\": \"i am going to\", \"imma\": \"i am going to\", \"i'ma\":\"i am going to\", \"cos\":\"because\", \"coz\":\"because\", \"stfu\": \"shut the fuck up\", \"wat\": \"what\"}\n",
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
    "\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#spell_checker = SpellChecker()\n",
    "\n",
    "cont_patterns = [\n",
    "    (r'(W|w)on\\'t', r'will not'),\n",
    "    (r'(C|c)an\\'t', r'can not'),\n",
    "    (r'(I|i)\\'m', r'i am'),\n",
    "    (r'(A|a)in\\'t', r'is not'),\n",
    "    (r'(\\w+)\\'ll', r'\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', r'\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', r'\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', r'\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', r'\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', r'\\g<1> would'),\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "#We will filter all characters except alphabet characters and some punctuation\n",
    "valid_characters = \" \" + \"@$\" + \"'!?-\" + \"abcdefghijklmnopqrstuvwxyz\" + \"abcdefghijklmnopqrstuvwxyz\".upper()\n",
    "valid_set = set(x for x in valid_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cs_nlu1iILY1"
   },
   "source": [
    "## Funcs for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GfUaIP-wIKTZ"
   },
   "outputs": [],
   "source": [
    "def replace_url(word):\n",
    "    if \"http://\" in word or \"www.\" in word or \"https://\" in word or \"wikipedia.org\" in word:\n",
    "        return \"\"\n",
    "    return word\n",
    "\n",
    "\n",
    "def word_tokenize(sentence, tokenizer):\n",
    "    sentence = sentence.replace(\"$\", \"s\")\n",
    "    sentence = sentence.replace(\"@\", \"a\")    \n",
    "    sentence = sentence.replace(\"!\", \" ! \")\n",
    "    sentence = sentence.replace(\"?\", \" ? \")\n",
    "\n",
    "    return tokenizer.tokenize(sentence)\n",
    "\n",
    "\n",
    "def split_toxic_and_normal_parts(word, toxic_words):\n",
    "    if word == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    lower = word.lower()\n",
    "    for toxic_word in toxic_words:\n",
    "        start = lower.find(toxic_word)\n",
    "        if start >= 0:\n",
    "            end = start + len(toxic_word)\n",
    "            result = \" \".join([word[0:start], word[start:end], split_toxic_and_normal_parts(word[end:], toxic_words)])\n",
    "            return result.replace(\"  \", \" \").strip()\n",
    "    return word\n",
    "\n",
    "\n",
    "def normalize_by_dictionary(normalized_word, dictionary):\n",
    "    result = []\n",
    "    for word in normalized_word.split():\n",
    "        if word == word.upper():\n",
    "            if word.lower() in dictionary:\n",
    "                result.append(dictionary[word.lower()].upper())\n",
    "            else:\n",
    "                result.append(word)\n",
    "        else:\n",
    "            if word.lower() in dictionary:\n",
    "                result.append(dictionary[word.lower()])\n",
    "            else:\n",
    "                result.append(word)\n",
    "    \n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "def normalize_comment(comment, tokenizer, max_comment_length):\n",
    "    comment = unidecode(comment)\n",
    "    comment = comment[:max_comment_length]\n",
    "\n",
    "    normalized_words = []\n",
    "    \n",
    "    # ('mother****ers', 'motherfuckers')\n",
    "    # for w in astericks_words:\n",
    "    #     if w[0] in comment:\n",
    "    #         comment = comment.replace(w[0], w[1])\n",
    "    #     if w[0].upper() in comment:\n",
    "    #         comment = comment.replace(w[0].upper(), w[1].upper())\n",
    "    \n",
    "    for word in word_tokenize(comment, tokenizer):\n",
    "        if word in english_stopwords: continue\n",
    "\n",
    "        # # '(W|w)on\\'t', r'will not'\n",
    "        # for (pattern, repl) in patterns:\n",
    "        #    word = re.sub(pattern, repl, word)\n",
    "\n",
    "        if word == \".\" or word == \",\":\n",
    "            normalized_words.append(word)\n",
    "            continue\n",
    "        \n",
    "        # drop url parts from links\n",
    "        # word = replace_url(word)\n",
    "\n",
    "        # replace single dots to whitespaces\n",
    "        if word.count(\".\") == 1:\n",
    "            word = word.replace(\".\", \" \")\n",
    "\n",
    "        # leave only legalized symbols\n",
    "        filtered_word = \"\".join([x for x in word if x in valid_set])\n",
    "                    \n",
    "        #Kind of hack: for every word check if it has a toxic word as a part of it\n",
    "        #If so, split this word by swear and non-swear part.\n",
    "        #normalized_word = split_toxic_and_normal_parts(filtered_word, toxic_words)\n",
    "        normalized_word = filtered_word\n",
    "\n",
    "#         normalized_word = normalize_by_dictionary(normalized_word, hyphens_dict)\n",
    "#         normalized_word = normalize_by_dictionary(normalized_word, merged_dict)\n",
    "        \n",
    "        # check misspellings\n",
    "        # temp = []\n",
    "        # for word in normalized_word.split():\n",
    "        #   temp.append(spell_checker.correction(word))\n",
    "        # normalized_word = \" \".join(temp)\n",
    "          \n",
    "        # normalized_word = normalize_by_dictionary(normalized_word, fasttext_misspelings)\n",
    "        normalized_word = normalize_by_dictionary(normalized_word, acronym_words)\n",
    "\n",
    "        normalized_words.append(normalized_word)\n",
    "        \n",
    "    normalized_comment = \" \".join(normalized_words)\n",
    "    \n",
    "    result = []\n",
    "    for word in normalized_comment.split():\n",
    "        # if word is upper\n",
    "        if word.upper() == word:\n",
    "            result.append(word)\n",
    "        else:\n",
    "            result.append(word.lower())\n",
    "    \n",
    "    #apparently, people on wikipedia love to talk about sockpuppets :-)\n",
    "    result = \" \".join(result)\n",
    "    if \"sock puppet\" in result:\n",
    "        result = result.replace(\"sock puppet\", \"sockpuppet\")\n",
    "    \n",
    "    if \"SOCK PUPPET\" in result:\n",
    "        result = result.replace(\"SOCK PUPPET\", \"SOCKPUPPET\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def preprocess_text(df, preprocessing_func, tokenizer, max_comment_length):\n",
    "  text_ndarray = df.fillna('_na').values\n",
    "  np_preprocessing_func = np.vectorize(preprocessing_func)\n",
    "\n",
    "  preprocessed_text = np_preprocessing_func(text_ndarray, tokenizer, max_comment_length)\n",
    "\n",
    "  print('Gained shape:', preprocessed_text.shape)\n",
    "  return preprocessed_text\n",
    "\n",
    "\n",
    "def tokenize_and_pad(text, tokenizer):\n",
    "\n",
    "  tokenized_text = tokenizer.texts_to_sequences(text)\n",
    "  padded_text = pad_sequences(tokenized_text, maxlen=MAX_LEN, dtype='int', value=0)\n",
    "\n",
    "  print('Gained shape:', padded_text.shape)\n",
    "  return padded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tn1-XTZEIZ3y"
   },
   "source": [
    "## Loading frames and processing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AWRdxZsHG3B"
   },
   "outputs": [],
   "source": [
    "TEXT_COLUMN = 'comment_text'\n",
    "TARGET_COLS = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "MAX_LEN = 220\n",
    "max_tokens = 20000\n",
    "max_comment_length = 20000 #We are going to truncate a comment if its length > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38360,
     "status": "ok",
     "timestamp": 1576579490605,
     "user": {
      "displayName": "Олег Литвинов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC8rY4OprXFCt_FxC9zp02Jpl3SMZAtMJxOocMO=s64",
      "userId": "10665736335049067601"
     },
     "user_tz": -180
    },
    "id": "BAe8xpV48CO1",
    "outputId": "405ec415-4e07-4d2b-e46e-d598b4c422d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 733 ms, sys: 114 ms, total: 847 ms\n",
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_df = pd.read_csv(train_filepath)\n",
    "train_labels = train_df[TARGET_COLS].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 101173,
     "status": "ok",
     "timestamp": 1576579553460,
     "user": {
      "displayName": "Олег Литвинов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC8rY4OprXFCt_FxC9zp02Jpl3SMZAtMJxOocMO=s64",
      "userId": "10665736335049067601"
     },
     "user_tz": -180
    },
    "id": "Dxwo5A8VfRRL",
    "outputId": "50b362cd-cd92-4892-f13a-d54a3f7a138b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gained shape: (159571,)\n",
      "[\"explanation why edits made username hardcore metallica fan reverted ? they vandalisms , closure gas I voted new york dolls FAC . and please remove template talk page since i'm retired . .\"\n",
      " \"d'aww ! he matches background colour i'm seemingly stuck . thanks . talk , january , UTC\"\n",
      " \"hey man , i'm really trying edit war . it's guy constantly removing relevant information talking edits instead talk page . he seems care formatting actual info .\"]\n",
      "CPU times: user 59.8 s, sys: 3.02 s, total: 1min 2s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "\n",
    "preprocessed_train = preprocess_text(train_df[TEXT_COLUMN], \n",
    "                                     normalize_comment, \n",
    "                                     tknzr,\n",
    "                                     max_comment_length)\n",
    "\n",
    "print(preprocessed_train[:3])\n",
    "\n",
    "del train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 158851,
     "status": "ok",
     "timestamp": 1576579611182,
     "user": {
      "displayName": "Олег Литвинов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC8rY4OprXFCt_FxC9zp02Jpl3SMZAtMJxOocMO=s64",
      "userId": "10665736335049067601"
     },
     "user_tz": -180
    },
    "id": "x5MLLz630n8i",
    "outputId": "f9106038-802c-4ee1-953b-6a20b71bc4e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gained shape: (153164,)\n",
      "CPU times: user 54.5 s, sys: 2.87 s, total: 57.4 s\n",
      "Wall time: 57.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_df = pd.read_csv(test_filepath)\n",
    "\n",
    "preprocessed_test = preprocess_text(test_df[TEXT_COLUMN], \n",
    "                                    normalize_comment, \n",
    "                                    tknzr,\n",
    "                                    max_comment_length)\n",
    "\n",
    "del test_df, tknzr\n",
    "gc.collect()\n",
    "# y_train = np.apply_along_axis(document_vector, 1, tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8APgvUyskaf"
   },
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCRY8qnNJTh_"
   },
   "source": [
    "## Preparing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jSAJK0AYsdW2"
   },
   "outputs": [],
   "source": [
    "# word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' %len(word_index))\n",
    "\n",
    "\n",
    "# # %%time\n",
    "# # embedding_matrix = np.load('../input/embedding-2/embedding_matrix_big.npy')\n",
    "\n",
    "\n",
    "# embedding_len = 25\n",
    "# #g = GloveEmbedding('twitter', d_emb=embedding_len, show_progress=True)\n",
    "# words = pd.read_csv(os.path.join(HW_folder, \"glove.twitter.27B.{}d.txt\".format(embedding_len)), \n",
    "#                     sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "\n",
    "# vocab_words = words[words.index.isin(list(word_index.keys()))]\n",
    "# print('vocab_words.shape', vocab_words.shape)\n",
    "\n",
    "\n",
    "# def get_vec(word, words_df):\n",
    "#   return words_df.loc[word].values\n",
    "\n",
    "\n",
    "# # embedding_matrix = np.zeros((len(vocab), embedding_len))\n",
    "# # for word, emb in tqdm(embeddings_dict.items(), total=len(embeddings_dict), mininterval=10):\n",
    "# #     index = list(embeddings_dict.keys()).index(word)\n",
    "# #     if emb is not None:\n",
    "# #       embedding_matrix[index] = emb\n",
    "\n",
    "\n",
    "# num_words = min(max_tokens, len(word_index) + 1)\n",
    "# embedding_matrix = np.zeros((num_words, embedding_len)) # zeroth row for UNK\n",
    "# for word, i in word_index.items():\n",
    "#   if i >= max_tokens:\n",
    "#     continue\n",
    "#   try:\n",
    "#     embedding_vector = get_vec(word, vocab_words)\n",
    "#   except:\n",
    "#     continue\n",
    "#   if embedding_vector is not None:\n",
    "#     # words not found in embedding index will be all-zeros.\n",
    "#     embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "# print('embedding_matrix.shape', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 179695,
     "status": "ok",
     "timestamp": 1576579632056,
     "user": {
      "displayName": "Олег Литвинов",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC8rY4OprXFCt_FxC9zp02Jpl3SMZAtMJxOocMO=s64",
      "userId": "10665736335049067601"
     },
     "user_tz": -180
    },
    "id": "xys_jQL_tOnx",
    "outputId": "5c365c4a-c02f-4fe4-9c9c-ae7fc72dca59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.3 s, sys: 508 ms, total: 20.8 s\n",
      "Wall time: 20.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(tokenizer=None,\n",
    "                            min_df=2,\n",
    "                            max_df=.625,\n",
    "                            strip_accents='unicode',\n",
    "                            use_idf=1,\n",
    "                            smooth_idf=True,\n",
    "                            sublinear_tf=True,\n",
    "                            max_features=max_tokens)#, stop_words=stop_words_en)\n",
    "\n",
    "train_features = tfidf_vec.fit_transform(preprocessed_train)\n",
    "test_features = tfidf_vec.transform(preprocessed_test)\n",
    "\n",
    "del preprocessed_train, preprocessed_test, tfidf_vec\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AslnHsTy0gCQ"
   },
   "source": [
    "## Build and fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhX9sRHYK7LB"
   },
   "source": [
    "### Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBSKOOA63aR-"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NhCdejbILCqa"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# def document_vector(doc, embedding_matrix=embedding_matrix):\n",
    "#     \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n",
    "#     doc = [embedding_matrix[index] for index in doc]\n",
    "#     return np.mean(doc, axis=0)\n",
    "\n",
    "\n",
    "# x_train = np.apply_along_axis(document_vector, 1, tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W-GuOc0A0evP"
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'LogisticRegression': LogisticRegression()\n",
    "}\n",
    "\n",
    "# the optimisation parameters for each of the above models\n",
    "params = {\n",
    "    'RandomForestClassifier':{ \n",
    "        \"n_estimators\": [100],\n",
    "        \"max_features\": [\"auto\"],\n",
    "        \"bootstrap\": [True],\n",
    "        \"criterion\": ['gini', 'entropy'],\n",
    "        \"oob_score\": [True, False]\n",
    "            },\n",
    "    'LogisticRegression': {\n",
    "        'solver': ['newton-cg', 'sag', 'lbfgs'],\n",
    "        'C': [0.01, 0.1, 0.5, 1, 10, 100, 1000]\n",
    "        }  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "kCAI_v8uOabV",
    "outputId": "11a1888b-093d-4e2a-f291-3369f5b4eba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Processing toxic comments...**\n",
      " **Processing RandomForestClassifier model...**\n",
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i, class_name in enumerate(TARGET_COLS):\n",
    "    print('**Processing {} comments...**'.format(class_name))\n",
    "\n",
    "    best_score = 0\n",
    "    best_model_with_params = tuple()\n",
    "\n",
    "    for name in models.keys():\n",
    "      print(' **Processing {} model...**'.format(name))\n",
    "      est = models[name]\n",
    "      est_params = params[name]\n",
    "      gscv = GridSearchCV(estimator=est, param_grid=est_params, iid=False, cv=4, \n",
    "                          verbose=1, n_jobs=-1, scoring='roc_auc')\n",
    "      gscv.fit(train_features, train_labels[:, i])\n",
    "      print(\"Best score for the model\\n{}\\nis:{}\".format(gscv.best_estimator_, gscv.best_score_))\n",
    "\n",
    "      if gscv.best_score_ > best_score:\n",
    "        best_score = gscv.best_score_\n",
    "        best_model = gscv.best_estimator_ \n",
    "    \n",
    "    best_model.fit(train_features, train_labels[:, i])\n",
    "    submission[class_name] = best_model.predict_proba(test_features)[:, 1]\n",
    "\n",
    "    print(\"\\nBest score for the class {} is {}\\n\".format(class_name, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQeEcE3N8oL8"
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ujanlBVs61IJ"
   },
   "source": [
    "## Cheaty evaluating on test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YUF4Uxrl8pXr"
   },
   "outputs": [],
   "source": [
    "# labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)\n",
    "test_labels_df = pd.read_csv(test_labels_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6crgLcLx7X-p"
   },
   "outputs": [],
   "source": [
    "test_labels_df = test_labels_df[(test_labels_df[\"toxic\"] != -1) &\n",
    "                                (test_labels_df[\"severe_toxic\"] != -1) &\n",
    "                                (test_labels_df[\"obscene\"] != -1) &\n",
    "                                (test_labels_df[\"threat\"] != -1) &\n",
    "                                (test_labels_df[\"insult\"] != -1) &\n",
    "                                (test_labels_df[\"identity_hate\"] != -1)]\n",
    "test_labels_df.shape                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ymumFOQ9sA1"
   },
   "outputs": [],
   "source": [
    "submission_to_evaluate = submission[submission['id'].isin(test_labels_df['id'].values)]\n",
    "submission_to_evaluate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3r8VTCK_TaM"
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for class_name in TARGET_COLS:\n",
    "    train_target = test_labels_df[class_name]\n",
    "    train_predicted = submission_to_evaluate[class_name]\n",
    "\n",
    "    cv_score = np.mean(roc_auc_score(train_target.values, train_predicted.values))\n",
    "    scores.append(cv_score)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKsSnwkd3TBJ"
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission_tfidf_sklean.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0o-WGik8qtjo"
   },
   "source": [
    "TFIDF_+_logreg  0.97620"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NLP advance course. HW3. Distributive semantic. TFIDF_+_sklearn_models.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
