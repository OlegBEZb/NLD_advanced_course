{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "NLP advance course. HW3. Distributive semantic. Embedings_+_Bidirectional_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GRh7ig80jk3s",
        "1_nWe7O-jqak",
        "TGzxJch3juGy",
        "sWmhQEFAj1VE",
        "_yDXCBmqtkPl",
        "0hTlPvNY8gi1"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlegBEZb/NLP_advanced_course/blob/master/HW1/NLP%20advance%20course.%20HW3.%20Distributive%20semantic.%20Embedings_%2B_Bidirectional_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVRaU_zG-PaI",
        "colab_type": "text"
      },
      "source": [
        "## Embeddings to check\n",
        "\n",
        "* Word Embeddings (word2vec, GloVe, etc.)\n",
        "* Starspace and other similarity learning embeddings\n",
        "* char-gram embeddings (bpemb, fasttext etc.)\n",
        "* doc and sentence embeddings (doc2vec, sent2vec etc.)\n",
        "* specific context models (BERT, ELMo)\n",
        "* Advanced: Poincare Embeddings concept"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30D0Scxk4kn3",
        "colab_type": "text"
      },
      "source": [
        "# Libs import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ZnXUdrxQ3TAN",
        "colab_type": "code",
        "outputId": "c477c575-a24c-4609-a5be-5aaaccfcf02a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "import os, math, operator, csv, random, pickle, re, sys\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import gc\n",
        "from collections import Counter\n",
        "\n",
        "import keras\n",
        "from keras.layers import MaxPooling1D, BatchNormalization, Permute, Lambda, \\\n",
        "Activation, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Dense, \\\n",
        "Embedding, Dropout, Input, CuDNNGRU, merge, CuDNNLSTM, Flatten, \\\n",
        "TimeDistributed, concatenate, SpatialDropout1D, Bidirectional\n",
        "from nltk.util import pad_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords');\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "! pip install Unidecode;\n",
        "from unidecode import unidecode\n",
        "#! pip install pyspellchecker;\n",
        "#from spellchecker import SpellChecker\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
        "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, CuDNNGRU, Conv1D\n",
        "from keras.preprocessing import text\n",
        "from keras.callbacks import LearningRateScheduler, Callback\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.1.1\n",
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypZqfsNI6-Ne",
        "colab_type": "text"
      },
      "source": [
        "# Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JtxOKtA35gA",
        "colab_type": "code",
        "trusted": true,
        "outputId": "97d8856b-7aad-49c4-9651-9817c781e5ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if tf.test.is_gpu_available(\n",
        "    cuda_only=False,\n",
        "    min_cuda_compute_capability=None\n",
        "):\n",
        "    print(tf.test.gpu_device_name())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i673BlTJEazT",
        "colab_type": "text"
      },
      "source": [
        "# Global vars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okymGz_lEjLC",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "COLAB = True\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "LSTM_UNITS = 128\n",
        "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
        "GLOBAL_EPOCHS = 2\n",
        "EPOCHS = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD2z8D2V7Dod",
        "colab_type": "text"
      },
      "source": [
        "# Authorization on Google drive and configurings paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auzLnrdK51Rg",
        "colab_type": "code",
        "trusted": true,
        "outputId": "9e7e45ca-e0e7-4773-ab0a-ef60838091cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "if COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    homework_folder = os.path.join('/content/drive/My Drive', 'Advanced NLP/Homework 1: Classical classification task like Kaggle Toxic or Quora')\n",
        "    data_folder = os.path.join(homework_folder, 'Toxic data')\n",
        "    embeddings_folder = os.path.join(homework_folder, 'embeddings')\n",
        "    print('data found:', os.listdir(data_folder))\n",
        "else: # in case of launching in Kaggle kernel\n",
        "    data_folder = '../input/jigsaw-toxic-comment-classification-challenge/'\n",
        "    embeddings_folder = '../input/glove-global-vectors-for-word-representation'\n",
        "    print('data found:', os.listdir(data_folder))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "data found: ['test.csv', 'test_labels.csv', 'train.csv', 'sample_submission.csv', 'submission.csv', 'submission_emb_sklearn.csv', 'submission_bidirectional_GRU.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UYdAXsdT3TAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_filepath = os.path.join(data_folder,\"train.csv\")\n",
        "test_filepath = os.path.join(data_folder,\"test.csv\")\n",
        "test_labels_filepath = os.path.join(data_folder,\"test_labels.csv\")\n",
        "\n",
        "#path to a submission\n",
        "submission_path = os.path.join(data_folder,\"sample_submission.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15GVrJYa717o",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYljyuKTCm8r",
        "colab_type": "text"
      },
      "source": [
        "## Dicts and lists of useful words and transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjL5O731CtDD",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#List of some words that often appear in toxic comments\n",
        "#Sorry about the level of toxicity in it!\n",
        "toxic_words = [\"poop\", \"crap\", \"prick\", \"twat\", \"wikipedia\", \"wiki\", \"hahahahaha\", \"lol\", \"bastard\", \"sluts\", \"slut\", \"douchebag\", \"douche\", \"blowjob\", \"nigga\", \"dumb\", \"jerk\", \"wanker\", \"wank\", \"penis\", \"motherfucker\", \"fucker\", \"fuk\", \"fucking\", \"fucked\", \"fuck\", \"bullshit\", \"shit\", \"stupid\", \"bitches\", \"bitch\", \"suck\", \"cunt\", \"dick\", \"cocks\", \"cock\", \"die\", \"kill\", \"gay\", \"jewish\", \"jews\", \"jew\", \"niggers\", \"nigger\", \"faggot\", \"fag\", \"asshole\"]\n",
        "astericks_words = [('mother****ers', 'motherfuckers'), ('motherf*cking', 'motherfucking'), ('mother****er', 'motherfucker'), ('motherf*cker', 'motherfucker'), ('bullsh*t', 'bullshit'), ('f**cking', 'fucking'), ('f*ucking', 'fucking'), ('fu*cking', 'fucking'), ('****ing', 'fucking'), ('a**hole', 'asshole'), ('assh*le', 'asshole'), ('f******', 'fucking'), ('f*****g', 'fucking'), ('f***ing', 'fucking'), ('f**king', 'fucking'), ('f*cking', 'fucking'), ('fu**ing', 'fucking'), ('fu*king', 'fucking'), ('fuc*ers', 'fuckers'), ('f*****', 'fucking'), ('f***ed', 'fucked'), ('f**ker', 'fucker'), ('f*cked', 'fucked'), ('f*cker', 'fucker'), ('f*ckin', 'fucking'), ('fu*ker', 'fucker'), ('fuc**n', 'fucking'), ('ni**as', 'niggas'), ('b**ch', 'bitch'), ('b*tch', 'bitch'), ('c*unt', 'cunt'), ('f**ks', 'fucks'), ('f*ing', 'fucking'), ('ni**a', 'nigga'), ('c*ck', 'cock'), ('c*nt', 'cunt'), ('cr*p', 'crap'), ('d*ck', 'dick'), ('f***', 'fuck'), ('f**k', 'fuck'), ('f*ck', 'fuck'), ('fc*k', 'fuck'), ('fu**', 'fuck'), ('fu*k', 'fuck'), ('s***', 'shit'), ('s**t', 'shit'), ('sh**', 'shit'), ('sh*t', 'shit'), ('tw*t', 'twat')]\n",
        "fasttext_misspelings = {\"'n'balls\": 'balls', \"-nazi's\": 'nazis', 'adminabuse': 'admin abuse', \"admins's\": 'admins', 'arsewipe': 'arse wipe', 'assfack': 'asshole', 'assholifity': 'asshole', 'assholivity': 'asshole', 'asshoul': 'asshole', 'asssholeee': 'asshole', 'belizeans': 'mexicans', \"blowing's\": 'blowing', 'bolivians': 'mexicans', 'celtofascists': 'fascists', 'censorshipmeisters': 'censor', 'chileans': 'mexicans', 'clerofascist': 'fascist', 'cowcrap': 'crap', 'crapity': 'crap', \"d'idiots\": 'idiots', 'deminazi': 'nazi', 'dftt': \"don't feed the troll\", 'dildohs': 'dildo', 'dramawhores': 'drama whores', 'edophiles': 'pedophiles', 'eurocommunist': 'communist', 'faggotkike': 'faggot', 'fantard': 'retard', 'fascismnazism': 'fascism', 'fascistisized': 'fascist', 'favremother': 'mother', 'fuxxxin': 'fucking', \"g'damn\": 'goddamn', 'harassmentat': 'harassment', 'harrasingme': 'harassing me', 'herfuc': 'motherfucker', 'hilterism': 'fascism', 'hitlerians': 'nazis', 'hitlerites': 'nazis', 'hubrises': 'pricks', 'idiotizing': 'idiotic', 'inadvandals': 'vandals', \"jackass's\": 'jackass', 'jiggabo': 'nigga', 'jizzballs': 'jizz balls', 'jmbass': 'dumbass', 'lejittament': 'legitimate', \"m'igger\": 'nigger', \"m'iggers\": 'niggers', 'motherfacking': 'motherfucker', 'motherfuckenkiwi': 'motherfucker', 'muthafuggas': 'niggas', 'nazisms': 'nazis', 'netsnipenigger': 'nigger', 'niggercock': 'nigger', 'niggerspic': 'nigger', 'nignog': 'nigga', 'niqqass': 'niggas', \"non-nazi's\": 'not a nazi', 'panamanians': 'mexicans', 'pedidiots': 'idiots', 'picohitlers': 'hitler', 'pidiots': 'idiots', 'poopia': 'poop', 'poopsies': 'poop', 'presumingly': 'obviously', 'propagandaanddisinformation': 'propaganda and disinformation', 'propagandaministerium': 'propaganda', 'puertoricans': 'mexicans', 'puertorricans': 'mexicans', 'pussiest': 'pussies', 'pussyitis': 'pussy', 'rayaridiculous': 'ridiculous', 'redfascists': 'fascists', 'retardzzzuuufff': 'retard', \"revertin'im\": 'reverting', 'scumstreona': 'scums', 'southamericans': 'mexicans', 'strasserism': 'fascism', 'stuptarded': 'retarded', \"t'nonsense\": 'nonsense', \"threatt's\": 'threat', 'titoists': 'communists', 'twatbags': 'douchebags', 'youbollocks': 'you bollocks'}\n",
        "acronym_words = {\"btw\":\"by the way\", \"yo\": \"you\", \"u\": \"you\", \"r\": \"are\", \"ur\": \"your\", \"ima\": \"i am going to\", \"imma\": \"i am going to\", \"i'ma\":\"i am going to\", \"cos\":\"because\", \"coz\":\"because\", \"stfu\": \"shut the fuck up\", \"wat\": \"what\"}\n",
        "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
        "\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "#spell_checker = SpellChecker()\n",
        "\n",
        "cont_patterns = [\n",
        "    (r'(W|w)on\\'t', r'will not'),\n",
        "    (r'(C|c)an\\'t', r'can not'),\n",
        "    (r'(I|i)\\'m', r'i am'),\n",
        "    (r'(A|a)in\\'t', r'is not'),\n",
        "    (r'(\\w+)\\'ll', r'\\g<1> will'),\n",
        "    (r'(\\w+)n\\'t', r'\\g<1> not'),\n",
        "    (r'(\\w+)\\'ve', r'\\g<1> have'),\n",
        "    (r'(\\w+)\\'s', r'\\g<1> is'),\n",
        "    (r'(\\w+)\\'re', r'\\g<1> are'),\n",
        "    (r'(\\w+)\\'d', r'\\g<1> would'),\n",
        "]\n",
        "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
        "\n",
        "#We will filter all characters except alphabet characters and some punctuation\n",
        "valid_characters = \" \" + \"@$\" + \"'!?-\" + \"abcdefghijklmnopqrstuvwxyz\" + \"abcdefghijklmnopqrstuvwxyz\".upper()\n",
        "valid_set = set(x for x in valid_characters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs_nlu1iILY1",
        "colab_type": "text"
      },
      "source": [
        "## Funcs for processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfUaIP-wIKTZ",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def replace_url(word):\n",
        "    if \"http://\" in word or \"www.\" in word or \"https://\" in word or \"wikipedia.org\" in word:\n",
        "        return \"\"\n",
        "    return word\n",
        "\n",
        "\n",
        "def word_tokenize(sentence, tokenizer):\n",
        "    sentence = sentence.replace(\"$\", \"s\")\n",
        "    sentence = sentence.replace(\"@\", \"a\")    \n",
        "    sentence = sentence.replace(\"!\", \" ! \")\n",
        "    sentence = sentence.replace(\"?\", \" ? \")\n",
        "\n",
        "    return tokenizer.tokenize(sentence)\n",
        "\n",
        "\n",
        "def split_toxic_and_normal_parts(word, toxic_words):\n",
        "    if word == \"\":\n",
        "        return \"\"\n",
        "    \n",
        "    lower = word.lower()\n",
        "    for toxic_word in toxic_words:\n",
        "        start = lower.find(toxic_word)\n",
        "        if start >= 0:\n",
        "            end = start + len(toxic_word)\n",
        "            result = \" \".join([word[0:start], word[start:end], split_toxic_and_normal_parts(word[end:], toxic_words)])\n",
        "            return result.replace(\"  \", \" \").strip()\n",
        "    return word\n",
        "\n",
        "\n",
        "def normalize_by_dictionary(normalized_word, dictionary):\n",
        "    result = []\n",
        "    for word in normalized_word.split():\n",
        "        if word == word.upper():\n",
        "            if word.lower() in dictionary:\n",
        "                result.append(dictionary[word.lower()].upper())\n",
        "            else:\n",
        "                result.append(word)\n",
        "        else:\n",
        "            if word.lower() in dictionary:\n",
        "                result.append(dictionary[word.lower()])\n",
        "            else:\n",
        "                result.append(word)\n",
        "    \n",
        "    return \" \".join(result)\n",
        "\n",
        "\n",
        "def normalize_comment(comment, tokenizer, max_comment_length):\n",
        "    comment = unidecode(comment)\n",
        "    comment = comment[:max_comment_length]\n",
        "\n",
        "    normalized_words = []\n",
        "    \n",
        "    # ('mother****ers', 'motherfuckers')\n",
        "    # for w in astericks_words:\n",
        "    #     if w[0] in comment:\n",
        "    #         comment = comment.replace(w[0], w[1])\n",
        "    #     if w[0].upper() in comment:\n",
        "    #         comment = comment.replace(w[0].upper(), w[1].upper())\n",
        "    \n",
        "    for word in word_tokenize(comment, tokenizer):\n",
        "        if word in english_stopwords: continue\n",
        "\n",
        "        # # '(W|w)on\\'t', r'will not'\n",
        "        # for (pattern, repl) in patterns:\n",
        "        #    word = re.sub(pattern, repl, word)\n",
        "\n",
        "        if word == \".\" or word == \",\":\n",
        "            normalized_words.append(word)\n",
        "            continue\n",
        "        \n",
        "        # drop url parts from links\n",
        "        # word = replace_url(word)\n",
        "\n",
        "        # replace single dots to whitespaces\n",
        "        if word.count(\".\") == 1:\n",
        "            word = word.replace(\".\", \" \")\n",
        "\n",
        "        # leave only legalized symbols\n",
        "        filtered_word = \"\".join([x for x in word if x in valid_set])\n",
        "                    \n",
        "        #Kind of hack: for every word check if it has a toxic word as a part of it\n",
        "        #If so, split this word by swear and non-swear part.\n",
        "        #normalized_word = split_toxic_and_normal_parts(filtered_word, toxic_words)\n",
        "        normalized_word = filtered_word\n",
        "\n",
        "#         normalized_word = normalize_by_dictionary(normalized_word, hyphens_dict)\n",
        "#         normalized_word = normalize_by_dictionary(normalized_word, merged_dict)\n",
        "        \n",
        "        # check misspellings\n",
        "        # temp = []\n",
        "        # for word in normalized_word.split():\n",
        "        #   temp.append(spell_checker.correction(word))\n",
        "        # normalized_word = \" \".join(temp)\n",
        "          \n",
        "        # normalized_word = normalize_by_dictionary(normalized_word, fasttext_misspelings)\n",
        "        normalized_word = normalize_by_dictionary(normalized_word, acronym_words)\n",
        "\n",
        "        normalized_words.append(normalized_word)\n",
        "        \n",
        "    normalized_comment = \" \".join(normalized_words)\n",
        "    \n",
        "    result = []\n",
        "    for word in normalized_comment.split():\n",
        "        # if word is upper\n",
        "        if word.upper() == word:\n",
        "            result.append(word)\n",
        "        else:\n",
        "            result.append(word.lower())\n",
        "    \n",
        "    #apparently, people on wikipedia love to talk about sockpuppets :-)\n",
        "    result = \" \".join(result)\n",
        "    if \"sock puppet\" in result:\n",
        "        result = result.replace(\"sock puppet\", \"sockpuppet\")\n",
        "    \n",
        "    if \"SOCK PUPPET\" in result:\n",
        "        result = result.replace(\"SOCK PUPPET\", \"SOCKPUPPET\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "def preprocess_text(df, preprocessing_func, tokenizer, max_comment_length):\n",
        "  text_ndarray = df.fillna('_na').values\n",
        "  np_preprocessing_func = np.vectorize(preprocessing_func)\n",
        "\n",
        "  preprocessed_text = np_preprocessing_func(text_ndarray, tokenizer, max_comment_length)\n",
        "\n",
        "  print('Gained shape:', preprocessed_text.shape)\n",
        "  return preprocessed_text\n",
        "\n",
        "\n",
        "def tokenize_and_pad(text, tokenizer):\n",
        "\n",
        "  tokenized_text = tokenizer.texts_to_sequences(text)\n",
        "  padded_text = pad_sequences(tokenized_text, maxlen=MAX_LEN, dtype='int', value=0)\n",
        "\n",
        "  print('Gained shape:', padded_text.shape)\n",
        "  return padded_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn1-XTZEIZ3y",
        "colab_type": "text"
      },
      "source": [
        "## Loading frames and processing them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AWRdxZsHG3B",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "TEXT_COLUMN = 'comment_text'\n",
        "TARGET_COLS = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "\n",
        "MAX_LEN = 220\n",
        "max_tokens = 20000\n",
        "max_comment_length = 20000 #We are going to truncate a comment if its length > threshold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAe8xpV48CO1",
        "colab_type": "code",
        "trusted": true,
        "outputId": "08f6a778-1e62-453e-a42f-da2732d8c513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "train_df = pd.read_csv(train_filepath)\n",
        "train_labels = train_df[TARGET_COLS].values"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 653 ms, sys: 70.9 ms, total: 724 ms\n",
            "Wall time: 767 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxwo5A8VfRRL",
        "colab_type": "code",
        "trusted": true,
        "outputId": "4719aa8f-b8f3-4ee6-d686-d91225ae8657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
        "\n",
        "preprocessed_train = preprocess_text(train_df[TEXT_COLUMN], \n",
        "                                     normalize_comment, \n",
        "                                     tknzr,\n",
        "                                     max_comment_length)\n",
        "# # preprocessed_train = train_df[TEXT_COLUMN].fillna(\"CVxTz\").values\n",
        "# print(preprocessed_train[:3])\n",
        "# preprocessed_train = train_df[TEXT_COLUMN]\n",
        "\n",
        "del train_df"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gained shape: (159571,)\n",
            "CPU times: user 56.3 s, sys: 3.02 s, total: 59.3 s\n",
            "Wall time: 59.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Waa0TI1tqpPt",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_tokens)\n",
        "tokenizer.fit_on_texts(list(preprocessed_train))\n",
        "\n",
        "tokenized_train = tokenize_and_pad(preprocessed_train, tokenizer)\n",
        "# list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
        "# list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
        "#print(tokenized_train[:3])\n",
        "\n",
        "del preprocessed_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h3CsFuN9TpL",
        "colab_type": "code",
        "trusted": true,
        "outputId": "187bb207-de88-443b-aed1-852fc6b20cc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "test_df = pd.read_csv(test_filepath)\n",
        "\n",
        "preprocessed_test = preprocess_text(test_df[TEXT_COLUMN], \n",
        "                                    normalize_comment, \n",
        "                                    tknzr,\n",
        "                                    max_comment_length)\n",
        "# preprocessed_test = test_df[TEXT_COLUMN].fillna(\"CVxTz\").values\n",
        "# preprocessed_test = test_df[TEXT_COLUMN]\n",
        "\n",
        "\n",
        "\n",
        "#del preprocessed_test\n",
        "del test_df\n",
        "gc.collect()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gained shape: (153164,)\n",
            "CPU times: user 52.5 s, sys: 260 ms, total: 52.7 s\n",
            "Wall time: 53.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjsjwMkWvVCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_test = tokenize_and_pad(preprocessed_test, tokenizer)\n",
        "\n",
        "del preprocessed_test\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8APgvUyskaf",
        "colab_type": "text"
      },
      "source": [
        "# Training model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCRY8qnNJTh_",
        "colab_type": "text"
      },
      "source": [
        "## Preparing features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jqSUK8Dpd3Q",
        "colab_type": "code",
        "outputId": "3d43d16b-2699-43e9-ebd9-ca298ab894c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "embedding_len = 1536\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' %len(word_index))\n",
        "\n",
        "#del tokenizer\n",
        "gc.collect()\n",
        "\n",
        "num_words = min(max_tokens, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_words, embedding_len)) # zeroth row for UNK\n",
        "print('embedding matrix shape', embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 184924 unique tokens.\n",
            "embedding matrix shape (20000, 1536)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRh7ig80jk3s",
        "colab_type": "text"
      },
      "source": [
        "### GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BNjZiXPZEYvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding_len = 200\n",
        "# words = pd.read_csv(os.path.join(embeddings_folder, \"glove.twitter.27B.{}d.txt\".format(embedding_len)), \n",
        "#                     sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
        "\n",
        "# embedding_len = 300\n",
        "# words = pd.read_csv(os.path.join(embeddings_folder, \"glove.840B.300d.txt\"), \n",
        "#                     sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
        "\n",
        "\n",
        "# vocab_words = words[words.index.isin(list(word_index.keys()))]\n",
        "# print('vocab_words.shape', vocab_words.shape)\n",
        "\n",
        "# del words\n",
        "# gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0-ZU5VYq-Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_vec(word, words_df):\n",
        "#     return words_df.loc[word].values\n",
        "\n",
        "\n",
        "\n",
        "# for word, i in word_index.items():\n",
        "#   if i >= max_tokens:\n",
        "#     continue\n",
        "#   try:\n",
        "#     embedding_vector = get_vec(word, vocab_words)\n",
        "#   except:\n",
        "#     continue\n",
        "#   if embedding_vector is not None:\n",
        "#     # words not found in embedding index will be all-zeros.\n",
        "#     embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# # %%time\n",
        "# # embedding_matrix = np.load('../input/embedding-2/embedding_matrix_big.npy')\n",
        "    \n",
        "# print('embedding_matrix.shape', embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_nWe7O-jqak",
        "colab_type": "text"
      },
      "source": [
        "### StarSpace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD8SO8xXfsVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Starspace attempt\n",
        "\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# count_vectorizer = CountVectorizer(min_df=0, max_df=0.99, max_features=10000)\n",
        "# X_train = count_vectorizer.fit_transform(article_contents.main_content.iloc[0:train_row])\n",
        "# X_train = count_vectorizer.inverse_transform(X_train)\n",
        "# label_prefix = '__label__'\n",
        "# with open(\"train_starspace.txt\", 'w+') as file:\n",
        "#     for i in range(10):\n",
        "#         file.write(' '.join(preprocessed_train[i].split(' ')) + ' ' + label_prefix + train_labels[i])\n",
        "#         file.write('\\n')\n",
        "# file.close()\n",
        "\n",
        "# The result file will look like this (all separeted by space, and label will have prefix __label__)\n",
        "# how are you ... __label__b\n",
        "# this is just an example ... __label__c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGzxJch3juGy",
        "colab_type": "text"
      },
      "source": [
        "### FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voep54c6Q-I-",
        "colab_type": "code",
        "outputId": "64afe0b8-56c3-40e5-87f5-7372096ffc2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (42.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.17.4)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBeehRN-oS9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import codecs\n",
        "\n",
        "# #load embeddings\n",
        "# print('loading word embeddings...')\n",
        "# embeddings_index = {}\n",
        "# f = codecs.open(os.path.join(embeddings_folder, 'crawl-300d-2M-subword.vec'), encoding='utf-8')\n",
        "# for line in tqdm(f, mininterval=5):\n",
        "#     values = line.rstrip().rsplit(' ')\n",
        "#     word = values[0]\n",
        "#     coefs = np.asarray(values[1:], dtype='float32')\n",
        "#     embeddings_index[word] = coefs\n",
        "# f.close()\n",
        "# print('found %s word vectors' % len(embeddings_index))\n",
        "# print('preparing embedding matrix...')\n",
        "# words_not_found = []\n",
        "\n",
        "\n",
        "# for word, i in word_index.items():\n",
        "#     if i >= max_tokens:\n",
        "#         continue\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "#     else:\n",
        "#         words_not_found.append(word)\n",
        "# print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "\n",
        "\n",
        "# words_not_found[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWmhQEFAj1VE",
        "colab_type": "text"
      },
      "source": [
        "### BPEmb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdMBN-HPjjek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install bpemb;\n",
        "# from bpemb import BPEmb\n",
        "# # load English BPEmb model with default vocabulary size of max_tokens\n",
        "# bpemb_en = BPEmb(lang=\"en\", dim=embedding_len, vs=max_tokens)\n",
        "# bpemb_en.vectors.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JQn_7WjrNP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # mean vector of word parts\n",
        "# def get_vec(word):\n",
        "#     vec = bpemb_en.embed(word)\n",
        "#     return vec.mean(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSAJK0AYsdW2",
        "colab_type": "code",
        "trusted": true,
        "outputId": "bd9383ce-edd7-4c2f-b258-1dea1d46675c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# %%time\n",
        "\n",
        "# for word, i in word_index.items():\n",
        "#   if i >= max_tokens:\n",
        "#     continue\n",
        "#   try:\n",
        "#     embedding_vector = get_vec(word)\n",
        "#   except:\n",
        "#     continue\n",
        "#   if embedding_vector is not None:\n",
        "#     # words not found in embedding index will be all-zeros.\n",
        "#     embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# print('embedding_matrix.shape', embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding_matrix.shape (20000, 300)\n",
            "CPU times: user 504 ms, sys: 18.1 ms, total: 522 ms\n",
            "Wall time: 520 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2ezEW492Nnj",
        "colab_type": "text"
      },
      "source": [
        "### Flair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScKTCXs-trr_",
        "colab_type": "code",
        "outputId": "01af3e73-bbbf-4cbf-9253-3a2dd3befe54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! pip install tiny-tokenizer\n",
        "! pip install flair\n",
        "\n",
        "import flair\n",
        "from flair.data import Sentence\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tiny-tokenizer\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/0f/aa52c227c5af69914be05723b3deaf221805a4ccbce87643194ef2cdde43/tiny_tokenizer-3.1.0.tar.gz\n",
            "Building wheels for collected packages: tiny-tokenizer\n",
            "  Building wheel for tiny-tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tiny-tokenizer: filename=tiny_tokenizer-3.1.0-cp36-none-any.whl size=10550 sha256=aa15269699b787314d1c790861bba985df9096ce9ad75c95d32baf09b8b5ec6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/c8/36/334497a689fab90128232e86b5829b800dd271a3d5d5959c53\n",
            "Successfully built tiny-tokenizer\n",
            "Installing collected packages: tiny-tokenizer\n",
            "Successfully installed tiny-tokenizer-3.1.0\n",
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/22/8fc8e5978ec05b710216735ca47415700e83f304dec7e4281d61cefb6831/flair-0.4.4-py3-none-any.whl (193kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 2.8MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.2.9\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Collecting transformers>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 48.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 51.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from flair) (3.10.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.6)\n",
            "Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.2)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from flair) (0.2.0)\n",
            "Requirement already satisfied: tiny-tokenizer[all] in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.0)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/89/62912e01f3cede11edcc0abf81298e3439d9c06c8dce644369380ed13f6d/Deprecated-1.2.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.3.1)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from flair) (0.4.2)\n",
            "Collecting ipython==7.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/2c/c7d44277b599df35af734d8f4142d501192fdb7aef5d04daf882d7eccfbc/ipython-7.6.1-py3-none-any.whl (774kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 35.3MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 43.1MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (1.17.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (1.10.47)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.22.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->flair) (1.12.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (42.0.2)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (8.0.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
            "Collecting SudachiDict-core@ https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_core-20190927.tar.gz ; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_core-20190927.tar.gz (70.7MB)\n",
            "\u001b[K     |████████████████████████████████| 70.7MB 44kB/s \n",
            "\u001b[?25hCollecting SudachiPy; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/c9/40bfb291a7995ad218451ef97083432f998b822e3ecbd9f586f593d2cfb6/SudachiPy-0.4.2-py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.2MB/s \n",
            "\u001b[?25hCollecting janome; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/f0/bd7f90806132d7d9d642d418bdc3e870cfdff5947254ea3cab27480983a7/Janome-0.3.10-py2.py3-none-any.whl (21.5MB)\n",
            "\u001b[K     |████████████████████████████████| 21.5MB 158kB/s \n",
            "\u001b[?25hCollecting natto-py; extra == \"all\"\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/14/1d4258247a00b7b8a115563effb1d0bd30501d69580629d36593ce0af92d/natto-py-0.9.2.tar.gz\n",
            "Collecting kytea; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/bc/702d01a96d5d094bd9f3c2eb1d12153daf8edf7bf5d78b9a2dae1202df07/kytea-0.1.4-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 30.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.2)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.9.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->flair) (6.2.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.4.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.1.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (2.1.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.7.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.7.5)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/61/2dfea88583d5454e3a64f9308a686071d58d59a55db638268a6413e1eb6d/prompt_toolkit-2.0.10-py3-none-any.whl (340kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 44.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.15.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.3.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (1.13.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (0.14.1)\n",
            "Collecting dartsclone~=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/4d/45acbe9d0795d8ceef0fee1f9ac2dcbf27dca3a0578a023fcdc3fef6fd89/dartsclone-0.6.tar.gz\n",
            "Requirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (2.1.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (1.13.2)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython==7.6.1->flair) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.6.1->flair) (0.1.8)\n",
            "Requirement already satisfied: parso>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython==7.6.1->flair) (0.5.2)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers>=2.0.0->flair) (0.15.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.6.0->SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (0.29.14)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (2.19)\n",
            "Building wheels for collected packages: langdetect, segtok, sqlitedict, mpld3, sacremoses, SudachiDict-core, natto-py, dartsclone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-cp36-none-any.whl size=993460 sha256=65bda05c34cc4f7d2803f6893027a63e2645e041b68e14522d5c50dedfda6109\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.7-cp36-none-any.whl size=23258 sha256=a6068dca9b07d0d23c5b2578cd91ef376e73852632a166e378c7b110177d5e19\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=db59b3364116b169a671b96c7c5deb471b6d402d4b0645175eef90fb39422521\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=787a055e5a4ff87af3e8b3181f09441fbba207b4b392baa00e0a0b7d410c92ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=a9bc6ff7947fee6ea9cb980e90406dbce3648c3047162a4fd6e55e8e9c6fca29\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "  Building wheel for SudachiDict-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SudachiDict-core: filename=SudachiDict_core-20190927-cp36-none-any.whl size=70878518 sha256=928042bfce672ea42c8ee0a4c92767c51249df8109d969ad92eeb9979adfd8fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/d8/6e/b107d7fef6e80915aa1e46db741b98a3da011f567526347ccc\n",
            "  Building wheel for natto-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for natto-py: filename=natto_py-0.9.2-cp36-none-any.whl size=45164 sha256=b575b15b9aef2c24bc8e25450742d03fcbf00a323ecc3ac97ccd477615c70910\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/51/dd/67f87608b124a23eecf5c1fc3557cc0b7ffdeae33fe6ee89df\n",
            "  Building wheel for dartsclone (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dartsclone: filename=dartsclone-0.6-cp36-cp36m-linux_x86_64.whl size=413251 sha256=4b52a1db155cf8b637be04577552e255282764ba767ae7c523150b48026fbe49\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/cd/70/fe43307bf7398243155108f4f5a258ef336923d65ec4af93cd\n",
            "Successfully built langdetect segtok sqlitedict mpld3 sacremoses SudachiDict-core natto-py dartsclone\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.6.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, bpemb, sacremoses, transformers, langdetect, segtok, deprecated, sqlitedict, prompt-toolkit, ipython, mpld3, flair, dartsclone, SudachiPy, SudachiDict-core, janome, natto-py, kytea\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "Successfully installed SudachiDict-core-20190927 SudachiPy-0.4.2 bpemb-0.3.0 dartsclone-0.6 deprecated-1.2.7 flair-0.4.4 ipython-7.6.1 janome-0.3.10 kytea-0.1.4 langdetect-1.0.7 mpld3-0.3 natto-py-0.9.2 prompt-toolkit-2.0.10 sacremoses-0.0.38 segtok-1.5.7 sentencepiece-0.1.85 sqlitedict-1.6.0 transformers-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yDXCBmqtkPl",
        "colab_type": "text"
      },
      "source": [
        "#### ELMo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVZ4xx34vSmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNDAZuOvt4wR",
        "colab_type": "code",
        "outputId": "306e7fb8-db28-4628-ea6e-c06cc0ae1183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from flair.embeddings import ELMoEmbeddings\n",
        "\n",
        "# init embedding\n",
        "embedding = ELMoEmbeddings('medium')\n",
        "\n",
        "# create a sentence #\n",
        "sentence = Sentence('test sent')\n",
        "# embed words in sentence #\n",
        "embedding.embed(sentence)\n",
        "embedding_len = sentence[0].embedding.size()[0]\n",
        "print('embedding len', embedding_len)\n",
        "\n",
        "\n",
        "def get_comment_emb(comment):\n",
        "  sentence = Sentence(comment)\n",
        "  embedding.embed(sentence)\n",
        "  emb_list = []\n",
        "  for word in sentence:\n",
        "    emb_list.append(word.embedding)\n",
        "  return emb_list\n",
        "\n",
        "get_comment_emb_vec = np.vectorize(get_comment_emb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding len 1536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFD8Xd565HgQ",
        "colab_type": "code",
        "outputId": "aaea15ab-6a6f-4f01-c47f-7d111ebd707c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for word, i in tqdm(word_index.items(), total=len(word_index)):\n",
        "    if i >= max_tokens:\n",
        "        continue\n",
        "    try:\n",
        "        word_sent = Sentence(word)\n",
        "        embedding.embed(word_sent)\n",
        "        embedding_vector = word_sent[0].embedding.cpu().detach().numpy()\n",
        "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "           embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        pass\n",
        "        #embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 184924/184924 [03:02<00:00, 1014.44it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6tfsIpFJ6YG",
        "colab_type": "code",
        "outputId": "c515518f-a7ce-4831-cf0e-713070b8baf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "del embedding\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I7VZVVF69Fh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # creating a tensor for storing sentence embeddings #\n",
        "# s = torch.zeros(0, embedding_len)\n",
        "\n",
        "# # iterating Sentence (tqdm tracks progress) #\n",
        "# for tweet in tqdm(preprocessed_train[10]):   \n",
        "#   # empty tensor for words #\n",
        "#   w = torch.zeros(0, embedding_len)   \n",
        "#   sentence = Sentence(tweet)\n",
        "#   embedding.embed(sentence)\n",
        "#   # for every word #\n",
        "#   for token in sentence:\n",
        "#     # storing word Embeddings of each word in a sentence #\n",
        "#     w = torch.cat((w,token.embedding.view(-1, embedding_len)),0)\n",
        "#   # storing sentence Embeddings (mean of embeddings of all words)   #\n",
        "#   s = torch.cat((s, w.mean(dim = 0).view(-1, embedding_len)),0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hTlPvNY8gi1",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gEnMtmwooBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.embeddings import RoBERTaEmbeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvXWeyYWo0J9",
        "colab_type": "code",
        "outputId": "0c27d2a0-5e69-4428-fed4-20262dc73bc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# init embedding\n",
        "embedding = RoBERTaEmbeddings('roberta-base')\n",
        "\n",
        "# create a sentence #\n",
        "sentence = Sentence('test sent')\n",
        "# embed words in sentence #\n",
        "embedding.embed(sentence)\n",
        "embedding_len = sentence[0].embedding.size()[0]\n",
        "print('embedding len', embedding_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding len 768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDdLhAjJqY-G",
        "colab_type": "code",
        "outputId": "5cf088ff-3fd9-4e44-f690-e37eb45c37db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for word, i in tqdm(word_index.items(), total=len(word_index)):\n",
        "    if i >= max_tokens:\n",
        "        continue\n",
        "    try:\n",
        "        word_sent = Sentence(word)\n",
        "        embedding.embed(word_sent)\n",
        "        embedding_vector = word_sent[0].embedding.cpu().detach().numpy()\n",
        "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "           embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        pass\n",
        "        #embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "del embedding\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 184924/184924 [04:29<00:00, 685.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McJb1Sn5tw4a",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa + ELMo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztnGoOlrt3PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install allennlp\n",
        "from flair.embeddings import StackedEmbeddings, RoBERTaEmbeddings, ELMoEmbeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_4YEiP1t5-n",
        "colab_type": "code",
        "outputId": "7009dff1-41c1-45be-b19b-bae803e40e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "elmo_embeddings = ELMoEmbeddings('small')\n",
        "roberta_embeddings = RoBERTaEmbeddings('roberta-base')\n",
        "stacked_embeddings = StackedEmbeddings([\n",
        "                                        elmo_embeddings,\n",
        "                                        roberta_embeddings,\n",
        "                                       ])\n",
        "\n",
        "# create a sentence #\n",
        "sentence = Sentence('test sent')\n",
        "# embed words in sentence #\n",
        "stacked_embeddings.embed(sentence)\n",
        "embedding_len = sentence[0].embedding.size()[0]\n",
        "print('embedding len', embedding_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 336/336 [00:00<00:00, 742511.14B/s]\n",
            "100%|██████████| 54402456/54402456 [00:02<00:00, 20212502.71B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "embedding len 1536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJNcWzMduAtb",
        "colab_type": "code",
        "outputId": "c878817b-90ae-40ce-a383-cd2934362b5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for word, i in tqdm(word_index.items(), total=len(word_index)):\n",
        "    if i >= max_tokens:\n",
        "        continue\n",
        "    try:\n",
        "        word_sent = Sentence(word)\n",
        "        stacked_embeddings.embed(word_sent)\n",
        "        embedding_vector = word_sent[0].embedding.cpu().detach().numpy()\n",
        "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "           embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        pass\n",
        "        #embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 184924/184924 [11:48<00:00, 260.89it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMl76miAyRyI",
        "colab_type": "code",
        "outputId": "3f1eae01-4bf3-4e00-ec66-7ec4bf4c2c87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "del stacked_embeddings\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "549"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyxmTYb4Q2dp",
        "colab_type": "text"
      },
      "source": [
        "### BERT fine-tuned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE-uRZUmQ4d9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahIw1Ve5RilL",
        "colab_type": "code",
        "outputId": "b40cfb76-44c2-4642-8a59-f4311b177a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#Installing BERT module\n",
        "!pip install bert-tensorflow"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 22.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 1.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpD98b1FRnvd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "879bc089-f0da-4377-e4f4-240d490030b1"
      },
      "source": [
        "#Importing BERT modules\n",
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dP_DlOkSZkx",
        "colab_type": "code",
        "outputId": "b9fd90fc-bcb0-486e-93c4-7f62061981a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "OUTPUT_DIR = os.path.join(homework_folder, 'bert_output')\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: /content/drive/My Drive/Advanced NLP/Homework 1: Classical classification task like Kaggle Toxic or Quora/bert_output *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMoU4oQJW8yi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "a8e86f8a-3fdd-4e7a-a3bf-52e8d83c8fee"
      },
      "source": [
        "preprocessed_train_with_labels = pd.concat([pd.DataFrame(preprocessed_train, columns=['preprocessed_text']),\n",
        "                                            pd.DataFrame(train_labels, columns=TARGET_COLS)], axis=1)\n",
        "preprocessed_train_with_labels.head(2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessed_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>explanation why edits made username hardcore m...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>d'aww ! he matches background colour i'm seemi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   preprocessed_text  ...  identity_hate\n",
              "0  explanation why edits made username hardcore m...  ...              0\n",
              "1  d'aww ! he matches background colour i'm seemi...  ...              0\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTP1fY-25QaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_examples(df, labels_available=True):\n",
        "  examples = []\n",
        "  for (i, row) in enumerate(df.values):\n",
        "    guid = None#row[0]\n",
        "    text_a = row[0]\n",
        "    if labels_available:\n",
        "      labels = tuple(row[1:])\n",
        "    else:\n",
        "      labels = tuple([0, 0, 0, 0, 0, 0])\n",
        "    examples.append(bert.run_classifier.InputExample(guid=guid, text_a = text_a, \n",
        "                                                     label = labels))\n",
        "  return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vJlMO4277_z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1992327e-7f47-4e94-c924-ac6a2f037454"
      },
      "source": [
        "%%time\n",
        "\n",
        "input_examples = create_examples(preprocessed_train_with_labels, labels_available=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 878 ms, sys: 22 ms, total: 900 ms\n",
            "Wall time: 901 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMiU0RrN77JS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3d937fa0-46dd-4458-888e-c581cf28dd2d"
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "bert_tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8svY6MK6-1Ur",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4a87e790-b2e1-44fd-f6cd-5ac3608a0bd0"
      },
      "source": [
        "%%time\n",
        "\n",
        "train_input_examples, valid_input_examples = train_test_split(input_examples, test_size = 0.1, shuffle=True)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 58.8 ms, sys: 991 µs, total: 59.8 ms\n",
            "Wall time: 59.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBu7EGV_AXv4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a3691bc-7a09-4251-a250-20be6d28cb92"
      },
      "source": [
        "len(train_input_examples), type(train_input_examples[0])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(143613, bert.run_classifier.InputExample)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b75PY9KeMULC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c35b8b6d-c0eb-4e1c-a81e-83f14f6c3820"
      },
      "source": [
        "train_input_examples[0].label"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 0, 1, 0, 0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS5SmpT1_r_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LENGTH = MAX_LEN\n",
        "\n",
        "# Convert our train and validation features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_input_examples, TARGET_COLS, MAX_SEQ_LENGTH, bert_tokenizer)\n",
        "\n",
        "val_features = bert.run_classifier.convert_examples_to_features(valid_input_examples, TARGET_COLS, MAX_SEQ_LENGTH, bert_tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AslnHsTy0gCQ",
        "colab_type": "text"
      },
      "source": [
        "## Build and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m6GDvsd3WRX",
        "colab_type": "code",
        "trusted": true,
        "outputId": "fdac63b2-b009-4c48-a5dd-127d8436cdfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "x_train, x_valid, y_train, y_valid = train_test_split(tokenized_train, \n",
        "                                                      train_labels, \n",
        "                                                      test_size = 0.1,\n",
        "                                                      shuffle=True)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_valid shape:', x_valid.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_valid shape:', y_valid.shape)\n",
        "\n",
        "del tokenized_train\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (143613, 220)\n",
            "x_valid shape: (15958, 220)\n",
            "y_train shape: (143613, 6)\n",
            "y_valid shape: (15958, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6306i8ha3TBF",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def build_model(embedding_matrix):\n",
        "    words = Input(shape=(MAX_LEN,))\n",
        "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
        "    #x = Input(batch_shape=(batch_size, max_comment_length, embedding_len))\n",
        "    \n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x)\n",
        "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "\n",
        "    hidden = concatenate([\n",
        "        GlobalMaxPooling1D()(x),\n",
        "        GlobalAveragePooling1D()(x),\n",
        "    ])\n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
        "    result = Dense(len(TARGET_COLS), activation='sigmoid')(hidden)\n",
        "    \n",
        "    \n",
        "    model = Model(inputs=words, outputs=result)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuPRFfOJf-tH",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# class RocAucEvaluation(Callback):\n",
        "#     def __init__(self, validation_data=(), interval=1):\n",
        "#         super(Callback, self).__init__()\n",
        "\n",
        "#         self.interval = interval\n",
        "#         self.X_val, self.y_val = validation_data\n",
        "\n",
        "#     def on_epoch_end(self, epoch, logs={}):\n",
        "#         if epoch % self.interval == 0:\n",
        "#             y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "#             score = 0\n",
        "#             for i in range(6):\n",
        "#              score += roc_auc_score(self.y_val[:,i], y_pred[:,i])/6.\n",
        "#             print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
        "\n",
        "# RocAuc_val = RocAucEvaluation(validation_data=(x_valid, y_valid), interval = 1)\n",
        "\n",
        "class RocAucEarlyStopping(Callback):\n",
        "    \"\"\"Callback for early stopping based on roc auc on the validation set\"\"\"\n",
        "\n",
        "    def __init__(self, validation_data, patience=0, digits=3):\n",
        "        super().__init__()\n",
        "        self.X_val, self.y_val = validation_data\n",
        "        self.best = 0\n",
        "        self.patience = patience\n",
        "        self.digits = digits\n",
        "        self.current_patience = patience\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "        score = 0\n",
        "        for i in range(6):\n",
        "          score += roc_auc_score(self.y_val[:,i], y_pred[:,i])/6.\n",
        "\n",
        "        score = round(score, self.digits)\n",
        "        print(\"ROC-AUC - epoch: {:d} - score: {}\\n\".format(epoch+1, score))\n",
        "\n",
        "        # check digits\n",
        "        if np.greater(score, self.best):\n",
        "            self.best = score\n",
        "            self.current_patience = self.patience\n",
        "        else:\n",
        "            print('\\nbest:{}\\ncurrent:{}'.format(self.best, score))\n",
        "            self.current_patience -= 1\n",
        "            if self.current_patience < 0:\n",
        "              self.model.stop_training = True\n",
        "              print('Early stopping due to lower roc auc')\n",
        "              self.current_patience = self.patience\n",
        "            else:\n",
        "              print('{} patience remained'.format(self.current_patience))\n",
        "\n",
        "RocAuc_ES = RocAucEarlyStopping(validation_data=(x_valid, y_valid), patience=1, digits=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TRvdcnky3TBH",
        "colab_type": "code",
        "outputId": "24f73e36-a368-4a42-fe8b-2bb0901d3766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "SEEDS = 2\n",
        "pred = 0\n",
        "\n",
        "for ii in tqdm(range(SEEDS), total=SEEDS):\n",
        "\n",
        "    model = build_model(embedding_matrix)\n",
        "    for global_epoch in range(GLOBAL_EPOCHS):\n",
        "        print('\\nglobal_epoch', global_epoch)\n",
        "\n",
        "\n",
        "        model.fit(\n",
        "                    x_train,\n",
        "                    y_train,\n",
        "                    validation_data = (x_valid, y_valid),\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    verbose=1,\n",
        "                    callbacks=[\n",
        "                        LearningRateScheduler(lambda _: 1e-3 * (0.5 ** global_epoch)),\n",
        "                        #RocAuc_val\n",
        "                        RocAuc_ES\n",
        "                    ]\n",
        "                )\n",
        "        print('fitted')    \n",
        "\n",
        "    pred += model.predict(tokenized_test, batch_size = 1024, verbose = 1)/SEEDS\n",
        "    np.save('pred', pred)\n",
        "    model.save_weights('model_weights_'+str(ii)+'.h5')\n",
        "    os.system('gzip '+'model_weights_'+str(ii)+'.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "\n",
            "global_epoch 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/5\n",
            "143613/143613 [==============================] - 87s 605us/step - loss: 0.0700 - val_loss: 0.0497\n",
            "ROC-AUC - epoch: 1 - score: 0.9787\n",
            "\n",
            "Epoch 2/5\n",
            "143613/143613 [==============================] - 82s 569us/step - loss: 0.0473 - val_loss: 0.0446\n",
            "ROC-AUC - epoch: 2 - score: 0.9856\n",
            "\n",
            "Epoch 3/5\n",
            "143613/143613 [==============================] - 82s 569us/step - loss: 0.0429 - val_loss: 0.0445\n",
            "ROC-AUC - epoch: 3 - score: 0.9868\n",
            "\n",
            "Epoch 4/5\n",
            "143613/143613 [==============================] - 82s 568us/step - loss: 0.0402 - val_loss: 0.0428\n",
            "ROC-AUC - epoch: 4 - score: 0.9879\n",
            "\n",
            "Epoch 5/5\n",
            "143613/143613 [==============================] - 82s 569us/step - loss: 0.0382 - val_loss: 0.0447\n",
            "ROC-AUC - epoch: 5 - score: 0.9881\n",
            "\n",
            "fitted\n",
            "\n",
            "global_epoch 1\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/5\n",
            "143613/143613 [==============================] - 82s 569us/step - loss: 0.0340 - val_loss: 0.0440\n",
            "ROC-AUC - epoch: 1 - score: 0.9878\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.9878\n",
            "0 patience remained\n",
            "Epoch 2/5\n",
            "143613/143613 [==============================] - 82s 570us/step - loss: 0.0321 - val_loss: 0.0443\n",
            "ROC-AUC - epoch: 2 - score: 0.9877\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.9877\n",
            "Early stopping due to lower roc auc\n",
            "fitted\n",
            "153164/153164 [==============================] - 33s 213us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 1/2 [11:51<11:51, 711.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "global_epoch 0\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/5\n",
            "143613/143613 [==============================] - 82s 570us/step - loss: 0.0693 - val_loss: 0.0518\n",
            "ROC-AUC - epoch: 1 - score: 0.98\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.98\n",
            "0 patience remained\n",
            "Epoch 2/5\n",
            "143613/143613 [==============================] - 81s 567us/step - loss: 0.0469 - val_loss: 0.0469\n",
            "ROC-AUC - epoch: 2 - score: 0.9854\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.9854\n",
            "Early stopping due to lower roc auc\n",
            "fitted\n",
            "\n",
            "global_epoch 1\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/5\n",
            "143613/143613 [==============================] - 81s 567us/step - loss: 0.0417 - val_loss: 0.0426\n",
            "ROC-AUC - epoch: 1 - score: 0.9873\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.9873\n",
            "0 patience remained\n",
            "Epoch 2/5\n",
            "143613/143613 [==============================] - 81s 566us/step - loss: 0.0395 - val_loss: 0.0427\n",
            "ROC-AUC - epoch: 2 - score: 0.9878\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.9878\n",
            "Early stopping due to lower roc auc\n",
            "fitted\n",
            "153164/153164 [==============================] - 33s 213us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 2/2 [18:48<00:00, 623.22s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10min 36s, sys: 4min, total: 14min 36s\n",
            "Wall time: 18min 48s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujanlBVs61IJ",
        "colab_type": "text"
      },
      "source": [
        "## Cheaty evaluating on test labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIOGCjO11DrT",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "submission = pd.read_csv(submission_path)\n",
        "submission[TARGET_COLS] = (pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCukoB8QB2Rz",
        "colab_type": "code",
        "trusted": true,
        "outputId": "3f589780-ac61-459f-d5ef-067a6170fdc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "submission.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00001cee341fdb12</td>\n",
              "      <td>0.998985</td>\n",
              "      <td>4.945387e-01</td>\n",
              "      <td>0.987257</td>\n",
              "      <td>0.097632</td>\n",
              "      <td>0.974627</td>\n",
              "      <td>0.440083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000247867823ef7</td>\n",
              "      <td>0.007164</td>\n",
              "      <td>2.753735e-05</td>\n",
              "      <td>0.001133</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.000974</td>\n",
              "      <td>0.000146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00013b17ad220c46</td>\n",
              "      <td>0.007884</td>\n",
              "      <td>1.225024e-04</td>\n",
              "      <td>0.002532</td>\n",
              "      <td>0.000309</td>\n",
              "      <td>0.002714</td>\n",
              "      <td>0.000617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00017563c3f7919a</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>8.791685e-07</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00017695ad8997eb</td>\n",
              "      <td>0.007049</td>\n",
              "      <td>2.688169e-05</td>\n",
              "      <td>0.001406</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.000862</td>\n",
              "      <td>0.000122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>0.000263</td>\n",
              "      <td>5.960464e-07</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>00024115d4cbde0f</td>\n",
              "      <td>0.002175</td>\n",
              "      <td>3.531575e-06</td>\n",
              "      <td>0.000255</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>0.594126</td>\n",
              "      <td>4.024178e-03</td>\n",
              "      <td>0.067739</td>\n",
              "      <td>0.007929</td>\n",
              "      <td>0.081680</td>\n",
              "      <td>0.004874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00025358d4737918</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>1.052022e-04</td>\n",
              "      <td>0.007950</td>\n",
              "      <td>0.000294</td>\n",
              "      <td>0.020561</td>\n",
              "      <td>0.001072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00026d1092fe71cc</td>\n",
              "      <td>0.001731</td>\n",
              "      <td>6.213784e-06</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.000039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id     toxic  severe_toxic  ...    threat    insult  identity_hate\n",
              "0  00001cee341fdb12  0.998985  4.945387e-01  ...  0.097632  0.974627       0.440083\n",
              "1  0000247867823ef7  0.007164  2.753735e-05  ...  0.000093  0.000974       0.000146\n",
              "2  00013b17ad220c46  0.007884  1.225024e-04  ...  0.000309  0.002714       0.000617\n",
              "3  00017563c3f7919a  0.000261  8.791685e-07  ...  0.000005  0.000044       0.000008\n",
              "4  00017695ad8997eb  0.007049  2.688169e-05  ...  0.000199  0.000862       0.000122\n",
              "5  0001ea8717f6de06  0.000263  5.960464e-07  ...  0.000005  0.000044       0.000006\n",
              "6  00024115d4cbde0f  0.002175  3.531575e-06  ...  0.000021  0.000245       0.000028\n",
              "7  000247e83dcc1211  0.594126  4.024178e-03  ...  0.007929  0.081680       0.004874\n",
              "8  00025358d4737918  0.117647  1.052022e-04  ...  0.000294  0.020561       0.001072\n",
              "9  00026d1092fe71cc  0.001731  6.213784e-06  ...  0.000031  0.000221       0.000039\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUF4Uxrl8pXr",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)\n",
        "test_labels_df = pd.read_csv(test_labels_filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6crgLcLx7X-p",
        "colab_type": "code",
        "trusted": true,
        "outputId": "5970545a-b755-49b4-a855-84c23902e39b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_labels_df = test_labels_df[(test_labels_df[\"toxic\"] != -1) &\n",
        "                                (test_labels_df[\"severe_toxic\"] != -1) &\n",
        "                                (test_labels_df[\"obscene\"] != -1) &\n",
        "                                (test_labels_df[\"threat\"] != -1) &\n",
        "                                (test_labels_df[\"insult\"] != -1) &\n",
        "                                (test_labels_df[\"identity_hate\"] != -1)]\n",
        "test_labels_df.shape                               "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(63978, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ymumFOQ9sA1",
        "colab_type": "code",
        "trusted": true,
        "outputId": "b17e62bd-a222-40dd-b4f9-ce21078f2d70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "submission_to_evaluate = submission[submission['id'].isin(test_labels_df['id'].values)]\n",
        "submission_to_evaluate.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(63978, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3r8VTCK_TaM",
        "colab_type": "code",
        "trusted": true,
        "outputId": "c282089b-22a1-49d9-f3f4-f288a67e664f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "scores = []\n",
        "for class_name in TARGET_COLS:\n",
        "    train_target = test_labels_df[class_name]\n",
        "    train_predicted = submission_to_evaluate[class_name]\n",
        "\n",
        "    cv_score = np.mean(roc_auc_score(train_target.values, train_predicted.values))\n",
        "    scores.append(cv_score)\n",
        "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
        "\n",
        "print('Total CV score is {}'.format(np.mean(scores)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV score for class toxic is 0.967239853233377\n",
            "CV score for class severe_toxic is 0.9892108184637406\n",
            "CV score for class obscene is 0.9771303428007556\n",
            "CV score for class threat is 0.991811197712763\n",
            "CV score for class insult is 0.9751569740998813\n",
            "CV score for class identity_hate is 0.9831161975458\n",
            "Total CV score is 0.9806108973093862\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YKsSnwkd3TBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.to_csv(os.path.join(data_folder,\"submission_bidirectional_GRU.csv\"), index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNf0KsAHCSJB",
        "colab_type": "text"
      },
      "source": [
        "Жирный шрифт означает изменения в процессе валидации и оценивания и при изменении действует начиная со строчки указания и ниже\n",
        "\n",
        "glove twitter 200:  0.98052\n",
        "\n",
        "**С поднятием колва epochs и global_epochs + RocAucEarlyStopping** скор упал с 0.98052 до 0.97573\n",
        "\n",
        "GLOBAL_EPOCHS = 2, EPOCHS = 5, glove.840B.300d.txt   0.97994  \n",
        "BPEmb(lang=\"en\", dim=25, vs=20000)   0.97375  \n",
        "BPEmb(lang=\"en\", dim=300, vs=20000) without preprocessing   0.97699  \n",
        "BPEmb(lang=\"en\", dim=300, vs=20000)   0.97865   \n",
        "fasttext crawl-300d-2M-subword 0.95732  \n",
        "BPEmb(lang=\"en\", dim=300, vs=20000) **Patience 2->1 SEEDS 1->1**  0.98041   \n",
        "flair ELMoEmbeddings('small') embedding len 768  0.98005  \n",
        "flair ELMoEmbeddings('medium') embedding len 1536  0.98038  \n",
        "flair RoBERTaEmbeddings('roberta-base') embedding len 768  0.97413  \n",
        "flair stack of ELMoEmbeddings('small') + RoBERTaEmbeddings('roberta-base') embedding len 1536  0.98069      \n"
      ]
    }
  ]
}