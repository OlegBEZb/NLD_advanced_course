{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "NLP advance course. HW3. Distributive semantic. Embedings_+_Bidirectional_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GRh7ig80jk3s",
        "1_nWe7O-jqak",
        "TGzxJch3juGy",
        "sWmhQEFAj1VE",
        "_yDXCBmqtkPl",
        "0hTlPvNY8gi1"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cccc82610dc7476ca26f1ae13688cec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7a442345ef6241dc9d047691be3d54a9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f756253d6f594f1299d531a1cdbd800f",
              "IPY_MODEL_9b5a3b51bf7740f493201579c1f9797a"
            ]
          }
        },
        "7a442345ef6241dc9d047691be3d54a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f756253d6f594f1299d531a1cdbd800f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c1ca1a06e3c44cd9a48e5faec94f0b34",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 159571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 159571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d2f753ae54d44416be36541d7a0992dd"
          }
        },
        "9b5a3b51bf7740f493201579c1f9797a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c318466ffeb544f2b27ceb449446a7a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 159571/159571 [02:06&lt;00:00, 1257.82it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_966b31080ad040bd9f1ee8e6a561059b"
          }
        },
        "c1ca1a06e3c44cd9a48e5faec94f0b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d2f753ae54d44416be36541d7a0992dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c318466ffeb544f2b27ceb449446a7a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "966b31080ad040bd9f1ee8e6a561059b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlegBEZb/NLP_advanced_course/blob/master/HW1/NLP%20advance%20course.%20HW3.%20Distributive%20semantic.%20Embedings_%2B_Bidirectional_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVRaU_zG-PaI",
        "colab_type": "text"
      },
      "source": [
        "## Embeddings to check\n",
        "\n",
        "* Word Embeddings (word2vec, GloVe, etc.)\n",
        "* Starspace and other similarity learning embeddings\n",
        "* char-gram embeddings (bpemb, fasttext etc.)\n",
        "* doc and sentence embeddings (doc2vec, sent2vec etc.)\n",
        "* specific context models (BERT, ELMo)\n",
        "* Advanced: Poincare Embeddings concept"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30D0Scxk4kn3",
        "colab_type": "text"
      },
      "source": [
        "# Libs import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ZnXUdrxQ3TAN",
        "colab_type": "code",
        "outputId": "3ebdfb06-cdf2-4c99-e399-01388ef12a1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import os,csv, random, pickle, re, sys\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "import nltk\n",
        "nltk.download('stopwords');\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "! pip install Unidecode;\n",
        "from unidecode import unidecode\n",
        "#! pip install pyspellchecker;\n",
        "#from spellchecker import SpellChecker\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold, train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
        "from tensorflow.keras.layers import Activation, Dropout, CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, CuDNNGRU\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, Callback\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijseJsWUosmg",
        "colab_type": "text"
      },
      "source": [
        "TODO: check if we can no to use this sess and use only default session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wi85ywI6MouM",
        "colab": {}
      },
      "source": [
        "# Initialize session\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypZqfsNI6-Ne",
        "colab_type": "text"
      },
      "source": [
        "# Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JtxOKtA35gA",
        "colab_type": "code",
        "trusted": true,
        "outputId": "f36d34f7-e375-4b96-adae-1d4c64aaaa6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if tf.test.is_gpu_available(\n",
        "    cuda_only=False,\n",
        "    min_cuda_compute_capability=None\n",
        "):\n",
        "    print(tf.test.gpu_device_name())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i673BlTJEazT",
        "colab_type": "text"
      },
      "source": [
        "# Global vars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okymGz_lEjLC",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "WORKSPACE = 'COLAB' # or 'KAGGLE'\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "LSTM_UNITS = 128\n",
        "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
        "GLOBAL_EPOCHS = 2\n",
        "EPOCHS = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD2z8D2V7Dod",
        "colab_type": "text"
      },
      "source": [
        "# Authorization on Google drive and configurings paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auzLnrdK51Rg",
        "colab_type": "code",
        "trusted": true,
        "outputId": "c6251e6e-e4ee-44fa-af22-e35b279f427b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "if WORKSPACE == 'COLAB':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    homework_folder = os.path.join('/content/drive/My Drive', 'Advanced NLP/Homework 1: Classical classification task like Kaggle Toxic or Quora')\n",
        "    data_folder = os.path.join(homework_folder, 'Toxic data')\n",
        "    embeddings_folder = os.path.join(homework_folder, 'embeddings')\n",
        "    output_folder = os.path.join(homework_folder, 'output')\n",
        "elif WORKSPACE == 'KAGGLE':\n",
        "    data_folder = '../input/jigsaw-toxic-comment-classification-challenge/'\n",
        "    embeddings_folder = '../input/glove-global-vectors-for-word-representation'\n",
        "else:\n",
        "    pass\n",
        "\n",
        "print('data found:', os.listdir(data_folder))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "data found: ['test.csv', 'test_labels.csv', 'train.csv', 'sample_submission.csv', 'submission.csv', 'submission_emb_sklearn.csv', 'submission_bidirectional_GRU.csv', 'train_labels.csv.npy', 'train_labels.npy', 'submission_fine_tuned_bert.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UYdAXsdT3TAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_filepath = os.path.join(data_folder,\"train.csv\")\n",
        "test_filepath = os.path.join(data_folder,\"test.csv\")\n",
        "test_labels_filepath = os.path.join(data_folder,\"test_labels.csv\")\n",
        "\n",
        "#path to a sample submission\n",
        "submission_path = os.path.join(data_folder,\"sample_submission.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15GVrJYa717o",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYljyuKTCm8r",
        "colab_type": "text"
      },
      "source": [
        "## Dicts and lists of useful words and transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjL5O731CtDD",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#List of some words that often appear in toxic comments\n",
        "#Sorry about the level of toxicity in it!\n",
        "toxic_words = [\"poop\", \"crap\", \"prick\", \"twat\", \"wikipedia\", \"wiki\", \"hahahahaha\", \"lol\", \"bastard\", \"sluts\", \"slut\", \"douchebag\", \"douche\", \"blowjob\", \"nigga\", \"dumb\", \"jerk\", \"wanker\", \"wank\", \"penis\", \"motherfucker\", \"fucker\", \"fuk\", \"fucking\", \"fucked\", \"fuck\", \"bullshit\", \"shit\", \"stupid\", \"bitches\", \"bitch\", \"suck\", \"cunt\", \"dick\", \"cocks\", \"cock\", \"die\", \"kill\", \"gay\", \"jewish\", \"jews\", \"jew\", \"niggers\", \"nigger\", \"faggot\", \"fag\", \"asshole\"]\n",
        "# todo: convert to dict and use in normalising by dict\n",
        "astericks_words = [('mother****ers', 'motherfuckers'), ('motherf*cking', 'motherfucking'), ('mother****er', 'motherfucker'), ('motherf*cker', 'motherfucker'), ('bullsh*t', 'bullshit'), ('f**cking', 'fucking'), ('f*ucking', 'fucking'), ('fu*cking', 'fucking'), ('****ing', 'fucking'), ('a**hole', 'asshole'), ('assh*le', 'asshole'), ('f******', 'fucking'), ('f*****g', 'fucking'), ('f***ing', 'fucking'), ('f**king', 'fucking'), ('f*cking', 'fucking'), ('fu**ing', 'fucking'), ('fu*king', 'fucking'), ('fuc*ers', 'fuckers'), ('f*****', 'fucking'), ('f***ed', 'fucked'), ('f**ker', 'fucker'), ('f*cked', 'fucked'), ('f*cker', 'fucker'), ('f*ckin', 'fucking'), ('fu*ker', 'fucker'), ('fuc**n', 'fucking'), ('ni**as', 'niggas'), ('b**ch', 'bitch'), ('b*tch', 'bitch'), ('c*unt', 'cunt'), ('f**ks', 'fucks'), ('f*ing', 'fucking'), ('ni**a', 'nigga'), ('c*ck', 'cock'), ('c*nt', 'cunt'), ('cr*p', 'crap'), ('d*ck', 'dick'), ('f***', 'fuck'), ('f**k', 'fuck'), ('f*ck', 'fuck'), ('fc*k', 'fuck'), ('fu**', 'fuck'), ('fu*k', 'fuck'), ('s***', 'shit'), ('s**t', 'shit'), ('sh**', 'shit'), ('sh*t', 'shit'), ('tw*t', 'twat')]\n",
        "fasttext_misspelings = {\"'n'balls\": 'balls', \"-nazi's\": 'nazis', 'adminabuse': 'admin abuse', \"admins's\": 'admins', 'arsewipe': 'arse wipe', 'assfack': 'asshole', 'assholifity': 'asshole', 'assholivity': 'asshole', 'asshoul': 'asshole', 'asssholeee': 'asshole', 'belizeans': 'mexicans', \"blowing's\": 'blowing', 'bolivians': 'mexicans', 'celtofascists': 'fascists', 'censorshipmeisters': 'censor', 'chileans': 'mexicans', 'clerofascist': 'fascist', 'cowcrap': 'crap', 'crapity': 'crap', \"d'idiots\": 'idiots', 'deminazi': 'nazi', 'dftt': \"don't feed the troll\", 'dildohs': 'dildo', 'dramawhores': 'drama whores', 'edophiles': 'pedophiles', 'eurocommunist': 'communist', 'faggotkike': 'faggot', 'fantard': 'retard', 'fascismnazism': 'fascism', 'fascistisized': 'fascist', 'favremother': 'mother', 'fuxxxin': 'fucking', \"g'damn\": 'goddamn', 'harassmentat': 'harassment', 'harrasingme': 'harassing me', 'herfuc': 'motherfucker', 'hilterism': 'fascism', 'hitlerians': 'nazis', 'hitlerites': 'nazis', 'hubrises': 'pricks', 'idiotizing': 'idiotic', 'inadvandals': 'vandals', \"jackass's\": 'jackass', 'jiggabo': 'nigga', 'jizzballs': 'jizz balls', 'jmbass': 'dumbass', 'lejittament': 'legitimate', \"m'igger\": 'nigger', \"m'iggers\": 'niggers', 'motherfacking': 'motherfucker', 'motherfuckenkiwi': 'motherfucker', 'muthafuggas': 'niggas', 'nazisms': 'nazis', 'netsnipenigger': 'nigger', 'niggercock': 'nigger', 'niggerspic': 'nigger', 'nignog': 'nigga', 'niqqass': 'niggas', \"non-nazi's\": 'not a nazi', 'panamanians': 'mexicans', 'pedidiots': 'idiots', 'picohitlers': 'hitler', 'pidiots': 'idiots', 'poopia': 'poop', 'poopsies': 'poop', 'presumingly': 'obviously', 'propagandaanddisinformation': 'propaganda and disinformation', 'propagandaministerium': 'propaganda', 'puertoricans': 'mexicans', 'puertorricans': 'mexicans', 'pussiest': 'pussies', 'pussyitis': 'pussy', 'rayaridiculous': 'ridiculous', 'redfascists': 'fascists', 'retardzzzuuufff': 'retard', \"revertin'im\": 'reverting', 'scumstreona': 'scums', 'southamericans': 'mexicans', 'strasserism': 'fascism', 'stuptarded': 'retarded', \"t'nonsense\": 'nonsense', \"threatt's\": 'threat', 'titoists': 'communists', 'twatbags': 'douchebags', 'youbollocks': 'you bollocks'}\n",
        "acronym_words = {\"btw\":\"by the way\", \"yo\": \"you\", \"u\": \"you\", \"r\": \"are\", \"ur\": \"your\", \"ima\": \"i am going to\", \"imma\": \"i am going to\", \"i'ma\":\"i am going to\", \"cos\":\"because\", \"coz\":\"because\", \"stfu\": \"shut the fuck up\", \"wat\": \"what\"}\n",
        "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
        "letter_replacement = {\"$\": \"s\", \"@\": \"a\"}\n",
        "unspaced_signs = {\"!\": \" ! \", \"?\": \" ? \"}\n",
        "url_replace_dict = {\"http://\": '',\n",
        "                    \"www.\": '',\n",
        "                    \"https://\": '',\n",
        "                    \"wikipedia.org\": ''}\n",
        "#apparently, people on wikipedia love to talk about sockpuppets :-)\n",
        "sockpuppets_dict = {\"sock puppet\": \"sockpuppet\", \"SOCK PUPPET\": \"SOCKPUPPET\"}\n",
        "\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "#spell_checker = SpellChecker()\n",
        "\n",
        "cont_patterns = [\n",
        "    (r'(W|w)on\\'t', r'will not'),\n",
        "    (r'(C|c)an\\'t', r'can not'),\n",
        "    (r'(I|i)\\'m', r'i am'),\n",
        "    (r'(A|a)in\\'t', r'is not'),\n",
        "    (r'(\\w+)\\'ll', r'\\g<1> will'),\n",
        "    (r'(\\w+)n\\'t', r'\\g<1> not'),\n",
        "    (r'(\\w+)\\'ve', r'\\g<1> have'),\n",
        "    (r'(\\w+)\\'s', r'\\g<1> is'),\n",
        "    (r'(\\w+)\\'re', r'\\g<1> are'),\n",
        "    (r'(\\w+)\\'d', r'\\g<1> would'),\n",
        "]\n",
        "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
        "\n",
        "#We will filter all characters except alphabet characters and some punctuation\n",
        "valid_characters = \" \" + \"@$\" + \"'!?-\" + \"abcdefghijklmnopqrstuvwxyz\" + \"abcdefghijklmnopqrstuvwxyz\".upper()\n",
        "valid_set = set(x for x in valid_characters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs_nlu1iILY1",
        "colab_type": "text"
      },
      "source": [
        "## Funcs for processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfUaIP-wIKTZ",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def replace_in_sentence(sentence, dictionary):\n",
        "    for key in dictionary.keys():\n",
        "        if key in sentence:\n",
        "            sentence = sentence.replace(key, dictionary[key])\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def split_toxic_and_normal_parts(word, toxic_words):\n",
        "    if word == \"\":\n",
        "        return \"\"\n",
        "    \n",
        "    lower = word.lower()\n",
        "    for toxic_word in toxic_words:\n",
        "        start = lower.find(toxic_word)\n",
        "        if start >= 0:\n",
        "            end = start + len(toxic_word)\n",
        "            result = \" \".join([word[0:start], word[start:end], split_toxic_and_normal_parts(word[end:], toxic_words)])\n",
        "            return result.replace(\"  \", \" \").strip()\n",
        "    return word\n",
        "\n",
        "\n",
        "def normalize_by_dictionary(normalized_word, dictionary):\n",
        "    \"\"\"\n",
        "    Word to be normalized (could be a phrase) with dictionary and the dictionary passed.\n",
        "    Replaces each word in passed word-phrase into their representations in dictionary.\n",
        "    Uppercased words are matched in lowercase but returned in upper. Other cases\n",
        "    are checked in lower and returned in lower.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for word in normalized_word.split():\n",
        "        if word == word.upper():\n",
        "            if word.lower() in dictionary:\n",
        "                result.append(dictionary[word.lower()].upper())\n",
        "            else:\n",
        "                result.append(word)\n",
        "        else:\n",
        "            #print('word', '\\'', word, '\\'', 'is not upper')\n",
        "            #print('word.lower()', word.lower())\n",
        "            if word.lower() in dictionary:\n",
        "                result.append(dictionary[word.lower()])\n",
        "            else:\n",
        "                result.append(word)\n",
        "    \n",
        "    return \" \".join(result)\n",
        "\n",
        "\n",
        "def normalize_comment(comment, tokenizer, max_chars_in_comment):\n",
        "    comment = unidecode(comment)\n",
        "    comment = comment[:max_chars_in_comment]\n",
        "\n",
        "    comment = replace_in_sentence(comment, letter_replacement)\n",
        "    comment = replace_in_sentence(comment, unspaced_signs)\n",
        "    comment = replace_in_sentence(comment, url_replace_dict)\n",
        "    comment = replace_in_sentence(comment, sockpuppets_dict)    \n",
        "\n",
        "    # ('mother****ers', 'motherfuckers')\n",
        "    # for w in astericks_words:\n",
        "    #     if w[0] in comment:\n",
        "    #         comment = comment.replace(w[0], w[1])\n",
        "    #     if w[0].upper() in comment:\n",
        "    #         comment = comment.replace(w[0].upper(), w[1].upper())\n",
        "    \n",
        "    normalized_words = []\n",
        "    for word in tokenizer.tokenize(comment):\n",
        "        if word in english_stopwords: continue\n",
        "\n",
        "        # # '(W|w)on\\'t', r'will not'\n",
        "        # for (pattern, repl) in patterns:\n",
        "        #    word = re.sub(pattern, repl, word)\n",
        "\n",
        "        if word == \".\" or word == \",\":\n",
        "            normalized_words.append(word)\n",
        "            continue\n",
        "        \n",
        "        # drop url parts from links\n",
        "        # word = replace_url(word)\n",
        "\n",
        "        # replace single dots to whitespaces\n",
        "        if word.count(\".\") == 1:\n",
        "            word = word.replace(\".\", \" \")\n",
        "\n",
        "        # leave only legalized symbols\n",
        "        filtered_word = \"\".join([x for x in word if x in valid_set])\n",
        "                    \n",
        "        #Kind of hack: for every word check if it has a toxic word as a part of it\n",
        "        #If so, split this word by swear and non-swear part.\n",
        "        #normalized_word = split_toxic_and_normal_parts(filtered_word, toxic_words)\n",
        "        normalized_word = filtered_word\n",
        "\n",
        "#         normalized_word = normalize_by_dictionary(normalized_word, hyphens_dict)\n",
        "#         normalized_word = normalize_by_dictionary(normalized_word, merged_dict)\n",
        "        \n",
        "        # check misspellings\n",
        "        # temp = []\n",
        "        # for word in normalized_word.split():\n",
        "        #   temp.append(spell_checker.correction(word))\n",
        "        # normalized_word = \" \".join(temp)\n",
        "          \n",
        "        # normalized_word = normalize_by_dictionary(normalized_word, fasttext_misspelings)\n",
        "        normalized_word = normalize_by_dictionary(normalized_word, acronym_words)\n",
        "\n",
        "        normalized_words.append(normalized_word)\n",
        "        \n",
        "    normalized_comment = \" \".join(normalized_words)\n",
        "    \n",
        "    result = []\n",
        "    for word in normalized_comment.split():\n",
        "        # if word is upper\n",
        "        if word.upper() == word:\n",
        "            result.append(word)\n",
        "        else:\n",
        "            result.append(word.lower())\n",
        "    \n",
        "    result = \" \".join(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def preprocess_text(df, preprocessing_func, tokenizer, max_chars_in_comment):\n",
        "  # cheap and bad preprocessing\n",
        "  # preprocessed_df = df[TEXT_COLUMN].fillna(\"CVxTz\").values\n",
        "  # preprocessed_df = df[TEXT_COLUMN]\n",
        "\n",
        "  text_ndarray = df.fillna('_na').values\n",
        "  np_preprocessing_func = np.vectorize(preprocessing_func)\n",
        "\n",
        "  preprocessed_df = np_preprocessing_func(text_ndarray, tokenizer, max_chars_in_comment)\n",
        "\n",
        "  print('Gained shape:', preprocessed_df.shape)\n",
        "  return preprocessed_df\n",
        "\n",
        "\n",
        "def tokenize_and_pad(text, tokenizer, max_words_in_comment):\n",
        "\n",
        "  tokenized_text = tokenizer.texts_to_sequences(text)\n",
        "  padded_text = pad_sequences(tokenized_text, maxlen=max_words_in_comment, dtype='int', value=0)\n",
        "\n",
        "  print('Gained shape:', padded_text.shape)\n",
        "  return padded_text\n",
        "\n",
        "\n",
        "def load_and_preprocess(filepath, preprocessing_func, tokenizer, \n",
        "                        max_chars_in_comment, get_labels=False):\n",
        "    df = pd.read_csv(filepath)\n",
        "    preprocessed_df = preprocess_text(df=df[TEXT_COLUMN], \n",
        "                                      preprocessing_func=preprocessing_func, \n",
        "                                      tokenizer=tokenizer,\n",
        "                                      max_chars_in_comment=max_chars_in_comment)\n",
        "    if get_labels:\n",
        "        labels = df[TARGET_COLS].values\n",
        "        return preprocessed_df, labels\n",
        "    else:\n",
        "        return preprocessed_df\n",
        "\n",
        "\n",
        "def preproc2tokenized(preprocessed_df, max_tokens, max_words_in_comment, tokenizer=None):\n",
        "    # keras text tokenizer. simple, only for splitting and padding\n",
        "    if not tokenizer:\n",
        "        tokenizer = Tokenizer(num_words=MAX_TOKENS)\n",
        "        tokenizer.fit_on_texts(list(preprocessed_df))\n",
        "    tokenized_df = tokenize_and_pad(preprocessed_df, tokenizer, max_words_in_comment)\n",
        "    return tokenized_df, tokenizer\n",
        "\n",
        "\n",
        "def get_preprocessed_dfs(train_filepath, test_filepath, preprocessing_func, \n",
        "                         max_chars_in_comment, max_tokens, max_words_in_comment,\n",
        "                         return_tokenized=False):\n",
        "\n",
        "    preprocessing_tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
        "    preprocessed_train, train_labels = load_and_preprocess(train_filepath, \n",
        "                                                           preprocessing_func, \n",
        "                                                           preprocessing_tknzr, \n",
        "                                                           max_chars_in_comment,\n",
        "                                                           get_labels=True)        \n",
        "    preprocessed_test = load_and_preprocess(test_filepath, preprocessing_func, \n",
        "                                            preprocessing_tknzr, max_chars_in_comment)        \n",
        "        \n",
        "    if return_tokenized:\n",
        "        tokenized_train, tokenizer = preproc2tokenized(preprocessed_train, \n",
        "                                                       max_tokens=max_tokens, \n",
        "                                                       max_words_in_comment=max_words_in_comment, \n",
        "                                                       tokenizer=None)\n",
        "        tokenized_test, _ = preproc2tokenized(preprocessed_test, \n",
        "                                              max_tokens=max_tokens, \n",
        "                                              max_words_in_comment=max_words_in_comment, \n",
        "                                              tokenizer=tokenizer)\n",
        "        return tokenized_train, tokenized_test, train_labels\n",
        "    else:\n",
        "        return preprocessed_train, preprocessed_test, train_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn1-XTZEIZ3y",
        "colab_type": "text"
      },
      "source": [
        "## Loading frames and processing them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AWRdxZsHG3B",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "TEXT_COLUMN = 'comment_text'\n",
        "TARGET_COLS = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "\n",
        "MAX_WORDS_IN_COMMENT = 220\n",
        "MAX_TOKENS = 20000\n",
        "MAX_CHARS_IN_COMMENT = 20000 #We are going to truncate a comment if its length > threshold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z54MJ5i2i-UA",
        "colab_type": "code",
        "outputId": "7489e84a-242a-4b58-c5fb-87ca83f762d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "preprocessed_train, preprocessed_test, train_labels = get_preprocessed_dfs(train_filepath, \n",
        "                                                                          test_filepath, \n",
        "                                                                          normalize_comment, \n",
        "                                                                          max_chars_in_comment=MAX_CHARS_IN_COMMENT, \n",
        "                                                                          max_tokens=MAX_TOKENS,\n",
        "                                                                          max_words_in_comment=MAX_WORDS_IN_COMMENT,\n",
        "                                                                          return_tokenized=False)\n",
        "gc.collect()\n",
        "\n",
        "print(preprocessed_train[:3])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gained shape: (159571,)\n",
            "Gained shape: (153164,)\n",
            "[\"explanation why edits made username hardcore metallica fan reverted ? they vandalisms , closure gas I voted new york dolls FAC . and please remove template talk page since i'm retired . .\"\n",
            " \"d'aww ! he matches background colour i'm seemingly stuck . thanks . talk , january , UTC\"\n",
            " \"hey man , i'm really trying edit war . it's guy constantly removing relevant information talking edits instead talk page . he seems care formatting actual info .\"]\n",
            "CPU times: user 1min 49s, sys: 6 s, total: 1min 55s\n",
            "Wall time: 1min 55s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8APgvUyskaf",
        "colab_type": "text"
      },
      "source": [
        "# Preparing features and training model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAROUFTRpHZI",
        "colab_type": "text"
      },
      "source": [
        "todo: refactor structure (embedding matrix + bigru/ transformer+fully-connected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCRY8qnNJTh_",
        "colab_type": "text"
      },
      "source": [
        "## Preparing features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jqSUK8Dpd3Q",
        "colab_type": "code",
        "outputId": "3d43d16b-2699-43e9-ebd9-ca298ab894c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "embedding_len = 1536\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' %len(word_index))\n",
        "\n",
        "#del tokenizer\n",
        "gc.collect()\n",
        "\n",
        "num_words = min(MAX_TOKENS, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_words, embedding_len)) # zeroth row for UNK\n",
        "print('embedding matrix shape', embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 184924 unique tokens.\n",
            "embedding matrix shape (20000, 1536)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRh7ig80jk3s",
        "colab_type": "text"
      },
      "source": [
        "### GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BNjZiXPZEYvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding_len = 200\n",
        "# words = pd.read_csv(os.path.join(embeddings_folder, \"glove.twitter.27B.{}d.txt\".format(embedding_len)), \n",
        "#                     sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
        "\n",
        "# embedding_len = 300\n",
        "# words = pd.read_csv(os.path.join(embeddings_folder, \"glove.840B.300d.txt\"), \n",
        "#                     sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
        "\n",
        "\n",
        "# vocab_words = words[words.index.isin(list(word_index.keys()))]\n",
        "# print('vocab_words.shape', vocab_words.shape)\n",
        "\n",
        "# del words\n",
        "# gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0-ZU5VYq-Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_vec(word, words_df):\n",
        "#     return words_df.loc[word].values\n",
        "\n",
        "\n",
        "\n",
        "# for word, i in word_index.items():\n",
        "#   if i >= max_tokens:\n",
        "#     continue\n",
        "#   try:\n",
        "#     embedding_vector = get_vec(word, vocab_words)\n",
        "#   except:\n",
        "#     continue\n",
        "#   if embedding_vector is not None:\n",
        "#     # words not found in embedding index will be all-zeros.\n",
        "#     embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# # %%time\n",
        "# # embedding_matrix = np.load('../input/embedding-2/embedding_matrix_big.npy')\n",
        "    \n",
        "# print('embedding_matrix.shape', embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_nWe7O-jqak",
        "colab_type": "text"
      },
      "source": [
        "### StarSpace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD8SO8xXfsVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Starspace attempt\n",
        "\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# count_vectorizer = CountVectorizer(min_df=0, max_df=0.99, max_features=10000)\n",
        "# X_train = count_vectorizer.fit_transform(article_contents.main_content.iloc[0:train_row])\n",
        "# X_train = count_vectorizer.inverse_transform(X_train)\n",
        "# label_prefix = '__label__'\n",
        "# with open(\"train_starspace.txt\", 'w+') as file:\n",
        "#     for i in range(10):\n",
        "#         file.write(' '.join(preprocessed_train[i].split(' ')) + ' ' + label_prefix + train_labels[i])\n",
        "#         file.write('\\n')\n",
        "# file.close()\n",
        "\n",
        "# The result file will look like this (all separeted by space, and label will have prefix __label__)\n",
        "# how are you ... __label__b\n",
        "# this is just an example ... __label__c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGzxJch3juGy",
        "colab_type": "text"
      },
      "source": [
        "### FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voep54c6Q-I-",
        "colab_type": "code",
        "outputId": "64afe0b8-56c3-40e5-87f5-7372096ffc2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (42.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.17.4)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBeehRN-oS9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import codecs\n",
        "\n",
        "# #load embeddings\n",
        "# print('loading word embeddings...')\n",
        "# embeddings_index = {}\n",
        "# f = codecs.open(os.path.join(embeddings_folder, 'crawl-300d-2M-subword.vec'), encoding='utf-8')\n",
        "# for line in tqdm(f, mininterval=5):\n",
        "#     values = line.rstrip().rsplit(' ')\n",
        "#     word = values[0]\n",
        "#     coefs = np.asarray(values[1:], dtype='float32')\n",
        "#     embeddings_index[word] = coefs\n",
        "# f.close()\n",
        "# print('found %s word vectors' % len(embeddings_index))\n",
        "# print('preparing embedding matrix...')\n",
        "# words_not_found = []\n",
        "\n",
        "\n",
        "# for word, i in word_index.items():\n",
        "#     if i >= max_tokens:\n",
        "#         continue\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "#     else:\n",
        "#         words_not_found.append(word)\n",
        "# print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "\n",
        "\n",
        "# words_not_found[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWmhQEFAj1VE",
        "colab_type": "text"
      },
      "source": [
        "### BPEmb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdMBN-HPjjek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install bpemb;\n",
        "# from bpemb import BPEmb\n",
        "# # load English BPEmb model with default vocabulary size of max_tokens\n",
        "# bpemb_en = BPEmb(lang=\"en\", dim=embedding_len, vs=max_tokens)\n",
        "# bpemb_en.vectors.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JQn_7WjrNP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # mean vector of word parts\n",
        "# def get_vec(word):\n",
        "#     vec = bpemb_en.embed(word)\n",
        "#     return vec.mean(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSAJK0AYsdW2",
        "colab_type": "code",
        "trusted": true,
        "outputId": "bd9383ce-edd7-4c2f-b258-1dea1d46675c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# %%time\n",
        "\n",
        "# for word, i in word_index.items():\n",
        "#   if i >= max_tokens:\n",
        "#     continue\n",
        "#   try:\n",
        "#     embedding_vector = get_vec(word)\n",
        "#   except:\n",
        "#     continue\n",
        "#   if embedding_vector is not None:\n",
        "#     # words not found in embedding index will be all-zeros.\n",
        "#     embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# print('embedding_matrix.shape', embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding_matrix.shape (20000, 300)\n",
            "CPU times: user 504 ms, sys: 18.1 ms, total: 522 ms\n",
            "Wall time: 520 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2ezEW492Nnj",
        "colab_type": "text"
      },
      "source": [
        "### Flair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScKTCXs-trr_",
        "colab_type": "code",
        "outputId": "01af3e73-bbbf-4cbf-9253-3a2dd3befe54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! pip install tiny-tokenizer\n",
        "! pip install flair\n",
        "\n",
        "import flair\n",
        "from flair.data import Sentence\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tiny-tokenizer\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/0f/aa52c227c5af69914be05723b3deaf221805a4ccbce87643194ef2cdde43/tiny_tokenizer-3.1.0.tar.gz\n",
            "Building wheels for collected packages: tiny-tokenizer\n",
            "  Building wheel for tiny-tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tiny-tokenizer: filename=tiny_tokenizer-3.1.0-cp36-none-any.whl size=10550 sha256=aa15269699b787314d1c790861bba985df9096ce9ad75c95d32baf09b8b5ec6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/c8/36/334497a689fab90128232e86b5829b800dd271a3d5d5959c53\n",
            "Successfully built tiny-tokenizer\n",
            "Installing collected packages: tiny-tokenizer\n",
            "Successfully installed tiny-tokenizer-3.1.0\n",
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/22/8fc8e5978ec05b710216735ca47415700e83f304dec7e4281d61cefb6831/flair-0.4.4-py3-none-any.whl (193kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 2.8MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.2.9\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Collecting transformers>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 48.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 51.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from flair) (3.10.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.6)\n",
            "Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.2)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from flair) (0.2.0)\n",
            "Requirement already satisfied: tiny-tokenizer[all] in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.0)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/89/62912e01f3cede11edcc0abf81298e3439d9c06c8dce644369380ed13f6d/Deprecated-1.2.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.3.1)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from flair) (0.4.2)\n",
            "Collecting ipython==7.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/2c/c7d44277b599df35af734d8f4142d501192fdb7aef5d04daf882d7eccfbc/ipython-7.6.1-py3-none-any.whl (774kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 35.3MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 43.1MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (1.17.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (1.10.47)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.22.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->flair) (1.12.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (42.0.2)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (8.0.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
            "Collecting SudachiDict-core@ https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_core-20190927.tar.gz ; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_core-20190927.tar.gz (70.7MB)\n",
            "\u001b[K     |████████████████████████████████| 70.7MB 44kB/s \n",
            "\u001b[?25hCollecting SudachiPy; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/c9/40bfb291a7995ad218451ef97083432f998b822e3ecbd9f586f593d2cfb6/SudachiPy-0.4.2-py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.2MB/s \n",
            "\u001b[?25hCollecting janome; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/f0/bd7f90806132d7d9d642d418bdc3e870cfdff5947254ea3cab27480983a7/Janome-0.3.10-py2.py3-none-any.whl (21.5MB)\n",
            "\u001b[K     |████████████████████████████████| 21.5MB 158kB/s \n",
            "\u001b[?25hCollecting natto-py; extra == \"all\"\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/14/1d4258247a00b7b8a115563effb1d0bd30501d69580629d36593ce0af92d/natto-py-0.9.2.tar.gz\n",
            "Collecting kytea; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/bc/702d01a96d5d094bd9f3c2eb1d12153daf8edf7bf5d78b9a2dae1202df07/kytea-0.1.4-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 30.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.2)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.9.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->flair) (6.2.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.4.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.1.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (2.1.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.7.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.7.5)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/61/2dfea88583d5454e3a64f9308a686071d58d59a55db638268a6413e1eb6d/prompt_toolkit-2.0.10-py3-none-any.whl (340kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 44.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.15.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.3.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (1.13.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (0.14.1)\n",
            "Collecting dartsclone~=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/4d/45acbe9d0795d8ceef0fee1f9ac2dcbf27dca3a0578a023fcdc3fef6fd89/dartsclone-0.6.tar.gz\n",
            "Requirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (2.1.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (1.13.2)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython==7.6.1->flair) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.6.1->flair) (0.1.8)\n",
            "Requirement already satisfied: parso>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython==7.6.1->flair) (0.5.2)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers>=2.0.0->flair) (0.15.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.6.0->SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (0.29.14)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (2.19)\n",
            "Building wheels for collected packages: langdetect, segtok, sqlitedict, mpld3, sacremoses, SudachiDict-core, natto-py, dartsclone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-cp36-none-any.whl size=993460 sha256=65bda05c34cc4f7d2803f6893027a63e2645e041b68e14522d5c50dedfda6109\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.7-cp36-none-any.whl size=23258 sha256=a6068dca9b07d0d23c5b2578cd91ef376e73852632a166e378c7b110177d5e19\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=db59b3364116b169a671b96c7c5deb471b6d402d4b0645175eef90fb39422521\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=787a055e5a4ff87af3e8b3181f09441fbba207b4b392baa00e0a0b7d410c92ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=a9bc6ff7947fee6ea9cb980e90406dbce3648c3047162a4fd6e55e8e9c6fca29\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "  Building wheel for SudachiDict-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SudachiDict-core: filename=SudachiDict_core-20190927-cp36-none-any.whl size=70878518 sha256=928042bfce672ea42c8ee0a4c92767c51249df8109d969ad92eeb9979adfd8fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/d8/6e/b107d7fef6e80915aa1e46db741b98a3da011f567526347ccc\n",
            "  Building wheel for natto-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for natto-py: filename=natto_py-0.9.2-cp36-none-any.whl size=45164 sha256=b575b15b9aef2c24bc8e25450742d03fcbf00a323ecc3ac97ccd477615c70910\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/51/dd/67f87608b124a23eecf5c1fc3557cc0b7ffdeae33fe6ee89df\n",
            "  Building wheel for dartsclone (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dartsclone: filename=dartsclone-0.6-cp36-cp36m-linux_x86_64.whl size=413251 sha256=4b52a1db155cf8b637be04577552e255282764ba767ae7c523150b48026fbe49\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/cd/70/fe43307bf7398243155108f4f5a258ef336923d65ec4af93cd\n",
            "Successfully built langdetect segtok sqlitedict mpld3 sacremoses SudachiDict-core natto-py dartsclone\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.6.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, bpemb, sacremoses, transformers, langdetect, segtok, deprecated, sqlitedict, prompt-toolkit, ipython, mpld3, flair, dartsclone, SudachiPy, SudachiDict-core, janome, natto-py, kytea\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "Successfully installed SudachiDict-core-20190927 SudachiPy-0.4.2 bpemb-0.3.0 dartsclone-0.6 deprecated-1.2.7 flair-0.4.4 ipython-7.6.1 janome-0.3.10 kytea-0.1.4 langdetect-1.0.7 mpld3-0.3 natto-py-0.9.2 prompt-toolkit-2.0.10 sacremoses-0.0.38 segtok-1.5.7 sentencepiece-0.1.85 sqlitedict-1.6.0 transformers-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yDXCBmqtkPl",
        "colab_type": "text"
      },
      "source": [
        "#### ELMo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVZ4xx34vSmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNDAZuOvt4wR",
        "colab_type": "code",
        "outputId": "306e7fb8-db28-4628-ea6e-c06cc0ae1183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from flair.embeddings import ELMoEmbeddings\n",
        "\n",
        "# init embedding\n",
        "embedding = ELMoEmbeddings('medium')\n",
        "\n",
        "# create a sentence #\n",
        "sentence = Sentence('test sent')\n",
        "# embed words in sentence #\n",
        "embedding.embed(sentence)\n",
        "embedding_len = sentence[0].embedding.size()[0]\n",
        "print('embedding len', embedding_len)\n",
        "\n",
        "\n",
        "def get_comment_emb(comment):\n",
        "  sentence = Sentence(comment)\n",
        "  embedding.embed(sentence)\n",
        "  emb_list = []\n",
        "  for word in sentence:\n",
        "    emb_list.append(word.embedding)\n",
        "  return emb_list\n",
        "\n",
        "get_comment_emb_vec = np.vectorize(get_comment_emb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding len 1536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFD8Xd565HgQ",
        "colab_type": "code",
        "outputId": "aaea15ab-6a6f-4f01-c47f-7d111ebd707c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for word, i in tqdm(word_index.items(), total=len(word_index)):\n",
        "    if i >= max_tokens:\n",
        "        continue\n",
        "    try:\n",
        "        word_sent = Sentence(word)\n",
        "        embedding.embed(word_sent)\n",
        "        embedding_vector = word_sent[0].embedding.cpu().detach().numpy()\n",
        "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "           embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        pass\n",
        "        #embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 184924/184924 [03:02<00:00, 1014.44it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6tfsIpFJ6YG",
        "colab_type": "code",
        "outputId": "c515518f-a7ce-4831-cf0e-713070b8baf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "del embedding\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I7VZVVF69Fh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # creating a tensor for storing sentence embeddings #\n",
        "# s = torch.zeros(0, embedding_len)\n",
        "\n",
        "# # iterating Sentence (tqdm tracks progress) #\n",
        "# for tweet in tqdm(preprocessed_train[10]):   \n",
        "#   # empty tensor for words #\n",
        "#   w = torch.zeros(0, embedding_len)   \n",
        "#   sentence = Sentence(tweet)\n",
        "#   embedding.embed(sentence)\n",
        "#   # for every word #\n",
        "#   for token in sentence:\n",
        "#     # storing word Embeddings of each word in a sentence #\n",
        "#     w = torch.cat((w,token.embedding.view(-1, embedding_len)),0)\n",
        "#   # storing sentence Embeddings (mean of embeddings of all words)   #\n",
        "#   s = torch.cat((s, w.mean(dim = 0).view(-1, embedding_len)),0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hTlPvNY8gi1",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gEnMtmwooBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.embeddings import RoBERTaEmbeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvXWeyYWo0J9",
        "colab_type": "code",
        "outputId": "0c27d2a0-5e69-4428-fed4-20262dc73bc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# init embedding\n",
        "embedding = RoBERTaEmbeddings('roberta-base')\n",
        "\n",
        "# create a sentence #\n",
        "sentence = Sentence('test sent')\n",
        "# embed words in sentence #\n",
        "embedding.embed(sentence)\n",
        "embedding_len = sentence[0].embedding.size()[0]\n",
        "print('embedding len', embedding_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding len 768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDdLhAjJqY-G",
        "colab_type": "code",
        "outputId": "5cf088ff-3fd9-4e44-f690-e37eb45c37db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for word, i in tqdm(word_index.items(), total=len(word_index)):\n",
        "    if i >= max_tokens:\n",
        "        continue\n",
        "    try:\n",
        "        word_sent = Sentence(word)\n",
        "        embedding.embed(word_sent)\n",
        "        embedding_vector = word_sent[0].embedding.cpu().detach().numpy()\n",
        "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "           embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        pass\n",
        "        #embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "del embedding\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 184924/184924 [04:29<00:00, 685.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McJb1Sn5tw4a",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa + ELMo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztnGoOlrt3PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install allennlp\n",
        "from flair.embeddings import StackedEmbeddings, RoBERTaEmbeddings, ELMoEmbeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_4YEiP1t5-n",
        "colab_type": "code",
        "outputId": "7009dff1-41c1-45be-b19b-bae803e40e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "elmo_embeddings = ELMoEmbeddings('small')\n",
        "roberta_embeddings = RoBERTaEmbeddings('roberta-base')\n",
        "stacked_embeddings = StackedEmbeddings([\n",
        "                                        elmo_embeddings,\n",
        "                                        roberta_embeddings,\n",
        "                                       ])\n",
        "\n",
        "# create a sentence #\n",
        "sentence = Sentence('test sent')\n",
        "# embed words in sentence #\n",
        "stacked_embeddings.embed(sentence)\n",
        "embedding_len = sentence[0].embedding.size()[0]\n",
        "print('embedding len', embedding_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 336/336 [00:00<00:00, 742511.14B/s]\n",
            "100%|██████████| 54402456/54402456 [00:02<00:00, 20212502.71B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "embedding len 1536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJNcWzMduAtb",
        "colab_type": "code",
        "outputId": "c878817b-90ae-40ce-a383-cd2934362b5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for word, i in tqdm(word_index.items(), total=len(word_index)):\n",
        "    if i >= MAX_TOKENS:\n",
        "        continue\n",
        "    try:\n",
        "        word_sent = Sentence(word)\n",
        "        stacked_embeddings.embed(word_sent)\n",
        "        embedding_vector = word_sent[0].embedding.cpu().detach().numpy()\n",
        "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "           embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        pass\n",
        "        #embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 184924/184924 [11:48<00:00, 260.89it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMl76miAyRyI",
        "colab_type": "code",
        "outputId": "3f1eae01-4bf3-4e00-ec66-7ec4bf4c2c87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "del stacked_embeddings\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "549"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyxmTYb4Q2dp",
        "colab_type": "text"
      },
      "source": [
        "### BERT fine-tuned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE-uRZUmQ4d9",
        "colab_type": "code",
        "outputId": "d4168b6b-56c3-48e1-f38f-32bce205abdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "print(hub.__version__)\n",
        "\n",
        "#Installing BERT module\n",
        "!pip install bert-tensorflow\n",
        "\n",
        "#Importing BERT modules\n",
        "import bert\n",
        "from bert import run_classifier, optimization, tokenization"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7.0\n",
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMoU4oQJW8yi",
        "colab_type": "code",
        "outputId": "55823c16-d264-4a24-a4fe-1664f63bb697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "preprocessed_train_with_labels = pd.concat([pd.DataFrame(preprocessed_train, columns=['preprocessed_text']),\n",
        "                                            pd.DataFrame(train_labels, columns=TARGET_COLS)], axis=1)\n",
        "preprocessed_train_with_labels.head(2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessed_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>explanation why edits made username hardcore m...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>d'aww ! he matches background colour i'm seemi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   preprocessed_text  ...  identity_hate\n",
              "0  explanation why edits made username hardcore m...  ...              0\n",
              "1  d'aww ! he matches background colour i'm seemi...  ...              0\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTP1fY-25QaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_examples(df, labels_available=True):\n",
        "    \"\"\"\n",
        "    Creates input examples for the sets of texts and labels.\n",
        "    https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/run_classifier.py#L127    \n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    for (i, row) in enumerate(df.values):\n",
        "      guid = None #row[0]\n",
        "      text_a = row[0]\n",
        "      if labels_available:\n",
        "        labels = row[1:].tolist()\n",
        "      else: # what should be here in test phase\n",
        "        labels = [0, 0, 0, 0, 0, 0]\n",
        "      examples.append(bert.run_classifier.InputExample(guid=guid, text_a = text_a, \n",
        "                                                       label = labels))\n",
        "    return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMiU0RrN77JS",
        "colab_type": "code",
        "outputId": "ea8ea02e-aa12-4ab1-bbf5-965f26337ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module(bert_path):\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    bert_module =  hub.Module(bert_path)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    vocab_file, do_lower_case = sess.run(\n",
        "        [\n",
        "            tokenization_info[\"vocab_file\"],\n",
        "            tokenization_info[\"do_lower_case\"],\n",
        "        ]\n",
        "    )\n",
        "    print('loading tokenizer with{} lowercasing'.format('out'*(not do_lower_case)))\n",
        "\n",
        "    return bert.tokenization.FullTokenizer(\n",
        "        vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "    \n",
        "bert_tokenizer = create_tokenizer_from_hub_module(BERT_MODEL_HUB)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loading tokenizer without lowercasing\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhlik1IGTLfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#todo: split to funct like here https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22\n",
        "class InputFeatures(object):\n",
        "    \"\"\"\n",
        "    A single set of features of data.\n",
        "    https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/run_classifier.py#L161\n",
        "    The only difference from the original implementation (line above) is label_ids instead of label_id because\n",
        "    here we solve multi-label not multi-class task.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_ids, is_real_example=True):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_ids = label_ids,\n",
        "        self.is_real_example=is_real_example\n",
        "\n",
        "def convert_examples_to_features(examples,  max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in tqdm(enumerate(examples), total=len(examples)):\n",
        "        \n",
        "        #print(example.text_a)\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids: 0   0   0   0  0     0 0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "\n",
        "        \"\"\"\n",
        "        # in original repo\n",
        "        tokens = []\n",
        "        segment_ids = []\n",
        "        tokens.append(\"[CLS]\")\n",
        "        segment_ids.append(0)\n",
        "        for token in tokens_a:\n",
        "            tokens.append(token)\n",
        "            segment_ids.append(0)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(0)\n",
        "        \"\"\"\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        \"\"\"\n",
        "        # in original repo\n",
        "        if tokens_b:\n",
        "        for token in tokens_b:\n",
        "            tokens.append(token)\n",
        "            segment_ids.append(1)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(1)\n",
        "        \"\"\"\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [\"[SEP]\"]\n",
        "            segment_ids += [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        '''\n",
        "        # in original repo\n",
        "        while len(input_ids) < max_seq_length:\n",
        "          input_ids.append(0)\n",
        "          input_mask.append(0)\n",
        "          segment_ids.append(0)\n",
        "        '''\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "        \n",
        "        labels_ids = []\n",
        "        for label in example.label:\n",
        "            labels_ids.append(int(label))\n",
        "\n",
        "        if ex_index < 2: # make as param with default value\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                    [str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\n",
        "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"label: %s (id = %s)\" % (example.label, labels_ids))\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              label_ids=labels_ids))\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXl9kZi-wuXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features_to_arrays(features):\n",
        "    \"\"\"Convert a list of InputFeatures to np.arrays\"\"\"\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_input_mask = []\n",
        "    all_segment_ids = []\n",
        "\n",
        "    for feature in features:\n",
        "        all_input_ids.append(feature.input_ids)\n",
        "        all_input_mask.append(feature.input_mask)\n",
        "        all_segment_ids.append(feature.segment_ids)\n",
        "\n",
        "    return (np.array(all_input_ids, dtype='int32'), \n",
        "            np.array(all_input_mask, dtype='int32'), \n",
        "            np.array(all_segment_ids, dtype='int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xcUsR46o-ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text2input_arrays(text, tokenizer, max_seq_length, labels_available):\n",
        "    examples = create_examples(text, labels_available=labels_available)\n",
        "    features = convert_examples_to_features(examples,  max_seq_length, tokenizer)\n",
        "    input_ids, input_masks, segment_ids = features_to_arrays(features)\n",
        "\n",
        "    return input_ids, input_masks, segment_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-2wIb1Ns-uv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354,
          "referenced_widgets": [
            "cccc82610dc7476ca26f1ae13688cec4",
            "7a442345ef6241dc9d047691be3d54a9",
            "f756253d6f594f1299d531a1cdbd800f",
            "9b5a3b51bf7740f493201579c1f9797a",
            "c1ca1a06e3c44cd9a48e5faec94f0b34",
            "d2f753ae54d44416be36541d7a0992dd",
            "c318466ffeb544f2b27ceb449446a7a7",
            "966b31080ad040bd9f1ee8e6a561059b"
          ]
        },
        "outputId": "867f4c11-6790-43d6-ee98-0c73aa9d0fe0"
      },
      "source": [
        "%%time\n",
        "\n",
        "train_input_ids, train_input_masks, train_segment_ids = text2input_arrays(text=preprocessed_train_with_labels, \n",
        "                                                                          tokenizer=bert_tokenizer, \n",
        "                                                                          max_seq_length=MAX_WORDS_IN_COMMENT, \n",
        "                                                                          labels_available=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cccc82610dc7476ca26f1ae13688cec4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=159571), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:*** Example ***\n",
            "INFO:__main__:guid: None\n",
            "INFO:__main__:tokens: [CLS] explanation why edit ##s made user ##name hardcore metallic ##a fan reverted ? they van ##dal ##isms , closure gas I voted new yo ##rk dolls FA ##C . and please remove template talk page since i ' m retired . . [SEP]\n",
            "INFO:__main__:input_ids: 101 7108 1725 14609 1116 1189 4795 16124 16883 13256 1161 5442 17464 136 1152 3498 6919 16762 117 8354 3245 146 4751 1207 26063 4661 22084 6820 1658 119 1105 4268 5782 27821 2037 3674 1290 178 112 182 2623 119 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:label: [0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0])\n",
            "INFO:__main__:*** Example ***\n",
            "INFO:__main__:guid: None\n",
            "INFO:__main__:tokens: [CLS] d ' a ##w ##w ! he matches background colour i ' m seemingly stuck . thanks . talk , j ##anu ##ary , UTC [SEP]\n",
            "INFO:__main__:input_ids: 101 173 112 170 2246 2246 106 1119 2697 3582 5922 178 112 182 9321 5342 119 5438 119 2037 117 179 19762 3113 117 11390 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:label: [0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "CPU times: user 2min 15s, sys: 1.68 s, total: 2min 16s\n",
            "Wall time: 2min 16s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDqqF7PZ7yrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input_ids, valid_input_ids, \\\n",
        "train_input_masks, valid_input_masks, \\\n",
        "train_segment_ids, valid_segment_ids, \\\n",
        "train_labels, valid_labels = \\\n",
        "train_test_split(train_input_ids, train_input_masks, train_segment_ids, train_labels, test_size = 0.1, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS5SmpT1_r_M",
        "colab_type": "code",
        "outputId": "7de13561-7e30-44e1-85e4-b25796abf6c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLJDThTwf_Vb",
        "colab_type": "text"
      },
      "source": [
        "TODO: create pipeline which uses features as is or without data->features->arrays but data->arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF8fnd_OMGPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, bert_path, verbose=True, n_fine_tune_layers=10, **kwargs):\n",
        "        self.bert_path = bert_path\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        # parse it with regular expression\n",
        "        self.output_size = 768\n",
        "        self.verbose = verbose\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(\n",
        "            self.bert_path,\n",
        "            trainable=self.trainable,\n",
        "            name=\"{}_module\".format(self.name)\n",
        "        )\n",
        "        trainable_vars = self.bert.variables\n",
        "        \n",
        "        # Remove unused layers\n",
        "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
        "        \n",
        "        # Select how many layers to fine tune\n",
        "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
        "        \n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "        \n",
        "        # Add non-trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"*** TRAINABLE VARS *** \")\n",
        "            for var in self._trainable_weights:\n",
        "                print(var)\n",
        "          \n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        #inputs = [inputs.input_ids, inputs.input_mask, inputs.segment_ids]\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "            \"pooled_output\"\n",
        "        ]\n",
        "\n",
        "        \"\"\"\n",
        "        output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "        \n",
        "        if self.pooling == \"cls\":\n",
        "            pooled = output[\"pooled_output\"]\n",
        "        else:\n",
        "            result = output[\"sequence_output\"]\n",
        "            \n",
        "            input_mask = tf.cast(input_mask, tf.float32)\n",
        "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "            \n",
        "            if self.pooling == \"mean\":\n",
        "              pooled = masked_reduce_mean(result, input_mask)\n",
        "            else:\n",
        "              pooled = mul_mask(result, input_mask)\n",
        "\n",
        "        return pooled\n",
        "        \"\"\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_size)\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'bert_path': bert_path, \n",
        "            'verbose': verbose, \n",
        "            'n_fine_tune_layers': n_fine_tune_layers,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# can take more fully model from https://github.com/strongio/keras-bert/blob/master/keras-bert.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D8BuKRaMmiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build model\n",
        "def build_model(max_seq_length): \n",
        "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
        "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
        "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
        "    bert_inputs = [in_id, in_mask, in_segment]\n",
        "    \n",
        "    # Instantiate the custom Bert Layer defined above\n",
        "    bert_output = BertLayer(BERT_MODEL_HUB, verbose=True, n_fine_tune_layers=10)(bert_inputs)\n",
        "\n",
        "    # Build the rest of the classifier \n",
        "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
        "    pred = tf.keras.layers.Dense(len(TARGET_COLS), activation='sigmoid')(dense)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def initialize_vars(sess):\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTf9q8JGsc5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class RocAucEvaluation(Callback):\n",
        "#     def __init__(self, validation_data=(), interval=1):\n",
        "#         super(Callback, self).__init__()\n",
        "\n",
        "#         self.interval = interval\n",
        "#         self.X_val, self.y_val = validation_data\n",
        "\n",
        "#     def on_epoch_end(self, epoch, logs={}):\n",
        "#         if epoch % self.interval == 0:\n",
        "#             y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "#             score = 0\n",
        "#             for i in range(6):\n",
        "#              score += roc_auc_score(self.y_val[:,i], y_pred[:,i])/6.\n",
        "#             print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
        "\n",
        "# RocAuc_val = RocAucEvaluation(validation_data=(x_valid, y_valid), interval = 1)\n",
        "\n",
        "class RocAucEarlyStopping(Callback):\n",
        "    \"\"\"Callback for early stopping based on roc auc on the validation set\"\"\"\n",
        "\n",
        "    def __init__(self, validation_data, patience=0, digits=3):\n",
        "        super().__init__()\n",
        "        self.X_val, self.y_val = validation_data\n",
        "        self.best = 0\n",
        "        self.patience = patience\n",
        "        self.digits = digits\n",
        "        self.current_patience = patience\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "        score = 0\n",
        "        for i in range(6):\n",
        "          score += roc_auc_score(self.y_val[:,i], y_pred[:,i])/6.\n",
        "\n",
        "        score = round(score, self.digits)\n",
        "        print(\"\\nROC-AUC - epoch: {:d} - score: {}\\n\".format(epoch+1, score))\n",
        "\n",
        "        # check digits\n",
        "        if np.greater(score, self.best):\n",
        "            self.best = score\n",
        "            self.current_patience = self.patience\n",
        "        else:\n",
        "            print('\\nbest:{}\\ncurrent:{}'.format(self.best, score))\n",
        "            self.current_patience -= 1\n",
        "            if self.current_patience < 0:\n",
        "              self.model.stop_training = True\n",
        "              print('Early stopping due to lower roc auc')\n",
        "              self.current_patience = self.patience\n",
        "            else:\n",
        "              print('{} patience remained'.format(self.current_patience))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNCf8ZpEsQcL",
        "colab_type": "text"
      },
      "source": [
        "TODO: clear memory and increase batchsize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiH9J6v7IsZV",
        "colab_type": "code",
        "outputId": "8dbf779b-09fd-4d36-ca48-369f10fbc241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        }
      },
      "source": [
        "gc.collect()\n",
        "\n",
        "model = build_model(max_seq_length=MAX_WORDS_IN_COMMENT)\n",
        "\n",
        "# Instantiate variables\n",
        "initialize_vars(sess)\n",
        "\n",
        "model.fit(\n",
        "        [train_input_ids, train_input_masks, train_segment_ids], \n",
        "        train_labels,\n",
        "        validation_data=([valid_input_ids, valid_input_masks, valid_segment_ids], valid_labels),\n",
        "        epochs=5,\n",
        "        batch_size=128,\n",
        "        callbacks=[RocAucEarlyStopping(validation_data=([valid_input_ids, valid_input_masks, valid_segment_ids], valid_labels), \n",
        "                                       patience=1, digits=4)]\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** TRAINABLE VARS *** \n",
            "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
            "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
            "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
            "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
            "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
            "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "<tf.Variable 'bert_layer_module/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>\n",
            "<tf.Variable 'bert_layer_module/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 220)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 220)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 220)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer (BertLayer)          (None, 768)          108931396   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 6)            1542        dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 109,129,802\n",
            "Trainable params: 6,103,558\n",
            "Non-trainable params: 103,026,244\n",
            "__________________________________________________________________________________________________\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/5\n",
            " 46464/143613 [========>.....................] - ETA: 17:10 - loss: 0.0652"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imo2XJzmTxY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(output_folder+'/BertModel_multilabel.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmerekHYnn7U",
        "colab_type": "code",
        "outputId": "6098a160-1739-4a19-ba6a-e81c7d42cfb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "preprocessed_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['you bitch ja rule succesful ever whats hating sad mofuckas bitch slap your pethedic white faces get kiss ass guys sicken . ja rule pride da music man . dont diss shit . nothin wrong bein like tupac brother fuckin white boys get things right next time . ,',\n",
              "       'from rfc the title fine , IMO .',\n",
              "       'sources zawe ashton lapland - -', ...,\n",
              "       \"okinotorishima categories I see changes agree correct . I gotten confused , found acknowledging japan's territorial rights okinotorishima however , category acknowledge japan's claim exclusive economic zone EEZ stemming okinotorishima . that , category disputed EEZ ?\",\n",
              "       \"one founding nations EU - germany - law return quite similar israel's this actually true , ? germany allows people whose ancestors citizens germany return , AFAIK allow descendants anglo-saxons return angeln saxony . israel , contrast , allows jews return israel , even can't trace particular ancestral line anyone lived modern state even mandate palestine . - -\",\n",
              "       \"stop already . your bullshit welcome . i'm fool , think kind explination enough , well pity .\"],\n",
              "      dtype='<U9950')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V21lhkWODcfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "test_input_ids, test_input_masks, test_segment_ids = text2input_arrays(text=preprocessed_test, \n",
        "                                                                          tokenizer=bert_tokenizer, \n",
        "                                                                          max_seq_length=MAX_WORDS_IN_COMMENT, \n",
        "                                                                          labels_available=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odnkbt-ZnIj6",
        "colab_type": "code",
        "outputId": "bb13c9ec-c804-46ec-a1b5-44f07318ef5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "pred  = model.predict([test_input_ids, test_input_masks, test_segment_ids])\n",
        "pred.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 13min 47s, sys: 9min 10s, total: 22min 58s\n",
            "Wall time: 20min 47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AslnHsTy0gCQ",
        "colab_type": "text"
      },
      "source": [
        "## Build and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m6GDvsd3WRX",
        "colab_type": "code",
        "trusted": true,
        "outputId": "fdac63b2-b009-4c48-a5dd-127d8436cdfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "x_train, x_valid, y_train, y_valid = train_test_split(tokenized_train, \n",
        "                                                      train_labels, \n",
        "                                                      test_size = 0.1,\n",
        "                                                      shuffle=True)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_valid shape:', x_valid.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_valid shape:', y_valid.shape)\n",
        "\n",
        "del tokenized_train\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (143613, 220)\n",
            "x_valid shape: (15958, 220)\n",
            "y_train shape: (143613, 6)\n",
            "y_valid shape: (15958, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6306i8ha3TBF",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def build_model(embedding_matrix):\n",
        "    words = Input(shape=(MAX_WORDS_IN_COMMENT,))\n",
        "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
        "    #x = Input(batch_shape=(batch_size, MAX_CHARS_IN_COMMENT, embedding_len))\n",
        "    \n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x)\n",
        "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "\n",
        "    hidden = concatenate([\n",
        "        GlobalMaxPooling1D()(x),\n",
        "        GlobalAveragePooling1D()(x),\n",
        "    ])\n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
        "    result = Dense(len(TARGET_COLS), activation='sigmoid')(hidden)\n",
        "    \n",
        "    \n",
        "    model = Model(inputs=words, outputs=result)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pEiRNein_rW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RocAuc_ES = RocAucEarlyStopping(validation_data=(x_valid, y_valid), patience=1, digits=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TRvdcnky3TBH",
        "colab_type": "code",
        "outputId": "24f73e36-a368-4a42-fe8b-2bb0901d3766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "SEEDS = 2\n",
        "pred = 0\n",
        "\n",
        "for ii in tqdm(range(SEEDS), total=SEEDS):\n",
        "\n",
        "    model = build_model(embedding_matrix)\n",
        "    for global_epoch in range(GLOBAL_EPOCHS):\n",
        "        print('\\nglobal_epoch', global_epoch)\n",
        "\n",
        "\n",
        "        model.fit(\n",
        "                    x_train,\n",
        "                    y_train,\n",
        "                    validation_data = (x_valid, y_valid),\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    verbose=1,\n",
        "                    callbacks=[\n",
        "                        LearningRateScheduler(lambda _: 1e-3 * (0.5 ** global_epoch)),\n",
        "                        #RocAuc_val\n",
        "                        RocAuc_ES\n",
        "                    ]\n",
        "                )\n",
        "        print('fitted')    \n",
        "\n",
        "    pred += model.predict(tokenized_test, batch_size = 1024, verbose = 1)/SEEDS\n",
        "    np.save('pred', pred)\n",
        "    model.save_weights('model_weights_'+str(ii)+'.h5')\n",
        "    os.system('gzip '+'model_weights_'+str(ii)+'.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "\n",
            "global_epoch 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/5\n",
            "143613/143613 [==============================] - 87s 605us/step - loss: 0.0700 - val_loss: 0.0497\n",
            "ROC-AUC - epoch: 1 - score: 0.9787\n",
            "\n",
            "Epoch 2/5\n",
            "143613/143613 [==============================] - 82s 569us/step - loss: 0.0473 - val_loss: 0.0446\n",
            "ROC-AUC - epoch: 2 - score: 0.9856\n",
            "\n",
            "Epoch 3/5\n",
            "143613/143613 [==============================] - 82s 569us/step - loss: 0.0429 - val_loss: 0.0445\n",
            "ROC-AUC - epoch: 3 - score: 0.9868\n",
            "\n",
            "Epoch 4/5\n",
            "143613/143613 [==============================] - 82s 568us/step - loss: 0.0402 - val_loss: 0.0428\n",
            "ROC-AUC - epoch: 4 - score: 0.9879\n",
            "\n",
            "Epoch 5/5\n",
            "143613/143613 [==============================] - 82s 569us/step - loss: 0.0382 - val_loss: 0.0447\n",
            "ROC-AUC - epoch: 5 - score: 0.9881\n",
            "\n",
            "fitted\n",
            "\n",
            "global_epoch 1\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/5\n",
            "143613/143613 [==============================] - 82s 569us/step - loss: 0.0340 - val_loss: 0.0440\n",
            "ROC-AUC - epoch: 1 - score: 0.9878\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.9878\n",
            "0 patience remained\n",
            "Epoch 2/5\n",
            "143613/143613 [==============================] - 82s 570us/step - loss: 0.0321 - val_loss: 0.0443\n",
            "ROC-AUC - epoch: 2 - score: 0.9877\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.9877\n",
            "Early stopping due to lower roc auc\n",
            "fitted\n",
            "153164/153164 [==============================] - 33s 213us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 1/2 [11:51<11:51, 711.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "global_epoch 0\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/5\n",
            "143613/143613 [==============================] - 82s 570us/step - loss: 0.0693 - val_loss: 0.0518\n",
            "ROC-AUC - epoch: 1 - score: 0.98\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.98\n",
            "0 patience remained\n",
            "Epoch 2/5\n",
            "143613/143613 [==============================] - 81s 567us/step - loss: 0.0469 - val_loss: 0.0469\n",
            "ROC-AUC - epoch: 2 - score: 0.9854\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.9854\n",
            "Early stopping due to lower roc auc\n",
            "fitted\n",
            "\n",
            "global_epoch 1\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/5\n",
            "143613/143613 [==============================] - 81s 567us/step - loss: 0.0417 - val_loss: 0.0426\n",
            "ROC-AUC - epoch: 1 - score: 0.9873\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.9873\n",
            "0 patience remained\n",
            "Epoch 2/5\n",
            "143613/143613 [==============================] - 81s 566us/step - loss: 0.0395 - val_loss: 0.0427\n",
            "ROC-AUC - epoch: 2 - score: 0.9878\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.9878\n",
            "Early stopping due to lower roc auc\n",
            "fitted\n",
            "153164/153164 [==============================] - 33s 213us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 2/2 [18:48<00:00, 623.22s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10min 36s, sys: 4min, total: 14min 36s\n",
            "Wall time: 18min 48s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujanlBVs61IJ",
        "colab_type": "text"
      },
      "source": [
        "## Cheaty evaluating on test labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIOGCjO11DrT",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "submission = pd.read_csv(submission_path)\n",
        "submission[TARGET_COLS] = (pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCukoB8QB2Rz",
        "colab_type": "code",
        "trusted": true,
        "outputId": "760be629-18f0-4d73-c279-07df46bebc54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "submission.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00001cee341fdb12</td>\n",
              "      <td>0.996676</td>\n",
              "      <td>3.414785e-01</td>\n",
              "      <td>0.912232</td>\n",
              "      <td>9.897777e-02</td>\n",
              "      <td>0.918225</td>\n",
              "      <td>0.090533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000247867823ef7</td>\n",
              "      <td>0.008345</td>\n",
              "      <td>3.433228e-05</td>\n",
              "      <td>0.000827</td>\n",
              "      <td>7.009506e-05</td>\n",
              "      <td>0.001426</td>\n",
              "      <td>0.000180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00013b17ad220c46</td>\n",
              "      <td>0.000289</td>\n",
              "      <td>3.278255e-07</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>1.728535e-06</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00017563c3f7919a</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>5.960464e-08</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>9.238720e-07</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00017695ad8997eb</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>1.132488e-05</td>\n",
              "      <td>0.000740</td>\n",
              "      <td>2.795458e-05</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>0.000034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>0.000428</td>\n",
              "      <td>3.576279e-07</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>3.904104e-06</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>00024115d4cbde0f</td>\n",
              "      <td>0.000689</td>\n",
              "      <td>5.662441e-07</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>4.947186e-06</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>0.677728</td>\n",
              "      <td>1.572847e-03</td>\n",
              "      <td>0.090437</td>\n",
              "      <td>3.046185e-03</td>\n",
              "      <td>0.142409</td>\n",
              "      <td>0.002204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00025358d4737918</td>\n",
              "      <td>0.275358</td>\n",
              "      <td>1.869202e-04</td>\n",
              "      <td>0.010572</td>\n",
              "      <td>1.259267e-03</td>\n",
              "      <td>0.062003</td>\n",
              "      <td>0.000249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00026d1092fe71cc</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>5.960464e-08</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>7.748604e-07</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id     toxic  ...    insult  identity_hate\n",
              "0  00001cee341fdb12  0.996676  ...  0.918225       0.090533\n",
              "1  0000247867823ef7  0.008345  ...  0.001426       0.000180\n",
              "2  00013b17ad220c46  0.000289  ...  0.000030       0.000009\n",
              "3  00017563c3f7919a  0.000140  ...  0.000014       0.000002\n",
              "4  00017695ad8997eb  0.003606  ...  0.000358       0.000034\n",
              "5  0001ea8717f6de06  0.000428  ...  0.000054       0.000005\n",
              "6  00024115d4cbde0f  0.000689  ...  0.000085       0.000008\n",
              "7  000247e83dcc1211  0.677728  ...  0.142409       0.002204\n",
              "8  00025358d4737918  0.275358  ...  0.062003       0.000249\n",
              "9  00026d1092fe71cc  0.000164  ...  0.000014       0.000003\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUF4Uxrl8pXr",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)\n",
        "test_labels_df = pd.read_csv(test_labels_filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6crgLcLx7X-p",
        "colab_type": "code",
        "trusted": true,
        "outputId": "5e6ca27c-387b-4a39-8a70-a9d2caf14d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_labels_df = test_labels_df[(test_labels_df[\"toxic\"] != -1) &\n",
        "                                (test_labels_df[\"severe_toxic\"] != -1) &\n",
        "                                (test_labels_df[\"obscene\"] != -1) &\n",
        "                                (test_labels_df[\"threat\"] != -1) &\n",
        "                                (test_labels_df[\"insult\"] != -1) &\n",
        "                                (test_labels_df[\"identity_hate\"] != -1)]\n",
        "test_labels_df.shape                               "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(63978, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ymumFOQ9sA1",
        "colab_type": "code",
        "trusted": true,
        "outputId": "994f1949-312d-4032-eb52-6e0eb9430982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "submission_to_evaluate = submission[submission['id'].isin(test_labels_df['id'].values)]\n",
        "submission_to_evaluate.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(63978, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3r8VTCK_TaM",
        "colab_type": "code",
        "trusted": true,
        "outputId": "e6010eaf-44be-4a9d-80a6-4bf92d2af2ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "scores = []\n",
        "for class_name in TARGET_COLS:\n",
        "    train_target = test_labels_df[class_name]\n",
        "    train_predicted = submission_to_evaluate[class_name]\n",
        "\n",
        "    cv_score = np.mean(roc_auc_score(train_target.values, train_predicted.values))\n",
        "    scores.append(cv_score)\n",
        "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
        "\n",
        "print('Total CV score is {}'.format(np.mean(scores)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV score for class toxic is 0.9623414567715155\n",
            "CV score for class severe_toxic is 0.9883083645713258\n",
            "CV score for class obscene is 0.9758702005183666\n",
            "CV score for class threat is 0.9763517387836063\n",
            "CV score for class insult is 0.972054283405765\n",
            "CV score for class identity_hate is 0.9841185309254274\n",
            "Total CV score is 0.9765074291626678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YKsSnwkd3TBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.to_csv(os.path.join(output_folder,\"submission_fine_tuned_bert.csv\"), index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNf0KsAHCSJB",
        "colab_type": "text"
      },
      "source": [
        "Жирный шрифт означает изменения в процессе валидации и оценивания и при изменении действует начиная со строчки указания и ниже\n",
        "\n",
        "* glove twitter 200:  0.98052\n",
        "\n",
        "**С поднятием колва epochs и global_epochs + RocAucEarlyStopping** скор упал с 0.98052 до 0.97573\n",
        "\n",
        "* GLOBAL_EPOCHS = 2, EPOCHS = 5, glove.840B.300d.txt   0.97994  \n",
        "* BPEmb(lang=\"en\", dim=25, vs=20000)   0.97375  \n",
        "* BPEmb(lang=\"en\", dim=300, vs=20000) without preprocessing   0.97699  \n",
        "* BPEmb(lang=\"en\", dim=300, vs=20000)   0.97865   \n",
        "* fasttext crawl-300d-2M-subword 0.95732  \n",
        "* BPEmb(lang=\"en\", dim=300, vs=20000) **Patience 2->1 SEEDS 1->1**  0.98041   \n",
        "* flair ELMoEmbeddings('small') embedding len 768  0.98005  \n",
        "* flair ELMoEmbeddings('medium') embedding len 1536  0.98038  \n",
        "* flair RoBERTaEmbeddings('roberta-base') embedding len 768  0.97413  \n",
        "* flair stack of ELMoEmbeddings('small') + RoBERTaEmbeddings('roberta-base') embedding len 1536  0.98069      \n",
        "* 3 tuned layers in BERT modeldense = tf.keras.layers.Dense(256, activation='relu')(bert_output)pred = tf.keras.layers.Dense(len(TARGET_COLS), activation='sigmoid')(dense)model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])epochs=1,batch_size=32   0.97658"
      ]
    }
  ]
}