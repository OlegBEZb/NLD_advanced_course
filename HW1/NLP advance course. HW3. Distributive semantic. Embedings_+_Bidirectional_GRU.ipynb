{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "NLP advance course. HW3. Distributive semantic. Embedings_+_Bidirectional_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GRh7ig80jk3s",
        "1_nWe7O-jqak",
        "TGzxJch3juGy",
        "sWmhQEFAj1VE",
        "_yDXCBmqtkPl",
        "0hTlPvNY8gi1"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlegBEZb/NLP_advanced_course/blob/master/HW1/NLP%20advance%20course.%20HW3.%20Distributive%20semantic.%20Embedings_%2B_Bidirectional_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVRaU_zG-PaI",
        "colab_type": "text"
      },
      "source": [
        "## Embeddings to check\n",
        "\n",
        "* Word Embeddings (word2vec, GloVe, etc.)\n",
        "* Starspace and other similarity learning embeddings\n",
        "* char-gram embeddings (bpemb, fasttext etc.)\n",
        "* doc and sentence embeddings (doc2vec, sent2vec etc.)\n",
        "* specific context models (BERT, ELMo)\n",
        "* Advanced: Poincare Embeddings concept"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30D0Scxk4kn3",
        "colab_type": "text"
      },
      "source": [
        "# Libs import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ZnXUdrxQ3TAN",
        "colab_type": "code",
        "outputId": "2a5588f4-28ac-420f-c869-c5416a092ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "import os,csv, random, pickle, re, sys\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "import nltk\n",
        "nltk.download('stopwords');\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "! pip install Unidecode;\n",
        "from unidecode import unidecode\n",
        "#! pip install pyspellchecker;\n",
        "#from spellchecker import SpellChecker\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold, train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
        "from tensorflow.keras.layers import Activation, Dropout, CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, CuDNNGRU\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, Callback\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wi85ywI6MouM",
        "colab": {}
      },
      "source": [
        "# Initialize session\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypZqfsNI6-Ne",
        "colab_type": "text"
      },
      "source": [
        "# Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JtxOKtA35gA",
        "colab_type": "code",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd34a359-464d-488b-d7bc-16a2dba6cb5e"
      },
      "source": [
        "if tf.test.is_gpu_available(\n",
        "    cuda_only=False,\n",
        "    min_cuda_compute_capability=None\n",
        "):\n",
        "    print(tf.test.gpu_device_name())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i673BlTJEazT",
        "colab_type": "text"
      },
      "source": [
        "# Global vars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okymGz_lEjLC",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "WORKSPACE = 'COLAB' # or 'KAGGLE'\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "LSTM_UNITS = 128\n",
        "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
        "GLOBAL_EPOCHS = 2\n",
        "EPOCHS = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD2z8D2V7Dod",
        "colab_type": "text"
      },
      "source": [
        "# Authorization on Google drive and configurings paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auzLnrdK51Rg",
        "colab_type": "code",
        "trusted": true,
        "outputId": "7a22ff63-0dbb-4073-afdd-c68818575ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "if WORKSPACE == 'COLAB':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    homework_folder = os.path.join('/content/drive/My Drive', 'Advanced NLP/Homework 1: Classical classification task like Kaggle Toxic or Quora')\n",
        "    data_folder = os.path.join(homework_folder, 'Toxic data')\n",
        "    embeddings_folder = os.path.join(homework_folder, 'embeddings')\n",
        "    output_folder = os.path.join(homework_folder, 'output')\n",
        "elif WORKSPACE == 'KAGGLE':\n",
        "    data_folder = '../input/jigsaw-toxic-comment-classification-challenge/'\n",
        "    embeddings_folder = '../input/glove-global-vectors-for-word-representation'\n",
        "else:\n",
        "    pass\n",
        "\n",
        "print('data found:', os.listdir(data_folder))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "data found: ['test.csv', 'test_labels.csv', 'train.csv', 'sample_submission.csv', 'submission.csv', 'submission_emb_sklearn.csv', 'submission_bidirectional_GRU.csv', 'train_labels.csv.npy', 'train_labels.npy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UYdAXsdT3TAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_filepath = os.path.join(data_folder,\"train.csv\")\n",
        "test_filepath = os.path.join(data_folder,\"test.csv\")\n",
        "test_labels_filepath = os.path.join(data_folder,\"test_labels.csv\")\n",
        "\n",
        "#path to a sample submission\n",
        "submission_path = os.path.join(data_folder,\"sample_submission.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15GVrJYa717o",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYljyuKTCm8r",
        "colab_type": "text"
      },
      "source": [
        "## Dicts and lists of useful words and transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjL5O731CtDD",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#List of some words that often appear in toxic comments\n",
        "#Sorry about the level of toxicity in it!\n",
        "toxic_words = [\"poop\", \"crap\", \"prick\", \"twat\", \"wikipedia\", \"wiki\", \"hahahahaha\", \"lol\", \"bastard\", \"sluts\", \"slut\", \"douchebag\", \"douche\", \"blowjob\", \"nigga\", \"dumb\", \"jerk\", \"wanker\", \"wank\", \"penis\", \"motherfucker\", \"fucker\", \"fuk\", \"fucking\", \"fucked\", \"fuck\", \"bullshit\", \"shit\", \"stupid\", \"bitches\", \"bitch\", \"suck\", \"cunt\", \"dick\", \"cocks\", \"cock\", \"die\", \"kill\", \"gay\", \"jewish\", \"jews\", \"jew\", \"niggers\", \"nigger\", \"faggot\", \"fag\", \"asshole\"]\n",
        "# todo: convert to dict and use in normalising by dict\n",
        "astericks_words = [('mother****ers', 'motherfuckers'), ('motherf*cking', 'motherfucking'), ('mother****er', 'motherfucker'), ('motherf*cker', 'motherfucker'), ('bullsh*t', 'bullshit'), ('f**cking', 'fucking'), ('f*ucking', 'fucking'), ('fu*cking', 'fucking'), ('****ing', 'fucking'), ('a**hole', 'asshole'), ('assh*le', 'asshole'), ('f******', 'fucking'), ('f*****g', 'fucking'), ('f***ing', 'fucking'), ('f**king', 'fucking'), ('f*cking', 'fucking'), ('fu**ing', 'fucking'), ('fu*king', 'fucking'), ('fuc*ers', 'fuckers'), ('f*****', 'fucking'), ('f***ed', 'fucked'), ('f**ker', 'fucker'), ('f*cked', 'fucked'), ('f*cker', 'fucker'), ('f*ckin', 'fucking'), ('fu*ker', 'fucker'), ('fuc**n', 'fucking'), ('ni**as', 'niggas'), ('b**ch', 'bitch'), ('b*tch', 'bitch'), ('c*unt', 'cunt'), ('f**ks', 'fucks'), ('f*ing', 'fucking'), ('ni**a', 'nigga'), ('c*ck', 'cock'), ('c*nt', 'cunt'), ('cr*p', 'crap'), ('d*ck', 'dick'), ('f***', 'fuck'), ('f**k', 'fuck'), ('f*ck', 'fuck'), ('fc*k', 'fuck'), ('fu**', 'fuck'), ('fu*k', 'fuck'), ('s***', 'shit'), ('s**t', 'shit'), ('sh**', 'shit'), ('sh*t', 'shit'), ('tw*t', 'twat')]\n",
        "fasttext_misspelings = {\"'n'balls\": 'balls', \"-nazi's\": 'nazis', 'adminabuse': 'admin abuse', \"admins's\": 'admins', 'arsewipe': 'arse wipe', 'assfack': 'asshole', 'assholifity': 'asshole', 'assholivity': 'asshole', 'asshoul': 'asshole', 'asssholeee': 'asshole', 'belizeans': 'mexicans', \"blowing's\": 'blowing', 'bolivians': 'mexicans', 'celtofascists': 'fascists', 'censorshipmeisters': 'censor', 'chileans': 'mexicans', 'clerofascist': 'fascist', 'cowcrap': 'crap', 'crapity': 'crap', \"d'idiots\": 'idiots', 'deminazi': 'nazi', 'dftt': \"don't feed the troll\", 'dildohs': 'dildo', 'dramawhores': 'drama whores', 'edophiles': 'pedophiles', 'eurocommunist': 'communist', 'faggotkike': 'faggot', 'fantard': 'retard', 'fascismnazism': 'fascism', 'fascistisized': 'fascist', 'favremother': 'mother', 'fuxxxin': 'fucking', \"g'damn\": 'goddamn', 'harassmentat': 'harassment', 'harrasingme': 'harassing me', 'herfuc': 'motherfucker', 'hilterism': 'fascism', 'hitlerians': 'nazis', 'hitlerites': 'nazis', 'hubrises': 'pricks', 'idiotizing': 'idiotic', 'inadvandals': 'vandals', \"jackass's\": 'jackass', 'jiggabo': 'nigga', 'jizzballs': 'jizz balls', 'jmbass': 'dumbass', 'lejittament': 'legitimate', \"m'igger\": 'nigger', \"m'iggers\": 'niggers', 'motherfacking': 'motherfucker', 'motherfuckenkiwi': 'motherfucker', 'muthafuggas': 'niggas', 'nazisms': 'nazis', 'netsnipenigger': 'nigger', 'niggercock': 'nigger', 'niggerspic': 'nigger', 'nignog': 'nigga', 'niqqass': 'niggas', \"non-nazi's\": 'not a nazi', 'panamanians': 'mexicans', 'pedidiots': 'idiots', 'picohitlers': 'hitler', 'pidiots': 'idiots', 'poopia': 'poop', 'poopsies': 'poop', 'presumingly': 'obviously', 'propagandaanddisinformation': 'propaganda and disinformation', 'propagandaministerium': 'propaganda', 'puertoricans': 'mexicans', 'puertorricans': 'mexicans', 'pussiest': 'pussies', 'pussyitis': 'pussy', 'rayaridiculous': 'ridiculous', 'redfascists': 'fascists', 'retardzzzuuufff': 'retard', \"revertin'im\": 'reverting', 'scumstreona': 'scums', 'southamericans': 'mexicans', 'strasserism': 'fascism', 'stuptarded': 'retarded', \"t'nonsense\": 'nonsense', \"threatt's\": 'threat', 'titoists': 'communists', 'twatbags': 'douchebags', 'youbollocks': 'you bollocks'}\n",
        "acronym_words = {\"btw\":\"by the way\", \"yo\": \"you\", \"u\": \"you\", \"r\": \"are\", \"ur\": \"your\", \"ima\": \"i am going to\", \"imma\": \"i am going to\", \"i'ma\":\"i am going to\", \"cos\":\"because\", \"coz\":\"because\", \"stfu\": \"shut the fuck up\", \"wat\": \"what\"}\n",
        "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
        "letter_replacement = {\"$\": \"s\", \"@\": \"a\"}\n",
        "unspaced_signs = {\"!\": \" ! \", \"?\": \" ? \"}\n",
        "url_replace_dict = {\"http://\": '',\n",
        "                    \"www.\": '',\n",
        "                    \"https://\": '',\n",
        "                    \"wikipedia.org\": ''}\n",
        "#apparently, people on wikipedia love to talk about sockpuppets :-)\n",
        "sockpuppets_dict = {\"sock puppet\": \"sockpuppet\", \"SOCK PUPPET\": \"SOCKPUPPET\"}\n",
        "\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "#spell_checker = SpellChecker()\n",
        "\n",
        "cont_patterns = [\n",
        "    (r'(W|w)on\\'t', r'will not'),\n",
        "    (r'(C|c)an\\'t', r'can not'),\n",
        "    (r'(I|i)\\'m', r'i am'),\n",
        "    (r'(A|a)in\\'t', r'is not'),\n",
        "    (r'(\\w+)\\'ll', r'\\g<1> will'),\n",
        "    (r'(\\w+)n\\'t', r'\\g<1> not'),\n",
        "    (r'(\\w+)\\'ve', r'\\g<1> have'),\n",
        "    (r'(\\w+)\\'s', r'\\g<1> is'),\n",
        "    (r'(\\w+)\\'re', r'\\g<1> are'),\n",
        "    (r'(\\w+)\\'d', r'\\g<1> would'),\n",
        "]\n",
        "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
        "\n",
        "#We will filter all characters except alphabet characters and some punctuation\n",
        "valid_characters = \" \" + \"@$\" + \"'!?-\" + \"abcdefghijklmnopqrstuvwxyz\" + \"abcdefghijklmnopqrstuvwxyz\".upper()\n",
        "valid_set = set(x for x in valid_characters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs_nlu1iILY1",
        "colab_type": "text"
      },
      "source": [
        "## Funcs for processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfUaIP-wIKTZ",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def replace_in_sentence(sentence, dictionary):\n",
        "    for key in dictionary.keys():\n",
        "        if key in sentence:\n",
        "            sentence = sentence.replace(key, dictionary[key])\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def split_toxic_and_normal_parts(word, toxic_words):\n",
        "    if word == \"\":\n",
        "        return \"\"\n",
        "    \n",
        "    lower = word.lower()\n",
        "    for toxic_word in toxic_words:\n",
        "        start = lower.find(toxic_word)\n",
        "        if start >= 0:\n",
        "            end = start + len(toxic_word)\n",
        "            result = \" \".join([word[0:start], word[start:end], split_toxic_and_normal_parts(word[end:], toxic_words)])\n",
        "            return result.replace(\"  \", \" \").strip()\n",
        "    return word\n",
        "\n",
        "\n",
        "def normalize_by_dictionary(normalized_word, dictionary):\n",
        "    \"\"\"\n",
        "    Word to be normalized (could be a phrase) with dictionary and the dictionary passed.\n",
        "    Replaces each word in passed word-phrase into their representations in dictionary.\n",
        "    Uppercased words are matched in lowercase but returned in upper. Other cases\n",
        "    are checked in lower and returned in lower.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for word in normalized_word.split():\n",
        "        if word == word.upper():\n",
        "            if word.lower() in dictionary:\n",
        "                result.append(dictionary[word.lower()].upper())\n",
        "            else:\n",
        "                result.append(word)\n",
        "        else:\n",
        "            #print('word', '\\'', word, '\\'', 'is not upper')\n",
        "            #print('word.lower()', word.lower())\n",
        "            if word.lower() in dictionary:\n",
        "                result.append(dictionary[word.lower()])\n",
        "            else:\n",
        "                result.append(word)\n",
        "    \n",
        "    return \" \".join(result)\n",
        "\n",
        "\n",
        "def normalize_comment(comment, tokenizer, max_chars_in_comment):\n",
        "    comment = unidecode(comment)\n",
        "    comment = comment[:max_chars_in_comment]\n",
        "\n",
        "    comment = replace_in_sentence(comment, letter_replacement)\n",
        "    comment = replace_in_sentence(comment, unspaced_signs)\n",
        "    comment = replace_in_sentence(comment, url_replace_dict)\n",
        "    comment = replace_in_sentence(comment, sockpuppets_dict)    \n",
        "\n",
        "    # ('mother****ers', 'motherfuckers')\n",
        "    # for w in astericks_words:\n",
        "    #     if w[0] in comment:\n",
        "    #         comment = comment.replace(w[0], w[1])\n",
        "    #     if w[0].upper() in comment:\n",
        "    #         comment = comment.replace(w[0].upper(), w[1].upper())\n",
        "    \n",
        "    normalized_words = []\n",
        "    for word in tokenizer.tokenize(comment):\n",
        "        if word in english_stopwords: continue\n",
        "\n",
        "        # # '(W|w)on\\'t', r'will not'\n",
        "        # for (pattern, repl) in patterns:\n",
        "        #    word = re.sub(pattern, repl, word)\n",
        "\n",
        "        if word == \".\" or word == \",\":\n",
        "            normalized_words.append(word)\n",
        "            continue\n",
        "        \n",
        "        # drop url parts from links\n",
        "        # word = replace_url(word)\n",
        "\n",
        "        # replace single dots to whitespaces\n",
        "        if word.count(\".\") == 1:\n",
        "            word = word.replace(\".\", \" \")\n",
        "\n",
        "        # leave only legalized symbols\n",
        "        filtered_word = \"\".join([x for x in word if x in valid_set])\n",
        "                    \n",
        "        #Kind of hack: for every word check if it has a toxic word as a part of it\n",
        "        #If so, split this word by swear and non-swear part.\n",
        "        #normalized_word = split_toxic_and_normal_parts(filtered_word, toxic_words)\n",
        "        normalized_word = filtered_word\n",
        "\n",
        "#         normalized_word = normalize_by_dictionary(normalized_word, hyphens_dict)\n",
        "#         normalized_word = normalize_by_dictionary(normalized_word, merged_dict)\n",
        "        \n",
        "        # check misspellings\n",
        "        # temp = []\n",
        "        # for word in normalized_word.split():\n",
        "        #   temp.append(spell_checker.correction(word))\n",
        "        # normalized_word = \" \".join(temp)\n",
        "          \n",
        "        # normalized_word = normalize_by_dictionary(normalized_word, fasttext_misspelings)\n",
        "        normalized_word = normalize_by_dictionary(normalized_word, acronym_words)\n",
        "\n",
        "        normalized_words.append(normalized_word)\n",
        "        \n",
        "    normalized_comment = \" \".join(normalized_words)\n",
        "    \n",
        "    result = []\n",
        "    for word in normalized_comment.split():\n",
        "        # if word is upper\n",
        "        if word.upper() == word:\n",
        "            result.append(word)\n",
        "        else:\n",
        "            result.append(word.lower())\n",
        "    \n",
        "    result = \" \".join(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def preprocess_text(df, preprocessing_func, tokenizer, max_chars_in_comment):\n",
        "  # cheap and bad preprocessing\n",
        "  # preprocessed_df = df[TEXT_COLUMN].fillna(\"CVxTz\").values\n",
        "  # preprocessed_df = df[TEXT_COLUMN]\n",
        "\n",
        "  text_ndarray = df.fillna('_na').values\n",
        "  np_preprocessing_func = np.vectorize(preprocessing_func)\n",
        "\n",
        "  preprocessed_df = np_preprocessing_func(text_ndarray, tokenizer, max_chars_in_comment)\n",
        "\n",
        "  print('Gained shape:', preprocessed_df.shape)\n",
        "  return preprocessed_df\n",
        "\n",
        "\n",
        "def tokenize_and_pad(text, tokenizer, max_words_in_comment):\n",
        "\n",
        "  tokenized_text = tokenizer.texts_to_sequences(text)\n",
        "  padded_text = pad_sequences(tokenized_text, maxlen=max_words_in_comment, dtype='int', value=0)\n",
        "\n",
        "  print('Gained shape:', padded_text.shape)\n",
        "  return padded_text\n",
        "\n",
        "\n",
        "def load_and_preprocess(filepath, preprocessing_func, tokenizer, \n",
        "                        max_chars_in_comment, get_labels=False):\n",
        "    df = pd.read_csv(filepath)\n",
        "    preprocessed_df = preprocess_text(df=df[TEXT_COLUMN], \n",
        "                                      preprocessing_func=preprocessing_func, \n",
        "                                      tokenizer=tokenizer,\n",
        "                                      max_chars_in_comment=max_chars_in_comment)\n",
        "    if get_labels:\n",
        "        labels = df[TARGET_COLS].values\n",
        "        return preprocessed_df, labels\n",
        "    else:\n",
        "        return preprocessed_df\n",
        "\n",
        "\n",
        "def preproc2tokenized(preprocessed_df, max_tokens, max_words_in_comment, tokenizer=None):\n",
        "    # keras text tokenizer. simple, only for splitting and padding\n",
        "    if not tokenizer:\n",
        "        tokenizer = Tokenizer(num_words=MAX_TOKENS)\n",
        "        tokenizer.fit_on_texts(list(preprocessed_df))\n",
        "    tokenized_df = tokenize_and_pad(preprocessed_df, tokenizer, max_words_in_comment)\n",
        "    return tokenized_df, tokenizer\n",
        "\n",
        "\n",
        "def get_preprocessed_dfs(train_filepath, test_filepath, preprocessing_func, \n",
        "                         max_chars_in_comment, max_tokens, max_words_in_comment,\n",
        "                         return_tokenized=False):\n",
        "\n",
        "    preprocessing_tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
        "    preprocessed_train, train_labels = load_and_preprocess(train_filepath, \n",
        "                                                           preprocessing_func, \n",
        "                                                           preprocessing_tknzr, \n",
        "                                                           max_chars_in_comment,\n",
        "                                                           get_labels=True)        \n",
        "    preprocessed_test = load_and_preprocess(test_filepath, preprocessing_func, \n",
        "                                            preprocessing_tknzr, max_chars_in_comment)        \n",
        "        \n",
        "    if return_tokenized:\n",
        "        tokenized_train, tokenizer = preproc2tokenized(preprocessed_train, \n",
        "                                                       max_tokens=max_tokens, \n",
        "                                                       max_words_in_comment=max_words_in_comment, \n",
        "                                                       tokenizer=None)\n",
        "        tokenized_test, _ = preproc2tokenized(preprocessed_test, \n",
        "                                              max_tokens=max_tokens, \n",
        "                                              max_words_in_comment=max_words_in_comment, \n",
        "                                              tokenizer=tokenizer)\n",
        "        return tokenized_train, tokenized_test, train_labels\n",
        "    else:\n",
        "        return preprocessed_train, preprocessed_test, train_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn1-XTZEIZ3y",
        "colab_type": "text"
      },
      "source": [
        "## Loading frames and processing them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AWRdxZsHG3B",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "TEXT_COLUMN = 'comment_text'\n",
        "TARGET_COLS = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "\n",
        "MAX_WORDS_IN_COMMENT = 220\n",
        "MAX_TOKENS = 20000\n",
        "MAX_CHARS_IN_COMMENT = 20000 #We are going to truncate a comment if its length > threshold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z54MJ5i2i-UA",
        "colab_type": "code",
        "outputId": "e6401210-f507-48cd-fc01-49ea34857335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "preprocessed_train, preprocessed_test, train_labels = get_preprocessed_dfs(train_filepath, \n",
        "                                                                          test_filepath, \n",
        "                                                                          normalize_comment, \n",
        "                                                                          max_chars_in_comment=MAX_CHARS_IN_COMMENT, \n",
        "                                                                          max_tokens=MAX_TOKENS,\n",
        "                                                                          max_words_in_comment=MAX_WORDS_IN_COMMENT,\n",
        "                                                                          return_tokenized=False)\n",
        "gc.collect()\n",
        "\n",
        "print(preprocessed_train[:3])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gained shape: (159571,)\n",
            "Gained shape: (153164,)\n",
            "[\"explanation why edits made username hardcore metallica fan reverted ? they vandalisms , closure gas I voted new york dolls FAC . and please remove template talk page since i'm retired . .\"\n",
            " \"d'aww ! he matches background colour i'm seemingly stuck . thanks . talk , january , UTC\"\n",
            " \"hey man , i'm really trying edit war . it's guy constantly removing relevant information talking edits instead talk page . he seems care formatting actual info .\"]\n",
            "CPU times: user 1min 44s, sys: 5.59 s, total: 1min 50s\n",
            "Wall time: 1min 54s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8APgvUyskaf",
        "colab_type": "text"
      },
      "source": [
        "# Preparing features and training model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAROUFTRpHZI",
        "colab_type": "text"
      },
      "source": [
        "todo: refactor structure (embedding matrix + bigru/ transformer+fully-connected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCRY8qnNJTh_",
        "colab_type": "text"
      },
      "source": [
        "## Preparing features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jqSUK8Dpd3Q",
        "colab_type": "code",
        "outputId": "3d43d16b-2699-43e9-ebd9-ca298ab894c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "embedding_len = 1536\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' %len(word_index))\n",
        "\n",
        "#del tokenizer\n",
        "gc.collect()\n",
        "\n",
        "num_words = min(MAX_TOKENS, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_words, embedding_len)) # zeroth row for UNK\n",
        "print('embedding matrix shape', embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 184924 unique tokens.\n",
            "embedding matrix shape (20000, 1536)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRh7ig80jk3s",
        "colab_type": "text"
      },
      "source": [
        "### GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BNjZiXPZEYvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding_len = 200\n",
        "# words = pd.read_csv(os.path.join(embeddings_folder, \"glove.twitter.27B.{}d.txt\".format(embedding_len)), \n",
        "#                     sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
        "\n",
        "# embedding_len = 300\n",
        "# words = pd.read_csv(os.path.join(embeddings_folder, \"glove.840B.300d.txt\"), \n",
        "#                     sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
        "\n",
        "\n",
        "# vocab_words = words[words.index.isin(list(word_index.keys()))]\n",
        "# print('vocab_words.shape', vocab_words.shape)\n",
        "\n",
        "# del words\n",
        "# gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0-ZU5VYq-Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_vec(word, words_df):\n",
        "#     return words_df.loc[word].values\n",
        "\n",
        "\n",
        "\n",
        "# for word, i in word_index.items():\n",
        "#   if i >= max_tokens:\n",
        "#     continue\n",
        "#   try:\n",
        "#     embedding_vector = get_vec(word, vocab_words)\n",
        "#   except:\n",
        "#     continue\n",
        "#   if embedding_vector is not None:\n",
        "#     # words not found in embedding index will be all-zeros.\n",
        "#     embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# # %%time\n",
        "# # embedding_matrix = np.load('../input/embedding-2/embedding_matrix_big.npy')\n",
        "    \n",
        "# print('embedding_matrix.shape', embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_nWe7O-jqak",
        "colab_type": "text"
      },
      "source": [
        "### StarSpace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD8SO8xXfsVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Starspace attempt\n",
        "\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# count_vectorizer = CountVectorizer(min_df=0, max_df=0.99, max_features=10000)\n",
        "# X_train = count_vectorizer.fit_transform(article_contents.main_content.iloc[0:train_row])\n",
        "# X_train = count_vectorizer.inverse_transform(X_train)\n",
        "# label_prefix = '__label__'\n",
        "# with open(\"train_starspace.txt\", 'w+') as file:\n",
        "#     for i in range(10):\n",
        "#         file.write(' '.join(preprocessed_train[i].split(' ')) + ' ' + label_prefix + train_labels[i])\n",
        "#         file.write('\\n')\n",
        "# file.close()\n",
        "\n",
        "# The result file will look like this (all separeted by space, and label will have prefix __label__)\n",
        "# how are you ... __label__b\n",
        "# this is just an example ... __label__c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGzxJch3juGy",
        "colab_type": "text"
      },
      "source": [
        "### FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voep54c6Q-I-",
        "colab_type": "code",
        "outputId": "64afe0b8-56c3-40e5-87f5-7372096ffc2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (42.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.17.4)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBeehRN-oS9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import codecs\n",
        "\n",
        "# #load embeddings\n",
        "# print('loading word embeddings...')\n",
        "# embeddings_index = {}\n",
        "# f = codecs.open(os.path.join(embeddings_folder, 'crawl-300d-2M-subword.vec'), encoding='utf-8')\n",
        "# for line in tqdm(f, mininterval=5):\n",
        "#     values = line.rstrip().rsplit(' ')\n",
        "#     word = values[0]\n",
        "#     coefs = np.asarray(values[1:], dtype='float32')\n",
        "#     embeddings_index[word] = coefs\n",
        "# f.close()\n",
        "# print('found %s word vectors' % len(embeddings_index))\n",
        "# print('preparing embedding matrix...')\n",
        "# words_not_found = []\n",
        "\n",
        "\n",
        "# for word, i in word_index.items():\n",
        "#     if i >= max_tokens:\n",
        "#         continue\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "#         # words not found in embedding index will be all-zeros.\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "#     else:\n",
        "#         words_not_found.append(word)\n",
        "# print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "\n",
        "\n",
        "# words_not_found[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWmhQEFAj1VE",
        "colab_type": "text"
      },
      "source": [
        "### BPEmb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdMBN-HPjjek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install bpemb;\n",
        "# from bpemb import BPEmb\n",
        "# # load English BPEmb model with default vocabulary size of max_tokens\n",
        "# bpemb_en = BPEmb(lang=\"en\", dim=embedding_len, vs=max_tokens)\n",
        "# bpemb_en.vectors.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JQn_7WjrNP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # mean vector of word parts\n",
        "# def get_vec(word):\n",
        "#     vec = bpemb_en.embed(word)\n",
        "#     return vec.mean(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSAJK0AYsdW2",
        "colab_type": "code",
        "trusted": true,
        "outputId": "bd9383ce-edd7-4c2f-b258-1dea1d46675c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# %%time\n",
        "\n",
        "# for word, i in word_index.items():\n",
        "#   if i >= max_tokens:\n",
        "#     continue\n",
        "#   try:\n",
        "#     embedding_vector = get_vec(word)\n",
        "#   except:\n",
        "#     continue\n",
        "#   if embedding_vector is not None:\n",
        "#     # words not found in embedding index will be all-zeros.\n",
        "#     embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# print('embedding_matrix.shape', embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding_matrix.shape (20000, 300)\n",
            "CPU times: user 504 ms, sys: 18.1 ms, total: 522 ms\n",
            "Wall time: 520 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2ezEW492Nnj",
        "colab_type": "text"
      },
      "source": [
        "### Flair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScKTCXs-trr_",
        "colab_type": "code",
        "outputId": "01af3e73-bbbf-4cbf-9253-3a2dd3befe54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! pip install tiny-tokenizer\n",
        "! pip install flair\n",
        "\n",
        "import flair\n",
        "from flair.data import Sentence\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tiny-tokenizer\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/0f/aa52c227c5af69914be05723b3deaf221805a4ccbce87643194ef2cdde43/tiny_tokenizer-3.1.0.tar.gz\n",
            "Building wheels for collected packages: tiny-tokenizer\n",
            "  Building wheel for tiny-tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tiny-tokenizer: filename=tiny_tokenizer-3.1.0-cp36-none-any.whl size=10550 sha256=aa15269699b787314d1c790861bba985df9096ce9ad75c95d32baf09b8b5ec6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/c8/36/334497a689fab90128232e86b5829b800dd271a3d5d5959c53\n",
            "Successfully built tiny-tokenizer\n",
            "Installing collected packages: tiny-tokenizer\n",
            "Successfully installed tiny-tokenizer-3.1.0\n",
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/22/8fc8e5978ec05b710216735ca47415700e83f304dec7e4281d61cefb6831/flair-0.4.4-py3-none-any.whl (193kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 2.8MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.2.9\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Collecting transformers>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 48.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 51.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from flair) (3.10.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.6)\n",
            "Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.2)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from flair) (0.2.0)\n",
            "Requirement already satisfied: tiny-tokenizer[all] in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.0)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/89/62912e01f3cede11edcc0abf81298e3439d9c06c8dce644369380ed13f6d/Deprecated-1.2.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.3.1)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from flair) (0.4.2)\n",
            "Collecting ipython==7.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/2c/c7d44277b599df35af734d8f4142d501192fdb7aef5d04daf882d7eccfbc/ipython-7.6.1-py3-none-any.whl (774kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 35.3MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 43.1MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (1.17.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (1.10.47)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.22.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->flair) (1.12.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (42.0.2)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (8.0.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
            "Collecting SudachiDict-core@ https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_core-20190927.tar.gz ; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_core-20190927.tar.gz (70.7MB)\n",
            "\u001b[K     |████████████████████████████████| 70.7MB 44kB/s \n",
            "\u001b[?25hCollecting SudachiPy; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/c9/40bfb291a7995ad218451ef97083432f998b822e3ecbd9f586f593d2cfb6/SudachiPy-0.4.2-py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.2MB/s \n",
            "\u001b[?25hCollecting janome; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/f0/bd7f90806132d7d9d642d418bdc3e870cfdff5947254ea3cab27480983a7/Janome-0.3.10-py2.py3-none-any.whl (21.5MB)\n",
            "\u001b[K     |████████████████████████████████| 21.5MB 158kB/s \n",
            "\u001b[?25hCollecting natto-py; extra == \"all\"\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/14/1d4258247a00b7b8a115563effb1d0bd30501d69580629d36593ce0af92d/natto-py-0.9.2.tar.gz\n",
            "Collecting kytea; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/bc/702d01a96d5d094bd9f3c2eb1d12153daf8edf7bf5d78b9a2dae1202df07/kytea-0.1.4-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 30.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.2)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.9.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->flair) (6.2.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.4.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.1.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (2.1.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.7.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.7.5)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/61/2dfea88583d5454e3a64f9308a686071d58d59a55db638268a6413e1eb6d/prompt_toolkit-2.0.10-py3-none-any.whl (340kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 44.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.15.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.3.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (1.13.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (0.14.1)\n",
            "Collecting dartsclone~=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/4d/45acbe9d0795d8ceef0fee1f9ac2dcbf27dca3a0578a023fcdc3fef6fd89/dartsclone-0.6.tar.gz\n",
            "Requirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (2.1.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (1.13.2)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython==7.6.1->flair) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.6.1->flair) (0.1.8)\n",
            "Requirement already satisfied: parso>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython==7.6.1->flair) (0.5.2)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers>=2.0.0->flair) (0.15.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.6.0->SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (0.29.14)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (2.19)\n",
            "Building wheels for collected packages: langdetect, segtok, sqlitedict, mpld3, sacremoses, SudachiDict-core, natto-py, dartsclone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-cp36-none-any.whl size=993460 sha256=65bda05c34cc4f7d2803f6893027a63e2645e041b68e14522d5c50dedfda6109\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.7-cp36-none-any.whl size=23258 sha256=a6068dca9b07d0d23c5b2578cd91ef376e73852632a166e378c7b110177d5e19\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=db59b3364116b169a671b96c7c5deb471b6d402d4b0645175eef90fb39422521\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=787a055e5a4ff87af3e8b3181f09441fbba207b4b392baa00e0a0b7d410c92ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=a9bc6ff7947fee6ea9cb980e90406dbce3648c3047162a4fd6e55e8e9c6fca29\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "  Building wheel for SudachiDict-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SudachiDict-core: filename=SudachiDict_core-20190927-cp36-none-any.whl size=70878518 sha256=928042bfce672ea42c8ee0a4c92767c51249df8109d969ad92eeb9979adfd8fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/d8/6e/b107d7fef6e80915aa1e46db741b98a3da011f567526347ccc\n",
            "  Building wheel for natto-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for natto-py: filename=natto_py-0.9.2-cp36-none-any.whl size=45164 sha256=b575b15b9aef2c24bc8e25450742d03fcbf00a323ecc3ac97ccd477615c70910\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/51/dd/67f87608b124a23eecf5c1fc3557cc0b7ffdeae33fe6ee89df\n",
            "  Building wheel for dartsclone (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dartsclone: filename=dartsclone-0.6-cp36-cp36m-linux_x86_64.whl size=413251 sha256=4b52a1db155cf8b637be04577552e255282764ba767ae7c523150b48026fbe49\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/cd/70/fe43307bf7398243155108f4f5a258ef336923d65ec4af93cd\n",
            "Successfully built langdetect segtok sqlitedict mpld3 sacremoses SudachiDict-core natto-py dartsclone\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.6.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, bpemb, sacremoses, transformers, langdetect, segtok, deprecated, sqlitedict, prompt-toolkit, ipython, mpld3, flair, dartsclone, SudachiPy, SudachiDict-core, janome, natto-py, kytea\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "Successfully installed SudachiDict-core-20190927 SudachiPy-0.4.2 bpemb-0.3.0 dartsclone-0.6 deprecated-1.2.7 flair-0.4.4 ipython-7.6.1 janome-0.3.10 kytea-0.1.4 langdetect-1.0.7 mpld3-0.3 natto-py-0.9.2 prompt-toolkit-2.0.10 sacremoses-0.0.38 segtok-1.5.7 sentencepiece-0.1.85 sqlitedict-1.6.0 transformers-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yDXCBmqtkPl",
        "colab_type": "text"
      },
      "source": [
        "#### ELMo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVZ4xx34vSmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNDAZuOvt4wR",
        "colab_type": "code",
        "outputId": "306e7fb8-db28-4628-ea6e-c06cc0ae1183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from flair.embeddings import ELMoEmbeddings\n",
        "\n",
        "# init embedding\n",
        "embedding = ELMoEmbeddings('medium')\n",
        "\n",
        "# create a sentence #\n",
        "sentence = Sentence('test sent')\n",
        "# embed words in sentence #\n",
        "embedding.embed(sentence)\n",
        "embedding_len = sentence[0].embedding.size()[0]\n",
        "print('embedding len', embedding_len)\n",
        "\n",
        "\n",
        "def get_comment_emb(comment):\n",
        "  sentence = Sentence(comment)\n",
        "  embedding.embed(sentence)\n",
        "  emb_list = []\n",
        "  for word in sentence:\n",
        "    emb_list.append(word.embedding)\n",
        "  return emb_list\n",
        "\n",
        "get_comment_emb_vec = np.vectorize(get_comment_emb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding len 1536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFD8Xd565HgQ",
        "colab_type": "code",
        "outputId": "aaea15ab-6a6f-4f01-c47f-7d111ebd707c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for word, i in tqdm(word_index.items(), total=len(word_index)):\n",
        "    if i >= max_tokens:\n",
        "        continue\n",
        "    try:\n",
        "        word_sent = Sentence(word)\n",
        "        embedding.embed(word_sent)\n",
        "        embedding_vector = word_sent[0].embedding.cpu().detach().numpy()\n",
        "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "           embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        pass\n",
        "        #embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 184924/184924 [03:02<00:00, 1014.44it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6tfsIpFJ6YG",
        "colab_type": "code",
        "outputId": "c515518f-a7ce-4831-cf0e-713070b8baf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "del embedding\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I7VZVVF69Fh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # creating a tensor for storing sentence embeddings #\n",
        "# s = torch.zeros(0, embedding_len)\n",
        "\n",
        "# # iterating Sentence (tqdm tracks progress) #\n",
        "# for tweet in tqdm(preprocessed_train[10]):   \n",
        "#   # empty tensor for words #\n",
        "#   w = torch.zeros(0, embedding_len)   \n",
        "#   sentence = Sentence(tweet)\n",
        "#   embedding.embed(sentence)\n",
        "#   # for every word #\n",
        "#   for token in sentence:\n",
        "#     # storing word Embeddings of each word in a sentence #\n",
        "#     w = torch.cat((w,token.embedding.view(-1, embedding_len)),0)\n",
        "#   # storing sentence Embeddings (mean of embeddings of all words)   #\n",
        "#   s = torch.cat((s, w.mean(dim = 0).view(-1, embedding_len)),0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hTlPvNY8gi1",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gEnMtmwooBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.embeddings import RoBERTaEmbeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvXWeyYWo0J9",
        "colab_type": "code",
        "outputId": "0c27d2a0-5e69-4428-fed4-20262dc73bc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# init embedding\n",
        "embedding = RoBERTaEmbeddings('roberta-base')\n",
        "\n",
        "# create a sentence #\n",
        "sentence = Sentence('test sent')\n",
        "# embed words in sentence #\n",
        "embedding.embed(sentence)\n",
        "embedding_len = sentence[0].embedding.size()[0]\n",
        "print('embedding len', embedding_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding len 768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDdLhAjJqY-G",
        "colab_type": "code",
        "outputId": "5cf088ff-3fd9-4e44-f690-e37eb45c37db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for word, i in tqdm(word_index.items(), total=len(word_index)):\n",
        "    if i >= max_tokens:\n",
        "        continue\n",
        "    try:\n",
        "        word_sent = Sentence(word)\n",
        "        embedding.embed(word_sent)\n",
        "        embedding_vector = word_sent[0].embedding.cpu().detach().numpy()\n",
        "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "           embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        pass\n",
        "        #embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "del embedding\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 184924/184924 [04:29<00:00, 685.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McJb1Sn5tw4a",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa + ELMo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztnGoOlrt3PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install allennlp\n",
        "from flair.embeddings import StackedEmbeddings, RoBERTaEmbeddings, ELMoEmbeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_4YEiP1t5-n",
        "colab_type": "code",
        "outputId": "7009dff1-41c1-45be-b19b-bae803e40e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "elmo_embeddings = ELMoEmbeddings('small')\n",
        "roberta_embeddings = RoBERTaEmbeddings('roberta-base')\n",
        "stacked_embeddings = StackedEmbeddings([\n",
        "                                        elmo_embeddings,\n",
        "                                        roberta_embeddings,\n",
        "                                       ])\n",
        "\n",
        "# create a sentence #\n",
        "sentence = Sentence('test sent')\n",
        "# embed words in sentence #\n",
        "stacked_embeddings.embed(sentence)\n",
        "embedding_len = sentence[0].embedding.size()[0]\n",
        "print('embedding len', embedding_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 336/336 [00:00<00:00, 742511.14B/s]\n",
            "100%|██████████| 54402456/54402456 [00:02<00:00, 20212502.71B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "embedding len 1536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJNcWzMduAtb",
        "colab_type": "code",
        "outputId": "c878817b-90ae-40ce-a383-cd2934362b5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for word, i in tqdm(word_index.items(), total=len(word_index)):\n",
        "    if i >= MAX_TOKENS:\n",
        "        continue\n",
        "    try:\n",
        "        word_sent = Sentence(word)\n",
        "        stacked_embeddings.embed(word_sent)\n",
        "        embedding_vector = word_sent[0].embedding.cpu().detach().numpy()\n",
        "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "           embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        pass\n",
        "        #embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 184924/184924 [11:48<00:00, 260.89it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMl76miAyRyI",
        "colab_type": "code",
        "outputId": "3f1eae01-4bf3-4e00-ec66-7ec4bf4c2c87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "del stacked_embeddings\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "549"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyxmTYb4Q2dp",
        "colab_type": "text"
      },
      "source": [
        "### BERT fine-tuned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE-uRZUmQ4d9",
        "colab_type": "code",
        "outputId": "d0e12156-3992-489b-a3e9-b825b94bbb6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "print(hub.__version__)\n",
        "\n",
        "#Installing BERT module\n",
        "!pip install bert-tensorflow\n",
        "\n",
        "#Importing BERT modules\n",
        "import bert\n",
        "from bert import run_classifier, optimization, tokenization"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7.0\n",
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMoU4oQJW8yi",
        "colab_type": "code",
        "outputId": "ea05b710-ba7c-4966-a974-9a50b6343bff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "preprocessed_train_with_labels = pd.concat([pd.DataFrame(preprocessed_train, columns=['preprocessed_text']),\n",
        "                                            pd.DataFrame(train_labels, columns=TARGET_COLS)], axis=1)\n",
        "preprocessed_train_with_labels.head(2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessed_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>explanation why edits made username hardcore m...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>d'aww ! he matches background colour i'm seemi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   preprocessed_text  ...  identity_hate\n",
              "0  explanation why edits made username hardcore m...  ...              0\n",
              "1  d'aww ! he matches background colour i'm seemi...  ...              0\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTP1fY-25QaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_examples(df, labels_available=True):\n",
        "    \"\"\"\n",
        "    Creates input examples for the sets of texts and labels.\n",
        "    https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/run_classifier.py#L127    \n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    for (i, row) in enumerate(df.values):\n",
        "      guid = None #row[0]\n",
        "      text_a = row[0]\n",
        "      if labels_available:\n",
        "        labels = row[1:].tolist()\n",
        "      else:\n",
        "        labels = [0, 0, 0, 0, 0, 0]\n",
        "      examples.append(bert.run_classifier.InputExample(guid=guid, text_a = text_a, \n",
        "                                                       label = labels))\n",
        "    return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODyQzlDIpoF2",
        "colab_type": "code",
        "outputId": "1fe15319-e49f-4d4b-888a-d74e730a45eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%time\n",
        "\n",
        "preprocessed_train_with_labels, preprocessed_valid_with_labels, train_labels, valid_labels = train_test_split(preprocessed_train_with_labels, train_labels, test_size = 0.1, shuffle=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.96 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vJlMO4277_z",
        "colab_type": "code",
        "outputId": "66cd5fcb-67e4-4b1b-bac4-6acf1a72c9d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "train_input_examples = create_examples(preprocessed_train_with_labels, labels_available=True)\n",
        "valid_input_examples = create_examples(preprocessed_valid_with_labels, labels_available=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 892 ms, sys: 23 ms, total: 915 ms\n",
            "Wall time: 915 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMiU0RrN77JS",
        "colab_type": "code",
        "outputId": "8659eec7-9ab1-4ddd-b90a-fd67fa6eb595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module(bert_path):\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    bert_module =  hub.Module(bert_path)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    vocab_file, do_lower_case = sess.run(\n",
        "        [\n",
        "            tokenization_info[\"vocab_file\"],\n",
        "            tokenization_info[\"do_lower_case\"],\n",
        "        ]\n",
        "    )\n",
        "    print('loading tokenizer with{} lowercasing'.format('out'*(not do_lower_case)))\n",
        "\n",
        "    return bert.tokenization.FullTokenizer(\n",
        "        vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "    \n",
        "bert_tokenizer = create_tokenizer_from_hub_module(BERT_MODEL_HUB)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loading tokenizer with lowercasing\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhlik1IGTLfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#todo: split to funct like here https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22\n",
        "class InputFeatures(object):\n",
        "    \"\"\"\n",
        "    A single set of features of data.\n",
        "    https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/run_classifier.py#L161\n",
        "    The only difference from the original implementation (line above) is label_ids instead of label_id because\n",
        "    here we solve multi-label not multi-class task.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_ids, is_real_example=True):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_ids = label_ids,\n",
        "        self.is_real_example=is_real_example\n",
        "\n",
        "def convert_examples_to_features(examples,  max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in tqdm(enumerate(examples), total=len(examples)):\n",
        "        \n",
        "        #print(example.text_a)\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids: 0   0   0   0  0     0 0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "\n",
        "        \"\"\"\n",
        "        # in original repo\n",
        "        tokens = []\n",
        "        segment_ids = []\n",
        "        tokens.append(\"[CLS]\")\n",
        "        segment_ids.append(0)\n",
        "        for token in tokens_a:\n",
        "            tokens.append(token)\n",
        "            segment_ids.append(0)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(0)\n",
        "        \"\"\"\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        \"\"\"\n",
        "        # in original repo\n",
        "        if tokens_b:\n",
        "        for token in tokens_b:\n",
        "            tokens.append(token)\n",
        "            segment_ids.append(1)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(1)\n",
        "        \"\"\"\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [\"[SEP]\"]\n",
        "            segment_ids += [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        '''\n",
        "        # in original repo\n",
        "        while len(input_ids) < max_seq_length:\n",
        "          input_ids.append(0)\n",
        "          input_mask.append(0)\n",
        "          segment_ids.append(0)\n",
        "        '''\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "        \n",
        "        labels_ids = []\n",
        "        for label in example.label:\n",
        "            labels_ids.append(int(label))\n",
        "\n",
        "        if ex_index < 2: # make as param with default value\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                    [str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\n",
        "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"label: %s (id = %s)\" % (example.label, labels_ids))\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              label_ids=labels_ids))\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS5SmpT1_r_M",
        "colab_type": "code",
        "outputId": "dc51741c-4bc7-4fa5-b9e1-c6f815ed5c23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "MAX_SEQ_LENGTH = MAX_WORDS_IN_COMMENT\n",
        "\n",
        "# Convert our train and validation features to InputFeatures that BERT understands.\n",
        "train_features = convert_examples_to_features(train_input_examples, MAX_SEQ_LENGTH, bert_tokenizer)\n",
        "\n",
        "valid_features = convert_examples_to_features(valid_input_examples, MAX_SEQ_LENGTH, bert_tokenizer)\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/143613 [00:00<?, ?it/s]INFO:__main__:*** Example ***\n",
            "INFO:__main__:guid: None\n",
            "INFO:__main__:tokens: [CLS] problems i sant ##e kim ##es article changes i wanting document issues i edit ##s sant ##e kim ##es article . all content here ##in refer said article . moments ago i edited lead content article . my reason lead rep ##eti ##tious . as i wrote editing note , state rep ##eti ##tious lead - lead info covered article order . previous lead white ##washed - kim ##es best known murders locations - this correction neutral factual i believe change i made neutral point view supported referenced links within said article . the change i made previously posted administrator i apparently trouble . i hoped work administrator constructive effort improve said article . however , appears said administrator making changes said article spite personal feelings article ' s subject . i mentioned administrator said article ' s discussion page problem articles subject , maybe another administrator could assist creating neutral point view article . on said article ' s discussion page , several biased statements said administrator article ' s subject . and , i feel , simply i would like said article ' balanced ' neutral . i wanting document issue problem later , documented talk page . note i naming administrator favor even though i doubt attempted favor appreciated . furthermore , problem later , [SEP]\n",
            "INFO:__main__:input_ids: 101 3471 1045 15548 2063 5035 2229 3720 3431 1045 5782 6254 3314 1045 10086 2015 15548 2063 5035 2229 3720 1012 2035 4180 2182 2378 6523 2056 3720 1012 5312 3283 1045 5493 2599 4180 3720 1012 2026 3114 2599 16360 20624 20771 1012 2004 1045 2626 9260 3602 1010 2110 16360 20624 20771 2599 1011 2599 18558 3139 3720 2344 1012 3025 2599 2317 27283 1011 5035 2229 2190 2124 9916 5269 1011 2023 18140 8699 25854 1045 2903 2689 1045 2081 8699 2391 3193 3569 14964 6971 2306 2056 3720 1012 1996 2689 1045 2081 3130 6866 8911 1045 4593 4390 1012 1045 5113 2147 8911 26157 3947 5335 2056 3720 1012 2174 1010 3544 2056 8911 2437 3431 2056 3720 8741 3167 5346 3720 1005 1055 3395 1012 1045 3855 8911 2056 3720 1005 1055 6594 3931 3291 4790 3395 1010 2672 2178 8911 2071 6509 4526 8699 2391 3193 3720 1012 2006 2056 3720 1005 1055 6594 3931 1010 2195 25352 8635 2056 8911 3720 1005 1055 3395 1012 1998 1010 1045 2514 1010 3432 1045 2052 2066 2056 3720 1005 12042 1005 8699 1012 1045 5782 6254 3277 3291 2101 1010 8832 2831 3931 1012 3602 1045 10324 8911 5684 2130 2295 1045 4797 4692 5684 12315 1012 7297 1010 3291 2101 1010 102\n",
            "INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:label: [0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0])\n",
            "INFO:__main__:*** Example ***\n",
            "INFO:__main__:guid: None\n",
            "INFO:__main__:tokens: [CLS] please stop disrupt ##ive editing . if continue van ##dal ##ize wikipedia , shirley bass ##ey , blocked editing . it ' s lead , meant quick summer ##y articles contents . also , obviously using one ip , reason keep changing based po ##v agenda . if shared ip address , make edit , consider creating account avoid irrelevant notices . [SEP]\n",
            "INFO:__main__:input_ids: 101 3531 2644 23217 3512 9260 1012 2065 3613 3158 9305 4697 16948 1010 11280 3321 3240 1010 8534 9260 1012 2009 1005 1055 2599 1010 3214 4248 2621 2100 4790 8417 1012 2036 1010 5525 2478 2028 12997 1010 3114 2562 5278 2241 13433 2615 11376 1012 2065 4207 12997 4769 1010 2191 10086 1010 5136 4526 4070 4468 22537 14444 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:label: [0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0])\n",
            "100%|██████████| 143613/143613 [01:53<00:00, 1268.48it/s]\n",
            "  0%|          | 0/15958 [00:00<?, ?it/s]INFO:__main__:*** Example ***\n",
            "INFO:__main__:guid: None\n",
            "INFO:__main__:tokens: [CLS] welcome ! we can ' t say loud big enough ! here links might find helpful be bold ! don ' t let gr ##ump ##y users scare . meet new users learn others play nice others contribute , contribute , contribute ! tell us you sign name talk pages votes typing software automatically converts user ##name date . if questions problems , matter , leave message talk page . we ' re glad ! talk ' ' ' [SEP]\n",
            "INFO:__main__:input_ids: 101 6160 999 2057 2064 1005 1056 2360 5189 2502 2438 999 2182 6971 2453 2424 14044 2022 7782 999 2123 1005 1056 2292 24665 24237 2100 5198 12665 1012 3113 2047 5198 4553 2500 2377 3835 2500 9002 1010 9002 1010 9002 999 2425 2149 2017 3696 2171 2831 5530 4494 22868 4007 8073 19884 5310 18442 3058 1012 2065 3980 3471 1010 3043 1010 2681 4471 2831 3931 1012 2057 1005 2128 5580 999 2831 1005 1005 1005 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:label: [0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0])\n",
            "INFO:__main__:*** Example ***\n",
            "INFO:__main__:guid: None\n",
            "INFO:__main__:tokens: [CLS] ant ##ago ##nism keep , . keep following around wikipedia th ##wart ##ing edit ##s . does make quake pleasure , i wonder ? was i put panties bind ? in case i ' m talking speedy del ##ete ph ##anta ##sy star iv image , uploaded violation w ##p policy , purpose improving quality article . i believe actions constitute abuse power . your little e - cock stroking edit - block days ago lost , i away vacation . i hope idea really got , though . but yeah , keep ant ##ago ##ni ##zing , see far takes . we ' ll see higher - ups regard behavior , including warning civil ##ity including lack thereof . e . calling dick . the ce ##nsor ##ed word , case wondering , two g ' s , ends t , begins f , ex ##ple ##tive wrong ##fully targeting homosexual ##s rightful ##ly targeting po ##s wikipedia editors like . - - ' ' ' t c w [SEP]\n",
            "INFO:__main__:input_ids: 101 14405 23692 28113 2562 1010 1012 2562 2206 2105 16948 16215 18367 2075 10086 2015 1012 2515 2191 27785 5165 1010 1045 4687 1029 2001 1045 2404 12095 14187 1029 1999 2553 1045 1005 1049 3331 26203 3972 12870 6887 26802 6508 2732 4921 3746 1010 21345 11371 1059 2361 3343 1010 3800 9229 3737 3720 1012 1045 2903 4506 12346 6905 2373 1012 2115 2210 1041 1011 10338 14943 10086 1011 3796 2420 3283 2439 1010 1045 2185 10885 1012 1045 3246 2801 2428 2288 1010 2295 1012 2021 3398 1010 2562 14405 23692 3490 6774 1010 2156 2521 3138 1012 2057 1005 2222 2156 3020 1011 11139 7634 5248 1010 2164 5432 2942 3012 2164 3768 21739 1012 1041 1012 4214 5980 1012 1996 8292 29577 2098 2773 1010 2553 6603 1010 2048 1043 1005 1055 1010 4515 1056 1010 4269 1042 1010 4654 10814 6024 3308 7699 14126 15667 2015 27167 2135 14126 13433 2015 16948 10195 2066 1012 1011 1011 1005 1005 1005 1056 1039 1059 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:label: [1, 0, 0, 0, 0, 0] (id = [1, 0, 0, 0, 0, 0])\n",
            "100%|██████████| 15958/15958 [00:13<00:00, 1196.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 7s, sys: 927 ms, total: 2min 8s\n",
            "Wall time: 2min 7s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXl9kZi-wuXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features_to_arrays(features):\n",
        "    \"\"\"Convert a list of InputFeatures to np.arrays\"\"\"\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_input_mask = []\n",
        "    all_segment_ids = []\n",
        "\n",
        "    for feature in features:\n",
        "        all_input_ids.append(feature.input_ids)\n",
        "        all_input_mask.append(feature.input_mask)\n",
        "        all_segment_ids.append(feature.segment_ids)\n",
        "\n",
        "    return (np.array(all_input_ids, dtype='int32'), \n",
        "            np.array(all_input_mask, dtype='int32'), \n",
        "            np.array(all_segment_ids, dtype='int32'))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UVNm-i9wxVW",
        "colab_type": "code",
        "outputId": "81208345-5055-4891-cb5a-531b9d0a946b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time \n",
        "\n",
        "train_input_ids, train_input_masks, train_segment_ids = features_to_arrays(train_features)\n",
        "\n",
        "valid_input_ids, valid_input_masks, valid_segment_ids = features_to_arrays(valid_features)\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 8.76 s, sys: 93.8 ms, total: 8.85 s\n",
            "Wall time: 8.83 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF8fnd_OMGPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, bert_path, verbose=True, n_fine_tune_layers=10, **kwargs):\n",
        "        self.bert_path = bert_path\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        # parse it with regular expression\n",
        "        self.output_size = 768\n",
        "        self.verbose = verbose\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(\n",
        "            self.bert_path,\n",
        "            trainable=self.trainable,\n",
        "            name=\"{}_module\".format(self.name)\n",
        "        )\n",
        "        trainable_vars = self.bert.variables\n",
        "        \n",
        "        # Remove unused layers\n",
        "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
        "        \n",
        "        # Select how many layers to fine tune\n",
        "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
        "        \n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "        \n",
        "        # Add non-trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"*** TRAINABLE VARS *** \")\n",
        "            for var in self._trainable_weights:\n",
        "                print(var)\n",
        "          \n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        #inputs = [inputs.input_ids, inputs.input_mask, inputs.segment_ids]\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "            \"pooled_output\"\n",
        "        ]\n",
        "\n",
        "        \"\"\"\n",
        "        output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "        \n",
        "        if self.pooling == \"cls\":\n",
        "            pooled = output[\"pooled_output\"]\n",
        "        else:\n",
        "            result = output[\"sequence_output\"]\n",
        "            \n",
        "            input_mask = tf.cast(input_mask, tf.float32)\n",
        "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "            \n",
        "            if self.pooling == \"mean\":\n",
        "              pooled = masked_reduce_mean(result, input_mask)\n",
        "            else:\n",
        "              pooled = mul_mask(result, input_mask)\n",
        "\n",
        "        return pooled\n",
        "        \"\"\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_size)\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'bert_path': bert_path, \n",
        "            'verbose': verbose, \n",
        "            'n_fine_tune_layers': n_fine_tune_layers,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# can take more fully model from https://github.com/strongio/keras-bert/blob/master/keras-bert.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D8BuKRaMmiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build model\n",
        "def build_model(max_seq_length): \n",
        "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
        "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
        "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
        "    bert_inputs = [in_id, in_mask, in_segment]\n",
        "    \n",
        "    # Instantiate the custom Bert Layer defined above\n",
        "    bert_output = BertLayer(BERT_MODEL_HUB, verbose=True, n_fine_tune_layers=3)(bert_inputs)\n",
        "\n",
        "    # Build the rest of the classifier \n",
        "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
        "    pred = tf.keras.layers.Dense(len(TARGET_COLS), activation='sigmoid')(dense)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def initialize_vars(sess):\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-XaniY05keu",
        "colab_type": "text"
      },
      "source": [
        "TODO: add early stopping and evaluating callbacks (start from NN callback)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiH9J6v7IsZV",
        "colab_type": "code",
        "outputId": "68f025d5-cd3f-4075-cc29-9875693a5586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "model = build_model(MAX_SEQ_LENGTH)\n",
        "\n",
        "# Instantiate variables\n",
        "initialize_vars(sess)\n",
        "\n",
        "model.fit(\n",
        "        [train_input_ids, train_input_masks, train_segment_ids], \n",
        "        train_labels,\n",
        "        validation_data=([valid_input_ids, valid_input_masks, valid_segment_ids], valid_labels),\n",
        "        epochs=1,\n",
        "        batch_size=32\n",
        "    )"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** TRAINABLE VARS *** \n",
            "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "<tf.Variable 'bert_layer_module/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>\n",
            "<tf.Variable 'bert_layer_module/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 220)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 220)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 220)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer (BertLayer)          (None, 768)          110104890   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 6)            1542        dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 110,303,296\n",
            "Trainable params: 3,148,294\n",
            "Non-trainable params: 107,155,002\n",
            "__________________________________________________________________________________________________\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "143613/143613 [==============================] - 1573s 11ms/sample - loss: 0.0527 - acc: 0.9806 - val_loss: 0.0468 - val_acc: 0.9823\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa4db5efa58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imo2XJzmTxY7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "11a24f5e-73e9-4fff-8233-9ee2b22a4125"
      },
      "source": [
        "model.save(output_folder+'/BertModel_multilabel.h5')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-d4eb5aef74eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/BertModel_multilabel.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \"\"\"\n\u001b[1;32m   1170\u001b[0m     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1171\u001b[0;31m                       signatures)\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures)\u001b[0m\n\u001b[1;32m    107\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    108\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 109\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    110\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mmodel_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[0;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[1;32m    158\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m   metadata = dict(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[0;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# From the earliest layers on.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m       \u001b[0mlayer_class_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m       \u001b[0mlayer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0mfiltered_inbound_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;31m# or that `get_config` has been overridden:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_is_default'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       raise NotImplementedError('Layers with arguments in `__init__` must '\n\u001b[0m\u001b[1;32m    581\u001b[0m                                 'override `get_config`.')\n\u001b[1;32m    582\u001b[0m     \u001b[0;31m# TODO(reedwm): Handle serializing self._dtype_policy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Layers with arguments in `__init__` must override `get_config`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmerekHYnn7U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "6098a160-1739-4a19-ba6a-e81c7d42cfb9"
      },
      "source": [
        "preprocessed_test"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['you bitch ja rule succesful ever whats hating sad mofuckas bitch slap your pethedic white faces get kiss ass guys sicken . ja rule pride da music man . dont diss shit . nothin wrong bein like tupac brother fuckin white boys get things right next time . ,',\n",
              "       'from rfc the title fine , IMO .',\n",
              "       'sources zawe ashton lapland - -', ...,\n",
              "       \"okinotorishima categories I see changes agree correct . I gotten confused , found acknowledging japan's territorial rights okinotorishima however , category acknowledge japan's claim exclusive economic zone EEZ stemming okinotorishima . that , category disputed EEZ ?\",\n",
              "       \"one founding nations EU - germany - law return quite similar israel's this actually true , ? germany allows people whose ancestors citizens germany return , AFAIK allow descendants anglo-saxons return angeln saxony . israel , contrast , allows jews return israel , even can't trace particular ancestral line anyone lived modern state even mandate palestine . - -\",\n",
              "       \"stop already . your bullshit welcome . i'm fool , think kind explination enough , well pity .\"],\n",
              "      dtype='<U9950')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXvo1jg0nbvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "ea3609ef-421a-4ba2-e011-cfaad7b43c25"
      },
      "source": [
        "%%time\n",
        "\n",
        "preprocessed_test_df = pd.DataFrame(preprocessed_test, columns=['preprocessed_text'])\n",
        "test_input_examples = create_examples(preprocessed_test_df, labels_available=False)\n",
        "test_features = convert_examples_to_features(test_input_examples, MAX_SEQ_LENGTH, bert_tokenizer)\n",
        "test_input_ids, test_input_masks, test_segment_ids = features_to_arrays(test_features)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/153164 [00:00<?, ?it/s]INFO:__main__:*** Example ***\n",
            "INFO:__main__:guid: None\n",
            "INFO:__main__:tokens: [CLS] you bitch ja rule su ##cc ##es ##ful ever what ##s hating sad mo ##fu ##ck ##as bitch slap your pet ##hed ##ic white faces get kiss ass guys sick ##en . ja rule pride da music man . don ##t di ##ss shit . nothin wrong bei ##n like tu ##pac brother fuck ##in white boys get things right next time . , [SEP]\n",
            "INFO:__main__:input_ids: 101 2017 7743 14855 3627 10514 9468 2229 3993 2412 2054 2015 22650 6517 9587 11263 3600 3022 7743 14308 2115 9004 9072 2594 2317 5344 2131 3610 4632 4364 5305 2368 1012 14855 3627 6620 4830 2189 2158 1012 2123 2102 4487 4757 4485 1012 24218 3308 21388 2078 2066 10722 19498 2567 6616 2378 2317 3337 2131 2477 2157 2279 2051 1012 1010 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:label: [0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0])\n",
            "INFO:__main__:*** Example ***\n",
            "INFO:__main__:guid: None\n",
            "INFO:__main__:tokens: [CLS] from rfc the title fine , im ##o . [SEP]\n",
            "INFO:__main__:input_ids: 101 2013 14645 1996 2516 2986 1010 10047 2080 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:__main__:label: [0, 0, 0, 0, 0, 0] (id = [0, 0, 0, 0, 0, 0])\n",
            "100%|██████████| 153164/153164 [01:55<00:00, 1330.79it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 6s, sys: 0 ns, total: 2min 6s\n",
            "Wall time: 2min 5s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odnkbt-ZnIj6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bb13c9ec-c804-46ec-a1b5-44f07318ef5f"
      },
      "source": [
        "%%time\n",
        "\n",
        "predictions  = model.predict([test_input_ids, test_input_masks, test_segment_ids])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 13min 47s, sys: 9min 10s, total: 22min 58s\n",
            "Wall time: 20min 47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMNL02XltYxZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5da95c1e-dc3d-4ac9-c86a-649153b13027"
      },
      "source": [
        "pred = predictions\n",
        "pred.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(153164, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AslnHsTy0gCQ",
        "colab_type": "text"
      },
      "source": [
        "## Build and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m6GDvsd3WRX",
        "colab_type": "code",
        "trusted": true,
        "outputId": "fdac63b2-b009-4c48-a5dd-127d8436cdfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "x_train, x_valid, y_train, y_valid = train_test_split(tokenized_train, \n",
        "                                                      train_labels, \n",
        "                                                      test_size = 0.1,\n",
        "                                                      shuffle=True)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_valid shape:', x_valid.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_valid shape:', y_valid.shape)\n",
        "\n",
        "del tokenized_train\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (143613, 220)\n",
            "x_valid shape: (15958, 220)\n",
            "y_train shape: (143613, 6)\n",
            "y_valid shape: (15958, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6306i8ha3TBF",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def build_model(embedding_matrix):\n",
        "    words = Input(shape=(MAX_WORDS_IN_COMMENT,))\n",
        "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
        "    #x = Input(batch_shape=(batch_size, MAX_CHARS_IN_COMMENT, embedding_len))\n",
        "    \n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x)\n",
        "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "\n",
        "    hidden = concatenate([\n",
        "        GlobalMaxPooling1D()(x),\n",
        "        GlobalAveragePooling1D()(x),\n",
        "    ])\n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
        "    result = Dense(len(TARGET_COLS), activation='sigmoid')(hidden)\n",
        "    \n",
        "    \n",
        "    model = Model(inputs=words, outputs=result)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuPRFfOJf-tH",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# class RocAucEvaluation(Callback):\n",
        "#     def __init__(self, validation_data=(), interval=1):\n",
        "#         super(Callback, self).__init__()\n",
        "\n",
        "#         self.interval = interval\n",
        "#         self.X_val, self.y_val = validation_data\n",
        "\n",
        "#     def on_epoch_end(self, epoch, logs={}):\n",
        "#         if epoch % self.interval == 0:\n",
        "#             y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "#             score = 0\n",
        "#             for i in range(6):\n",
        "#              score += roc_auc_score(self.y_val[:,i], y_pred[:,i])/6.\n",
        "#             print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
        "\n",
        "# RocAuc_val = RocAucEvaluation(validation_data=(x_valid, y_valid), interval = 1)\n",
        "\n",
        "class RocAucEarlyStopping(Callback):\n",
        "    \"\"\"Callback for early stopping based on roc auc on the validation set\"\"\"\n",
        "\n",
        "    def __init__(self, validation_data, patience=0, digits=3):\n",
        "        super().__init__()\n",
        "        self.X_val, self.y_val = validation_data\n",
        "        self.best = 0\n",
        "        self.patience = patience\n",
        "        self.digits = digits\n",
        "        self.current_patience = patience\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "        score = 0\n",
        "        for i in range(6):\n",
        "          score += roc_auc_score(self.y_val[:,i], y_pred[:,i])/6.\n",
        "\n",
        "        score = round(score, self.digits)\n",
        "        print(\"ROC-AUC - epoch: {:d} - score: {}\\n\".format(epoch+1, score))\n",
        "\n",
        "        # check digits\n",
        "        if np.greater(score, self.best):\n",
        "            self.best = score\n",
        "            self.current_patience = self.patience\n",
        "        else:\n",
        "            print('\\nbest:{}\\ncurrent:{}'.format(self.best, score))\n",
        "            self.current_patience -= 1\n",
        "            if self.current_patience < 0:\n",
        "              self.model.stop_training = True\n",
        "              print('Early stopping due to lower roc auc')\n",
        "              self.current_patience = self.patience\n",
        "            else:\n",
        "              print('{} patience remained'.format(self.current_patience))\n",
        "\n",
        "RocAuc_ES = RocAucEarlyStopping(validation_data=(x_valid, y_valid), patience=1, digits=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TRvdcnky3TBH",
        "colab_type": "code",
        "outputId": "24f73e36-a368-4a42-fe8b-2bb0901d3766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "SEEDS = 2\n",
        "pred = 0\n",
        "\n",
        "for ii in tqdm(range(SEEDS), total=SEEDS):\n",
        "\n",
        "    model = build_model(embedding_matrix)\n",
        "    for global_epoch in range(GLOBAL_EPOCHS):\n",
        "        print('\\nglobal_epoch', global_epoch)\n",
        "\n",
        "\n",
        "        model.fit(\n",
        "                    x_train,\n",
        "                    y_train,\n",
        "                    validation_data = (x_valid, y_valid),\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    verbose=1,\n",
        "                    callbacks=[\n",
        "                        LearningRateScheduler(lambda _: 1e-3 * (0.5 ** global_epoch)),\n",
        "                        #RocAuc_val\n",
        "                        RocAuc_ES\n",
        "                    ]\n",
        "                )\n",
        "        print('fitted')    \n",
        "\n",
        "    pred += model.predict(tokenized_test, batch_size = 1024, verbose = 1)/SEEDS\n",
        "    np.save('pred', pred)\n",
        "    model.save_weights('model_weights_'+str(ii)+'.h5')\n",
        "    os.system('gzip '+'model_weights_'+str(ii)+'.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "\n",
            "global_epoch 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/5\n",
            "143613/143613 [==============================] - 87s 605us/step - loss: 0.0700 - val_loss: 0.0497\n",
            "ROC-AUC - epoch: 1 - score: 0.9787\n",
            "\n",
            "Epoch 2/5\n",
            "143613/143613 [==============================] - 82s 569us/step - loss: 0.0473 - val_loss: 0.0446\n",
            "ROC-AUC - epoch: 2 - score: 0.9856\n",
            "\n",
            "Epoch 3/5\n",
            "143613/143613 [==============================] - 82s 569us/step - loss: 0.0429 - val_loss: 0.0445\n",
            "ROC-AUC - epoch: 3 - score: 0.9868\n",
            "\n",
            "Epoch 4/5\n",
            "143613/143613 [==============================] - 82s 568us/step - loss: 0.0402 - val_loss: 0.0428\n",
            "ROC-AUC - epoch: 4 - score: 0.9879\n",
            "\n",
            "Epoch 5/5\n",
            "143613/143613 [==============================] - 82s 569us/step - loss: 0.0382 - val_loss: 0.0447\n",
            "ROC-AUC - epoch: 5 - score: 0.9881\n",
            "\n",
            "fitted\n",
            "\n",
            "global_epoch 1\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/5\n",
            "143613/143613 [==============================] - 82s 569us/step - loss: 0.0340 - val_loss: 0.0440\n",
            "ROC-AUC - epoch: 1 - score: 0.9878\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.9878\n",
            "0 patience remained\n",
            "Epoch 2/5\n",
            "143613/143613 [==============================] - 82s 570us/step - loss: 0.0321 - val_loss: 0.0443\n",
            "ROC-AUC - epoch: 2 - score: 0.9877\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.9877\n",
            "Early stopping due to lower roc auc\n",
            "fitted\n",
            "153164/153164 [==============================] - 33s 213us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 1/2 [11:51<11:51, 711.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "global_epoch 0\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/5\n",
            "143613/143613 [==============================] - 82s 570us/step - loss: 0.0693 - val_loss: 0.0518\n",
            "ROC-AUC - epoch: 1 - score: 0.98\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.98\n",
            "0 patience remained\n",
            "Epoch 2/5\n",
            "143613/143613 [==============================] - 81s 567us/step - loss: 0.0469 - val_loss: 0.0469\n",
            "ROC-AUC - epoch: 2 - score: 0.9854\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.9854\n",
            "Early stopping due to lower roc auc\n",
            "fitted\n",
            "\n",
            "global_epoch 1\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/5\n",
            "143613/143613 [==============================] - 81s 567us/step - loss: 0.0417 - val_loss: 0.0426\n",
            "ROC-AUC - epoch: 1 - score: 0.9873\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.9873\n",
            "0 patience remained\n",
            "Epoch 2/5\n",
            "143613/143613 [==============================] - 81s 566us/step - loss: 0.0395 - val_loss: 0.0427\n",
            "ROC-AUC - epoch: 2 - score: 0.9878\n",
            "\n",
            "\n",
            "best:0.9881\n",
            "current:0.9878\n",
            "Early stopping due to lower roc auc\n",
            "fitted\n",
            "153164/153164 [==============================] - 33s 213us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 2/2 [18:48<00:00, 623.22s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10min 36s, sys: 4min, total: 14min 36s\n",
            "Wall time: 18min 48s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujanlBVs61IJ",
        "colab_type": "text"
      },
      "source": [
        "## Cheaty evaluating on test labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIOGCjO11DrT",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "submission = pd.read_csv(submission_path)\n",
        "submission[TARGET_COLS] = (pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCukoB8QB2Rz",
        "colab_type": "code",
        "trusted": true,
        "outputId": "760be629-18f0-4d73-c279-07df46bebc54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "submission.head(10)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00001cee341fdb12</td>\n",
              "      <td>0.996676</td>\n",
              "      <td>3.414785e-01</td>\n",
              "      <td>0.912232</td>\n",
              "      <td>9.897777e-02</td>\n",
              "      <td>0.918225</td>\n",
              "      <td>0.090533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000247867823ef7</td>\n",
              "      <td>0.008345</td>\n",
              "      <td>3.433228e-05</td>\n",
              "      <td>0.000827</td>\n",
              "      <td>7.009506e-05</td>\n",
              "      <td>0.001426</td>\n",
              "      <td>0.000180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00013b17ad220c46</td>\n",
              "      <td>0.000289</td>\n",
              "      <td>3.278255e-07</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>1.728535e-06</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00017563c3f7919a</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>5.960464e-08</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>9.238720e-07</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00017695ad8997eb</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>1.132488e-05</td>\n",
              "      <td>0.000740</td>\n",
              "      <td>2.795458e-05</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>0.000034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>0.000428</td>\n",
              "      <td>3.576279e-07</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>3.904104e-06</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>00024115d4cbde0f</td>\n",
              "      <td>0.000689</td>\n",
              "      <td>5.662441e-07</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>4.947186e-06</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>0.677728</td>\n",
              "      <td>1.572847e-03</td>\n",
              "      <td>0.090437</td>\n",
              "      <td>3.046185e-03</td>\n",
              "      <td>0.142409</td>\n",
              "      <td>0.002204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00025358d4737918</td>\n",
              "      <td>0.275358</td>\n",
              "      <td>1.869202e-04</td>\n",
              "      <td>0.010572</td>\n",
              "      <td>1.259267e-03</td>\n",
              "      <td>0.062003</td>\n",
              "      <td>0.000249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00026d1092fe71cc</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>5.960464e-08</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>7.748604e-07</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id     toxic  ...    insult  identity_hate\n",
              "0  00001cee341fdb12  0.996676  ...  0.918225       0.090533\n",
              "1  0000247867823ef7  0.008345  ...  0.001426       0.000180\n",
              "2  00013b17ad220c46  0.000289  ...  0.000030       0.000009\n",
              "3  00017563c3f7919a  0.000140  ...  0.000014       0.000002\n",
              "4  00017695ad8997eb  0.003606  ...  0.000358       0.000034\n",
              "5  0001ea8717f6de06  0.000428  ...  0.000054       0.000005\n",
              "6  00024115d4cbde0f  0.000689  ...  0.000085       0.000008\n",
              "7  000247e83dcc1211  0.677728  ...  0.142409       0.002204\n",
              "8  00025358d4737918  0.275358  ...  0.062003       0.000249\n",
              "9  00026d1092fe71cc  0.000164  ...  0.000014       0.000003\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUF4Uxrl8pXr",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# labels for the test data; value of -1 indicates it was not used for scoring; (Note: file added after competition close!)\n",
        "test_labels_df = pd.read_csv(test_labels_filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6crgLcLx7X-p",
        "colab_type": "code",
        "trusted": true,
        "outputId": "5e6ca27c-387b-4a39-8a70-a9d2caf14d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_labels_df = test_labels_df[(test_labels_df[\"toxic\"] != -1) &\n",
        "                                (test_labels_df[\"severe_toxic\"] != -1) &\n",
        "                                (test_labels_df[\"obscene\"] != -1) &\n",
        "                                (test_labels_df[\"threat\"] != -1) &\n",
        "                                (test_labels_df[\"insult\"] != -1) &\n",
        "                                (test_labels_df[\"identity_hate\"] != -1)]\n",
        "test_labels_df.shape                               "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(63978, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ymumFOQ9sA1",
        "colab_type": "code",
        "trusted": true,
        "outputId": "994f1949-312d-4032-eb52-6e0eb9430982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "submission_to_evaluate = submission[submission['id'].isin(test_labels_df['id'].values)]\n",
        "submission_to_evaluate.shape"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(63978, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3r8VTCK_TaM",
        "colab_type": "code",
        "trusted": true,
        "outputId": "e6010eaf-44be-4a9d-80a6-4bf92d2af2ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "scores = []\n",
        "for class_name in TARGET_COLS:\n",
        "    train_target = test_labels_df[class_name]\n",
        "    train_predicted = submission_to_evaluate[class_name]\n",
        "\n",
        "    cv_score = np.mean(roc_auc_score(train_target.values, train_predicted.values))\n",
        "    scores.append(cv_score)\n",
        "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
        "\n",
        "print('Total CV score is {}'.format(np.mean(scores)))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV score for class toxic is 0.9623414567715155\n",
            "CV score for class severe_toxic is 0.9883083645713258\n",
            "CV score for class obscene is 0.9758702005183666\n",
            "CV score for class threat is 0.9763517387836063\n",
            "CV score for class insult is 0.972054283405765\n",
            "CV score for class identity_hate is 0.9841185309254274\n",
            "Total CV score is 0.9765074291626678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YKsSnwkd3TBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.to_csv(os.path.join(output_folder,\"submission_fine_tuned_bert.csv\"), index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNf0KsAHCSJB",
        "colab_type": "text"
      },
      "source": [
        "Жирный шрифт означает изменения в процессе валидации и оценивания и при изменении действует начиная со строчки указания и ниже\n",
        "\n",
        "* glove twitter 200:  0.98052\n",
        "\n",
        "**С поднятием колва epochs и global_epochs + RocAucEarlyStopping** скор упал с 0.98052 до 0.97573\n",
        "\n",
        "* GLOBAL_EPOCHS = 2, EPOCHS = 5, glove.840B.300d.txt   0.97994  \n",
        "* BPEmb(lang=\"en\", dim=25, vs=20000)   0.97375  \n",
        "* BPEmb(lang=\"en\", dim=300, vs=20000) without preprocessing   0.97699  \n",
        "* BPEmb(lang=\"en\", dim=300, vs=20000)   0.97865   \n",
        "* fasttext crawl-300d-2M-subword 0.95732  \n",
        "* BPEmb(lang=\"en\", dim=300, vs=20000) **Patience 2->1 SEEDS 1->1**  0.98041   \n",
        "* flair ELMoEmbeddings('small') embedding len 768  0.98005  \n",
        "* flair ELMoEmbeddings('medium') embedding len 1536  0.98038  \n",
        "* flair RoBERTaEmbeddings('roberta-base') embedding len 768  0.97413  \n",
        "* flair stack of ELMoEmbeddings('small') + RoBERTaEmbeddings('roberta-base') embedding len 1536  0.98069      \n",
        "* 3 tuned layers in BERT modeldense = tf.keras.layers.Dense(256, activation='relu')(bert_output)pred = tf.keras.layers.Dense(len(TARGET_COLS), activation='sigmoid')(dense)model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])epochs=1,batch_size=32   0.97658"
      ]
    }
  ]
}